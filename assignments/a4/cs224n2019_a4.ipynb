{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Ole1mreSrOqa",
    "outputId": "56525737-0252-44f8-84bc-3f40f6ffe35b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "file_path = '/content/drive/My Drive/en_es_data/'\n",
    "\n",
    "args = {\n",
    "    \"--cuda\": True,\n",
    "    \"--train-src\": file_path + \"train.es\",\n",
    "    \"--train-tgt\": file_path + \"train.en\",\n",
    "    \"--dev-src\": file_path + \"dev.es\",\n",
    "    \"--dev-tgt\": file_path + \"dev.en\",\n",
    "    \"--vocab\": file_path + \"vocab.json\",\n",
    "    \"--seed\": 0,\n",
    "    \"--batch-size\": 32,\n",
    "    \"--embed-size\": 256,\n",
    "    \"--hidden-size\": 256,\n",
    "    \"--clip-grad\": 5.0,\n",
    "    \"--log-every\": 10,\n",
    "    \"--max-epoch\": 30,\n",
    "    \"--input-feed\": True,\n",
    "    \"--patience\": 5,\n",
    "    \"--max-num-trial\": 5,\n",
    "    \"--lr-decay\": 0.5,\n",
    "    \"--beam-size\": 5,\n",
    "    \"--sample-size\": 5,\n",
    "    \"--lr\": 0.001,\n",
    "    \"--uniform-init\": 0.1,\n",
    "    \"--save-to\": \"model.bin\",\n",
    "    \"--valid-niter\": 2000,\n",
    "    \"--dropout\": 0.3,\n",
    "    \"--max-decoding-time-step\": 70,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AW6rGSu_kVcV"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This file would merge all the code into one, to run on Google Colaboratory.\n",
    "\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "model_embeddings.py: Embeddings for the NMT model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Anand Dhoot <anandd@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # default values\n",
    "        self.source = None\n",
    "        self.target = None\n",
    "\n",
    "        src_pad_token_idx = vocab.src['<pad>']\n",
    "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
    "\n",
    "        ### YOUR CODE HERE (~2 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.source (Embedding Layer for source language)\n",
    "        ###     self.target (Embedding Layer for target langauge)\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###     1. `vocab` object contains two vocabularies:\n",
    "        ###            `vocab.src` for source\n",
    "        ###            `vocab.tgt` for target\n",
    "        ###     2. You can get the length of a specific vocabulary by running:\n",
    "        ###             `len(vocab.<specific_vocabulary>)`\n",
    "        ###     3. Remember to include the padding token for the specific vocabulary\n",
    "        ###        when creating your Embedding.\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     Embedding Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        self.source = nn.Embedding(len(vocab.src), self.embed_size, src_pad_token_idx)\n",
    "        self.target = nn.Embedding(len(vocab.tgt), self.embed_size, tgt_pad_token_idx)\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxVZDsC2rOqe"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "nmt_model.py: NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    \"\"\" Simple Neural Machine Translation Model:\n",
    "        - Bidrectional LSTM Encoder\n",
    "        - Unidirection LSTM Decoder\n",
    "        - Global Attention Model (Luong, et al. 2015)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        \"\"\" Init NMT Model.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param hidden_size (int): Hidden Size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        @param dropout_rate (float): Dropout probability, for attention\n",
    "        \"\"\"\n",
    "        super(NMT, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)  # Should we give an initial weight here?\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # default values\n",
    "        self.encoder = None \n",
    "        self.decoder = None\n",
    "        self.h_projection = None\n",
    "        self.c_projection = None\n",
    "        self.att_projection = None\n",
    "        self.combined_output_projection = None\n",
    "        self.target_vocab_projection = None\n",
    "        self.dropout = None\n",
    "\n",
    "\n",
    "        ### YOUR CODE HERE (~8 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.encoder (Bidirectional LSTM with bias)\n",
    "        ###     self.decoder (LSTM Cell with bias)\n",
    "        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n",
    "        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n",
    "        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n",
    "        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n",
    "        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n",
    "        ###     self.dropout (Dropout Layer)\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     LSTM:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "        ###     LSTM Cell:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n",
    "        ###     Linear Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Dropout Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n",
    "        # Why no bias?\n",
    "        self.h_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n",
    "        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
    "        target sentences under the language models learned by the NMT system.\n",
    "\n",
    "        @param source (List[List[str]]): list of source sentence tokens\n",
    "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
    "                                    log-likelihood of generating the gold-standard target sentence for\n",
    "                                    each example in the input batch. Here b = batch size.\n",
    "        \"\"\"\n",
    "        # Compute sentence lengths\n",
    "        source_lengths = [len(s) for s in source]\n",
    "\n",
    "        # Convert list of lists into tensors\n",
    "        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n",
    "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n",
    "\n",
    "        ###     Run the network forward:\n",
    "        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n",
    "        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n",
    "        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n",
    "        ###     4. Compute log probability distribution over the target vocabulary using the\n",
    "        ###        combined_outputs returned by the `self.decode()` function.\n",
    "\n",
    "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
    "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
    "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n",
    "        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
    "\n",
    "        # Zero out, probabilities for which we have nothing in the target text\n",
    "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
    "        \n",
    "        # Compute log probability of generating true target words\n",
    "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n",
    "        scores = target_gold_words_log_prob.sum(dim=0)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
    "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
    "\n",
    "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
    "                                        b = batch_size, src_len = maximum source sentence length. Note that \n",
    "                                       these have already been sorted in order of longest to shortest sentence.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
    "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
    "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
    "                                                hidden state and cell.\n",
    "        \"\"\"\n",
    "        enc_hiddens, dec_init_state = None, None\n",
    "\n",
    "        ### YOUR CODE HERE (~ 8 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n",
    "        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n",
    "        ###         that there is no initial hidden state or cell for the decoder.\n",
    "        ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n",
    "        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n",
    "        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n",
    "        ###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to\n",
    "        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n",
    "        ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n",
    "        ###         - `init_decoder_hidden`:\n",
    "        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n",
    "        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###         - `init_decoder_cell`:\n",
    "        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n",
    "        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###\n",
    "        ### See the following docs, as you may need to use some of the following functions in your implementation:\n",
    "        ###     Pack the padded sequence X before passing to the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
    "        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tensor Permute:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n",
    "        '''\n",
    "        How could the sanity test control the initial weight of Linear and embedding, to match the result?\n",
    "        Answer: \n",
    "        \n",
    "        '''\n",
    "        X = self.model_embeddings.source(source_padded)  # (src_len, b, e)\n",
    "        packed_sequence = pack_padded_sequence(X, source_lengths)\n",
    "        output, (last_hidden, last_cell) = self.encoder(packed_sequence)\n",
    "        enc_hiddens, enc_length = pad_packed_sequence(output)\n",
    "        enc_hiddens.transpose_(0, 1)\n",
    "        layer_num, batch_size, _ = last_hidden.size()\n",
    "        ''' \n",
    "        # One method\n",
    "        last_hidden = torch.cat([last_hidden[i] for i in range(layer_num)], 1)\n",
    "        '''\n",
    "        # Another\n",
    "        last_hidden = last_hidden.permute(1, 0, 2)\n",
    "        # print(last_hidden.size())\n",
    "        last_hidden = last_hidden.contiguous().view(batch_size, -1)\n",
    "        last_cell = torch.cat([last_cell[i] for i in range(layer_num)], 1)\n",
    "        init_decoder_hidden = self.h_projection(last_hidden)\n",
    "        init_decoder_cell = self.c_projection(last_cell)\n",
    "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return enc_hiddens, dec_init_state\n",
    "\n",
    "\n",
    "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n",
    "                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute combined output vectors for a batch.\n",
    "\n",
    "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
    "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
    "                                     b = batch size, src_len = maximum source sentence length.\n",
    "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
    "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n",
    "                                       tgt_len = maximum target sentence length, b = batch size. \n",
    "\n",
    "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
    "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
    "        \"\"\"\n",
    "        # Chop of the <END> token for max length sentences.\n",
    "        target_padded = target_padded[:-1]\n",
    "\n",
    "        # Initialize the decoder state (hidden and cell)\n",
    "        dec_state = dec_init_state\n",
    "\n",
    "        # Initialize previous combined output vector o_{t-1} as zero\n",
    "        batch_size = enc_hiddens.size(0)\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        # Initialize a list we will use to collect the combined output o_t on each step\n",
    "        combined_outputs = []\n",
    "\n",
    "        ### YOUR CODE HERE (~9 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n",
    "        ###         which should be shape (b, src_len, h),\n",
    "        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n",
    "        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
    "        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n",
    "        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n",
    "        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n",
    "        ###             - Construct Ybar_t by concatenating Y_t with o_prev.\n",
    "        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n",
    "        ###               as well as the new combined output o_t.\n",
    "        ###             - Append o_t to combined_outputs\n",
    "        ###             - Update o_prev to the new o_t.\n",
    "        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n",
    "        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###   \n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Zeros Tensor:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n",
    "        ###     Tensor Splitting (iteration):\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n",
    "        ###     Tensor Dimension Squeezing:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tensor Stacking:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n",
    "        enc_hiddens_proj = self.att_projection(enc_hiddens)   # (b, src_len, h)     torch.matmul( ,\n",
    "        Y = self.model_embeddings.target(target_padded)     # (tgt_len, b, e)\n",
    "        Y_splited = torch.split(Y, 1)\n",
    "        output_list = []\n",
    "        for Y_t in Y_splited:\n",
    "            Y_t = torch.squeeze(Y_t, 0)    # (b, e)\n",
    "            Ybar_t = torch.cat((Y_t, o_prev), dim=1)  # (b, h + e)\n",
    "            dec_state, o_t, e_t = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n",
    "            output_list.append(o_t)\n",
    "            o_prev = o_t\n",
    "        combined_outputs = torch.stack(output_list)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return combined_outputs\n",
    "\n",
    "\n",
    "    def step(self, Ybar_t: torch.Tensor,\n",
    "            dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "            enc_hiddens: torch.Tensor,\n",
    "            enc_hiddens_proj: torch.Tensor,\n",
    "            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
    "\n",
    "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
    "                                where b = batch size, e = embedding size, h = hidden size.\n",
    "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
    "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
    "                                    src_len = maximum source length, h = hidden size.\n",
    "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
    "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
    "                                    where b = batch size, src_len is maximum source length. \n",
    "\n",
    "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
    "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
    "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
    "                                Note: You will not use this outside of this function.\n",
    "                                      We are simply returning this value so that we can sanity check\n",
    "                                      your implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        combined_output = None\n",
    "\n",
    "        ### YOUR CODE HERE (~3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n",
    "        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n",
    "        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). \n",
    "        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###       Hints:\n",
    "        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n",
    "        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n",
    "        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t.\n",
    "        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n",
    "        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Batch Multiplication:\n",
    "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        ###     Tensor Unsqueeze:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
    "        ###     Tensor Squeeze:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
    "        dec_state = self.decoder(Ybar_t, dec_state)\n",
    "        dec_hidden, dec_cell = dec_state    # (b, h)\n",
    "        e_t = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(dim=2))    # (b, src_len, h) @ (b, h, 1) -> (b, src_len, 1)\n",
    "        e_t.squeeze_(dim=2)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Set e_t to -inf where enc_masks has 1\n",
    "        if enc_masks is not None:\n",
    "            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))    # for softmax\n",
    "\n",
    "        ### YOUR CODE HERE (~6 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply softmax to e_t to yield alpha_t\n",
    "        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n",
    "        ###         attention output vector, a_t.\n",
    "        #$$     Hints:\n",
    "        ###           - alpha_t is shape (b, src_len)\n",
    "        ###           - enc_hiddens is shape (b, src_len, 2h)\n",
    "        ###           - a_t should be shape (b, 2h)\n",
    "        ###           - You will need to do some squeezing and unsqueezing.\n",
    "        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n",
    "        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n",
    "        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Softmax:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n",
    "        ###     Batch Multiplication:\n",
    "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        ###     Tensor View:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tanh:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n",
    "        alpha_t = F.softmax(e_t, dim=1)   # (b, src_len)\n",
    "        a_t = torch.bmm(alpha_t.unsqueeze(dim=1), enc_hiddens)   # (b, 1, 2h)\n",
    "        a_t.squeeze_(dim=1)  # (b, h)\n",
    "        U_t = torch.cat((dec_hidden, a_t), dim=1)          # (b, 3h)\n",
    "        V_t = self.combined_output_projection(U_t)         # (b, h)\n",
    "        O_t = self.dropout(torch.tanh(V_t))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        combined_output = O_t\n",
    "        return dec_state, combined_output, e_t\n",
    "\n",
    "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
    "        \"\"\" Generate sentence masks for encoder hidden states.\n",
    "\n",
    "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
    "                                     src_len = max source length, h = hidden size. \n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
    "        \n",
    "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
    "                                    where src_len = max source length, h = hidden size.\n",
    "        \"\"\"\n",
    "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
    "        for e_id, src_len in enumerate(source_lengths):\n",
    "            enc_masks[e_id, src_len:] = 1\n",
    "        return enc_masks.to(self.device)\n",
    "\n",
    "\n",
    "    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
    "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
    "        @param src_sent (List[str]): a single source sentence (words)\n",
    "        @param beam_size (int): beam size\n",
    "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
    "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
    "                value: List[str]: the decoded target sentence, represented as a list of words\n",
    "                score: float: the log-likelihood of the target sentence\n",
    "        \"\"\"\n",
    "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
    "\n",
    "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
    "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
    "\n",
    "        h_tm1 = dec_init_vec\n",
    "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
    "\n",
    "        eos_id = self.vocab.tgt['</s>']\n",
    "\n",
    "        hypotheses = [['<s>']]\n",
    "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
    "        completed_hypotheses = []\n",
    "\n",
    "        t = 0\n",
    "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
    "            t += 1\n",
    "            hyp_num = len(hypotheses)\n",
    "\n",
    "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
    "                                                     src_encodings.size(1),   # batch: 1\n",
    "                                                     src_encodings.size(2))   # 2 * h\n",
    "\n",
    "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
    "                                                                           src_encodings_att_linear.size(1),\n",
    "                                                                           src_encodings_att_linear.size(2))\n",
    "\n",
    "            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
    "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
    "\n",
    "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
    "\n",
    "            (h_t, cell_t), att_t, _ = self.step(x, h_tm1,\n",
    "                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
    "\n",
    "            # log probabilities over target words\n",
    "            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
    "\n",
    "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
    "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
    "\n",
    "            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
    "\n",
    "            new_hypotheses = []\n",
    "            live_hyp_ids = []\n",
    "            new_hyp_scores = []\n",
    "\n",
    "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
    "                prev_hyp_id = prev_hyp_id.item()\n",
    "                hyp_word_id = hyp_word_id.item()\n",
    "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
    "\n",
    "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
    "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
    "                if hyp_word == '</s>':\n",
    "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
    "                                                           score=cand_new_hyp_score))\n",
    "                else:\n",
    "                    new_hypotheses.append(new_hyp_sent)\n",
    "                    live_hyp_ids.append(prev_hyp_id)\n",
    "                    new_hyp_scores.append(cand_new_hyp_score)\n",
    "\n",
    "            if len(completed_hypotheses) == beam_size:\n",
    "                break\n",
    "\n",
    "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
    "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
    "            att_tm1 = att_t[live_hyp_ids]\n",
    "\n",
    "            hypotheses = new_hypotheses\n",
    "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
    "\n",
    "        if len(completed_hypotheses) == 0:\n",
    "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
    "                                                   score=hyp_scores[0].item()))\n",
    "\n",
    "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "\n",
    "        return completed_hypotheses\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
    "        \"\"\"\n",
    "        return self.model_embeddings.source.weight.device\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model_path: str):\n",
    "        \"\"\" Load the model from a file.\n",
    "        @param model_path (str): path to model\n",
    "        \"\"\"\n",
    "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "        args = params['args']\n",
    "        model = NMT(vocab=params['vocab'], **args)\n",
    "        model.load_state_dict(params['state_dict'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\" Save the odel to a file.\n",
    "        @param path (str): path to the model\n",
    "        \"\"\"\n",
    "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
    "\n",
    "        params = {\n",
    "            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n",
    "            'vocab': self.vocab,\n",
    "            'state_dict': self.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlNrpa74rOqg"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "nmt.py: NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "from docopt import docopt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def pad_sents(sents, pad_token):\n",
    "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
    "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
    "                                    is represented as a list of words\n",
    "    @param pad_token (str): padding token\n",
    "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
    "        than the max length sentence are padded out with the pad_token, such that\n",
    "        each sentences in the batch now has equal length.\n",
    "    \"\"\"\n",
    "    sents_padded = []\n",
    "\n",
    "    ### YOUR CODE HERE (~6 Lines)\n",
    "    sents_len = list(map(lambda x: len(x), sents))\n",
    "    max_len = reduce(lambda x, y: x if x > y else y, sents_len, 0)\n",
    "    sents_padded = [sent + [pad_token] * (max_len - len(sent)) for sent in sents]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return sents_padded\n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(file_path, source):\n",
    "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
    "    @param file_path (str): path to file containing corpus\n",
    "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
    "        is of the source language or target language\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in open(file_path):\n",
    "        sent = line.strip().split(' ')\n",
    "        # only append <s> and </s> to the target sentence\n",
    "        if source == 'tgt':\n",
    "            sent = ['<s>'] + sent + ['</s>']\n",
    "        data.append(sent)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(data) / batch_size)\n",
    "    index_array = list(range(len(data)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
    "        examples = [data[idx] for idx in indices]\n",
    "\n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "        src_sents = [e[0] for e in examples]\n",
    "        tgt_sents = [e[1] for e in examples]\n",
    "\n",
    "        yield src_sents, tgt_sents\n",
    "\n",
    "\n",
    "def pad_sents_sanity_check():\n",
    "    sents = [['I', 'want', 'to', 'eat'],\n",
    "             ['Good', 'day'],\n",
    "             ['What']]\n",
    "    print(pad_sents(sents, '<UNK>'))\n",
    "    print('pad_sents sanity check passed!')\n",
    "\n",
    "# pad_sents_sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H331h85krOqi"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "vocab.py: Vocabulary Generation\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\n",
    "Usage:\n",
    "    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n",
    "\n",
    "Options:\n",
    "    -h --help                  Show this screen.\n",
    "    --train-src=<file>         File of training source sentences\n",
    "    --train-tgt=<file>         File of training target sentences\n",
    "    --size=<int>               vocab size [default: 50000]\n",
    "    --freq-cutoff=<int>        frequency cutoff [default: 2]\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0   # Pad Token\n",
    "            self.word2id['<s>'] = 1 # Start Token\n",
    "            self.word2id['</s>'] = 2    # End Token\n",
    "            self.word2id['<unk>'] = 3   # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word \n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained    \n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of words or list of sentences of words\n",
    "        into list or list of list of indices.\n",
    "        @param sents (list[str] or list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        if type(sents[0]) == list:\n",
    "            return [[self[w] for w in s] for s in sents]\n",
    "        else:\n",
    "            return [self[w] for w in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\" Vocab encapsulating src and target langauges.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
    "        \"\"\" Init Vocab.\n",
    "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
    "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
    "        \"\"\"\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n",
    "        \"\"\" Build Vocabulary.\n",
    "        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n",
    "        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n",
    "        @param vocab_size (int): Size of vocabulary for both source and target languages\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n",
    "        \"\"\"\n",
    "        assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "        print('initialize source vocabulary ..')\n",
    "        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
    "\n",
    "        print('initialize target vocabulary ..')\n",
    "        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
    "\n",
    "        return Vocab(src, tgt)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\" Save Vocab to file as JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        \"\"\"\n",
    "        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\" Load vocabulary from JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        @returns Vocab object loaded from JSON dump\n",
    "        \"\"\"\n",
    "        entry = json.load(open(file_path, 'r'))\n",
    "        src_word2id = entry['src_word2id']\n",
    "        tgt_word2id = entry['tgt_word2id']\n",
    "\n",
    "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sfmXo6lPrOqo",
    "outputId": "10d73d7e-b47c-4398-f7c3-cbb5fd0976d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uniformly initialize parameters [-0.100000, +0.100000]\n",
      "use device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin Maximum Likelihood training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1, iter 10, avg. loss 167.52, avg. ppl 23950.18 cum. examples 320, speed 2580.76 words/sec, time elapsed 2.06 sec\n",
      "epoch 1, iter 20, avg. loss 141.44, avg. ppl 3021.65 cum. examples 640, speed 2901.25 words/sec, time elapsed 4.01 sec\n",
      "epoch 1, iter 30, avg. loss 138.50, avg. ppl 1836.76 cum. examples 960, speed 2907.08 words/sec, time elapsed 6.04 sec\n",
      "epoch 1, iter 40, avg. loss 131.01, avg. ppl 1695.28 cum. examples 1280, speed 2751.72 words/sec, time elapsed 8.09 sec\n",
      "epoch 1, iter 50, avg. loss 132.02, avg. ppl 1522.26 cum. examples 1600, speed 3012.86 words/sec, time elapsed 10.00 sec\n",
      "epoch 1, iter 60, avg. loss 127.41, avg. ppl 1385.73 cum. examples 1920, speed 2930.69 words/sec, time elapsed 11.92 sec\n",
      "epoch 1, iter 70, avg. loss 117.53, avg. ppl 1286.55 cum. examples 2240, speed 2794.78 words/sec, time elapsed 13.80 sec\n",
      "epoch 1, iter 80, avg. loss 133.69, avg. ppl 1262.70 cum. examples 2560, speed 2996.29 words/sec, time elapsed 15.80 sec\n",
      "epoch 1, iter 90, avg. loss 123.51, avg. ppl 1199.45 cum. examples 2880, speed 2815.93 words/sec, time elapsed 17.78 sec\n",
      "epoch 1, iter 100, avg. loss 119.23, avg. ppl 1178.87 cum. examples 3200, speed 2714.80 words/sec, time elapsed 19.77 sec\n",
      "epoch 1, iter 110, avg. loss 119.20, avg. ppl 1128.15 cum. examples 3520, speed 2904.46 words/sec, time elapsed 21.64 sec\n",
      "epoch 1, iter 120, avg. loss 116.62, avg. ppl 1060.06 cum. examples 3840, speed 2766.68 words/sec, time elapsed 23.57 sec\n",
      "epoch 1, iter 130, avg. loss 119.55, avg. ppl 955.10 cum. examples 4160, speed 2798.70 words/sec, time elapsed 25.57 sec\n",
      "epoch 1, iter 140, avg. loss 120.00, avg. ppl 975.74 cum. examples 4480, speed 2826.06 words/sec, time elapsed 27.54 sec\n",
      "epoch 1, iter 150, avg. loss 119.11, avg. ppl 862.13 cum. examples 4800, speed 2872.23 words/sec, time elapsed 29.51 sec\n",
      "epoch 1, iter 160, avg. loss 114.16, avg. ppl 873.49 cum. examples 5120, speed 2643.90 words/sec, time elapsed 31.55 sec\n",
      "epoch 1, iter 170, avg. loss 122.32, avg. ppl 873.00 cum. examples 5440, speed 3072.14 words/sec, time elapsed 33.43 sec\n",
      "epoch 1, iter 180, avg. loss 120.60, avg. ppl 824.59 cum. examples 5760, speed 2835.78 words/sec, time elapsed 35.45 sec\n",
      "epoch 1, iter 190, avg. loss 113.63, avg. ppl 793.51 cum. examples 6080, speed 2781.19 words/sec, time elapsed 37.41 sec\n",
      "epoch 1, iter 200, avg. loss 114.08, avg. ppl 724.80 cum. examples 6400, speed 2911.55 words/sec, time elapsed 39.32 sec\n",
      "epoch 1, iter 210, avg. loss 120.55, avg. ppl 783.56 cum. examples 6720, speed 2956.94 words/sec, time elapsed 41.28 sec\n",
      "epoch 1, iter 220, avg. loss 117.00, avg. ppl 668.94 cum. examples 7040, speed 2836.94 words/sec, time elapsed 43.30 sec\n",
      "epoch 1, iter 230, avg. loss 116.26, avg. ppl 772.33 cum. examples 7360, speed 2745.66 words/sec, time elapsed 45.34 sec\n",
      "epoch 1, iter 240, avg. loss 117.45, avg. ppl 647.01 cum. examples 7680, speed 3026.85 words/sec, time elapsed 47.26 sec\n",
      "epoch 1, iter 250, avg. loss 109.52, avg. ppl 583.10 cum. examples 8000, speed 2715.76 words/sec, time elapsed 49.29 sec\n",
      "epoch 1, iter 260, avg. loss 116.34, avg. ppl 626.42 cum. examples 8320, speed 2827.77 words/sec, time elapsed 51.33 sec\n",
      "epoch 1, iter 270, avg. loss 113.66, avg. ppl 645.03 cum. examples 8640, speed 2882.46 words/sec, time elapsed 53.28 sec\n",
      "epoch 1, iter 280, avg. loss 116.47, avg. ppl 630.13 cum. examples 8960, speed 2683.15 words/sec, time elapsed 55.44 sec\n",
      "epoch 1, iter 290, avg. loss 109.39, avg. ppl 577.54 cum. examples 9280, speed 2858.79 words/sec, time elapsed 57.36 sec\n",
      "epoch 1, iter 300, avg. loss 108.21, avg. ppl 601.41 cum. examples 9600, speed 2784.84 words/sec, time elapsed 59.31 sec\n",
      "epoch 1, iter 310, avg. loss 116.24, avg. ppl 552.23 cum. examples 9920, speed 2745.30 words/sec, time elapsed 61.45 sec\n",
      "epoch 1, iter 320, avg. loss 108.20, avg. ppl 531.06 cum. examples 10240, speed 2905.73 words/sec, time elapsed 63.35 sec\n",
      "epoch 1, iter 330, avg. loss 116.29, avg. ppl 538.60 cum. examples 10560, speed 3064.40 words/sec, time elapsed 65.28 sec\n",
      "epoch 1, iter 340, avg. loss 111.63, avg. ppl 496.25 cum. examples 10880, speed 2844.90 words/sec, time elapsed 67.31 sec\n",
      "epoch 1, iter 350, avg. loss 111.53, avg. ppl 526.12 cum. examples 11200, speed 2920.78 words/sec, time elapsed 69.26 sec\n",
      "epoch 1, iter 360, avg. loss 113.94, avg. ppl 516.34 cum. examples 11520, speed 2763.42 words/sec, time elapsed 71.37 sec\n",
      "epoch 1, iter 370, avg. loss 105.54, avg. ppl 436.79 cum. examples 11840, speed 2771.08 words/sec, time elapsed 73.38 sec\n",
      "epoch 1, iter 380, avg. loss 100.56, avg. ppl 483.56 cum. examples 12160, speed 2620.13 words/sec, time elapsed 75.36 sec\n",
      "epoch 1, iter 390, avg. loss 114.44, avg. ppl 459.92 cum. examples 12480, speed 3131.37 words/sec, time elapsed 77.27 sec\n",
      "epoch 1, iter 400, avg. loss 100.69, avg. ppl 415.08 cum. examples 12800, speed 2707.90 words/sec, time elapsed 79.24 sec\n",
      "epoch 1, iter 410, avg. loss 107.05, avg. ppl 429.71 cum. examples 13120, speed 2697.12 words/sec, time elapsed 81.34 sec\n",
      "epoch 1, iter 420, avg. loss 105.64, avg. ppl 410.53 cum. examples 13440, speed 2877.65 words/sec, time elapsed 83.29 sec\n",
      "epoch 1, iter 430, avg. loss 103.33, avg. ppl 419.10 cum. examples 13760, speed 2637.12 words/sec, time elapsed 85.37 sec\n",
      "epoch 1, iter 440, avg. loss 105.27, avg. ppl 393.00 cum. examples 14080, speed 3000.11 words/sec, time elapsed 87.25 sec\n",
      "epoch 1, iter 450, avg. loss 106.34, avg. ppl 435.62 cum. examples 14400, speed 2965.65 words/sec, time elapsed 89.14 sec\n",
      "epoch 1, iter 460, avg. loss 101.01, avg. ppl 393.83 cum. examples 14720, speed 2805.57 words/sec, time elapsed 91.07 sec\n",
      "epoch 1, iter 470, avg. loss 110.68, avg. ppl 406.71 cum. examples 15040, speed 2953.43 words/sec, time elapsed 93.06 sec\n",
      "epoch 1, iter 480, avg. loss 102.33, avg. ppl 344.51 cum. examples 15360, speed 2620.96 words/sec, time elapsed 95.20 sec\n",
      "epoch 1, iter 490, avg. loss 98.99, avg. ppl 361.90 cum. examples 15680, speed 2705.49 words/sec, time elapsed 97.19 sec\n",
      "epoch 1, iter 500, avg. loss 102.41, avg. ppl 380.72 cum. examples 16000, speed 2749.56 words/sec, time elapsed 99.20 sec\n",
      "epoch 1, iter 510, avg. loss 103.39, avg. ppl 333.81 cum. examples 16320, speed 2852.22 words/sec, time elapsed 101.19 sec\n",
      "epoch 1, iter 520, avg. loss 100.81, avg. ppl 357.88 cum. examples 16640, speed 2835.76 words/sec, time elapsed 103.13 sec\n",
      "epoch 1, iter 530, avg. loss 96.66, avg. ppl 301.28 cum. examples 16960, speed 2709.19 words/sec, time elapsed 105.13 sec\n",
      "epoch 1, iter 540, avg. loss 103.79, avg. ppl 316.43 cum. examples 17280, speed 2816.96 words/sec, time elapsed 107.18 sec\n",
      "epoch 1, iter 550, avg. loss 98.00, avg. ppl 289.69 cum. examples 17600, speed 2761.24 words/sec, time elapsed 109.18 sec\n",
      "epoch 1, iter 560, avg. loss 100.46, avg. ppl 318.71 cum. examples 17920, speed 3010.84 words/sec, time elapsed 111.03 sec\n",
      "epoch 1, iter 570, avg. loss 101.47, avg. ppl 310.37 cum. examples 18240, speed 2950.50 words/sec, time elapsed 112.95 sec\n",
      "epoch 1, iter 580, avg. loss 103.69, avg. ppl 337.97 cum. examples 18560, speed 2811.67 words/sec, time elapsed 114.98 sec\n",
      "epoch 1, iter 590, avg. loss 101.61, avg. ppl 299.35 cum. examples 18880, speed 2897.75 words/sec, time elapsed 116.95 sec\n",
      "epoch 1, iter 600, avg. loss 101.49, avg. ppl 300.26 cum. examples 19200, speed 2946.86 words/sec, time elapsed 118.88 sec\n",
      "epoch 1, iter 610, avg. loss 102.38, avg. ppl 299.40 cum. examples 19520, speed 2721.12 words/sec, time elapsed 120.99 sec\n",
      "epoch 1, iter 620, avg. loss 102.24, avg. ppl 258.37 cum. examples 19840, speed 3023.67 words/sec, time elapsed 122.94 sec\n",
      "epoch 1, iter 630, avg. loss 97.76, avg. ppl 272.66 cum. examples 20160, speed 2862.11 words/sec, time elapsed 124.89 sec\n",
      "epoch 1, iter 640, avg. loss 103.21, avg. ppl 274.31 cum. examples 20480, speed 2813.04 words/sec, time elapsed 126.98 sec\n",
      "epoch 1, iter 650, avg. loss 105.88, avg. ppl 287.45 cum. examples 20800, speed 2828.18 words/sec, time elapsed 129.10 sec\n",
      "epoch 1, iter 660, avg. loss 101.61, avg. ppl 267.93 cum. examples 21120, speed 2958.39 words/sec, time elapsed 131.06 sec\n",
      "epoch 1, iter 670, avg. loss 97.84, avg. ppl 255.06 cum. examples 21440, speed 2848.09 words/sec, time elapsed 133.05 sec\n",
      "epoch 1, iter 680, avg. loss 101.19, avg. ppl 253.90 cum. examples 21760, speed 2856.22 words/sec, time elapsed 135.09 sec\n",
      "epoch 1, iter 690, avg. loss 96.71, avg. ppl 245.85 cum. examples 22080, speed 2977.14 words/sec, time elapsed 136.98 sec\n",
      "epoch 1, iter 700, avg. loss 94.90, avg. ppl 240.16 cum. examples 22400, speed 2734.87 words/sec, time elapsed 139.01 sec\n",
      "epoch 1, iter 710, avg. loss 98.13, avg. ppl 244.35 cum. examples 22720, speed 2780.58 words/sec, time elapsed 141.06 sec\n",
      "epoch 1, iter 720, avg. loss 99.95, avg. ppl 231.01 cum. examples 23040, speed 2949.13 words/sec, time elapsed 143.06 sec\n",
      "epoch 1, iter 730, avg. loss 99.51, avg. ppl 252.95 cum. examples 23360, speed 2921.15 words/sec, time elapsed 145.03 sec\n",
      "epoch 1, iter 740, avg. loss 101.53, avg. ppl 237.85 cum. examples 23680, speed 2941.67 words/sec, time elapsed 147.05 sec\n",
      "epoch 1, iter 750, avg. loss 95.01, avg. ppl 224.72 cum. examples 24000, speed 2874.36 words/sec, time elapsed 149.00 sec\n",
      "epoch 1, iter 760, avg. loss 98.54, avg. ppl 244.93 cum. examples 24320, speed 2896.45 words/sec, time elapsed 150.98 sec\n",
      "epoch 1, iter 770, avg. loss 101.08, avg. ppl 233.22 cum. examples 24640, speed 2915.37 words/sec, time elapsed 153.01 sec\n",
      "epoch 1, iter 780, avg. loss 102.82, avg. ppl 245.48 cum. examples 24960, speed 2942.36 words/sec, time elapsed 155.05 sec\n",
      "epoch 1, iter 790, avg. loss 94.90, avg. ppl 192.75 cum. examples 25280, speed 2874.19 words/sec, time elapsed 157.05 sec\n",
      "epoch 1, iter 800, avg. loss 96.73, avg. ppl 215.50 cum. examples 25600, speed 2843.09 words/sec, time elapsed 159.08 sec\n",
      "epoch 1, iter 810, avg. loss 95.29, avg. ppl 212.11 cum. examples 25920, speed 2752.37 words/sec, time elapsed 161.15 sec\n",
      "epoch 1, iter 820, avg. loss 95.39, avg. ppl 213.91 cum. examples 26240, speed 2850.01 words/sec, time elapsed 163.15 sec\n",
      "epoch 1, iter 830, avg. loss 96.36, avg. ppl 205.48 cum. examples 26560, speed 2885.80 words/sec, time elapsed 165.15 sec\n",
      "epoch 1, iter 840, avg. loss 91.92, avg. ppl 215.65 cum. examples 26880, speed 2908.09 words/sec, time elapsed 167.04 sec\n",
      "epoch 1, iter 850, avg. loss 96.40, avg. ppl 204.80 cum. examples 27200, speed 2873.59 words/sec, time elapsed 169.05 sec\n",
      "epoch 1, iter 860, avg. loss 102.00, avg. ppl 197.88 cum. examples 27520, speed 3022.59 words/sec, time elapsed 171.10 sec\n",
      "epoch 1, iter 870, avg. loss 94.89, avg. ppl 189.66 cum. examples 27840, speed 2926.41 words/sec, time elapsed 173.07 sec\n",
      "epoch 1, iter 880, avg. loss 90.22, avg. ppl 182.67 cum. examples 28160, speed 2872.44 words/sec, time elapsed 175.00 sec\n",
      "epoch 1, iter 890, avg. loss 97.42, avg. ppl 184.28 cum. examples 28480, speed 2985.11 words/sec, time elapsed 177.01 sec\n",
      "epoch 1, iter 900, avg. loss 94.83, avg. ppl 203.02 cum. examples 28800, speed 2828.89 words/sec, time elapsed 179.03 sec\n",
      "epoch 1, iter 910, avg. loss 90.89, avg. ppl 191.96 cum. examples 29120, speed 2939.30 words/sec, time elapsed 180.91 sec\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "run.py: Run Script for Simple NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\n",
    "Usage:\n",
    "    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n",
    "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n",
    "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n",
    "\n",
    "Options:\n",
    "    -h --help                               show this screen.\n",
    "    --cuda                                  use GPU\n",
    "    --train-src=<file>                      train source file\n",
    "    --train-tgt=<file>                      train target file\n",
    "    --dev-src=<file>                        dev source file\n",
    "    --dev-tgt=<file>                        dev target file\n",
    "    --vocab=<file>                          vocab file\n",
    "    --seed=<int>                            seed [default: 0]\n",
    "    --batch-size=<int>                      batch size [default: 32]\n",
    "    --embed-size=<int>                      embedding size [default: 256]\n",
    "    --hidden-size=<int>                     hidden size [default: 256]\n",
    "    --clip-grad=<float>                     gradient clipping [default: 5.0]\n",
    "    --log-every=<int>                       log every [default: 10]\n",
    "    --max-epoch=<int>                       max epoch [default: 30]\n",
    "    --input-feed                            use input feeding\n",
    "    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n",
    "    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n",
    "    --lr-decay=<float>                      learning rate decay [default: 0.5]\n",
    "    --beam-size=<int>                       beam size [default: 5]\n",
    "    --sample-size=<int>                     sample size [default: 5]\n",
    "    --lr=<float>                            learning rate [default: 0.001]\n",
    "    --uniform-init=<float>       `           uniformly initialize all parameters [default: 0.1]\n",
    "    --save-to=<file>                        model save path [default: model.bin]\n",
    "    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n",
    "    --dropout=<float>                       dropout [default: 0.3]\n",
    "    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n",
    "\"\"\"\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "\n",
    "\n",
    "def evaluate_ppl(model, dev_data, batch_size=32):\n",
    "    \"\"\" Evaluate perplexity on dev sentences\n",
    "    @param model (NMT): NMT Model\n",
    "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (batch size)\n",
    "    @returns ppl (perplixty on dev sentences)\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    cum_loss = 0.\n",
    "    cum_tgt_words = 0.\n",
    "\n",
    "    # no_grad() signals backend to throw away all gradients\n",
    "    with torch.no_grad():\n",
    "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
    "            loss = -model(src_sents, tgt_sents).sum()\n",
    "\n",
    "            cum_loss += loss.item()\n",
    "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
    "            cum_tgt_words += tgt_word_num_to_predict\n",
    "\n",
    "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
    "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
    "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
    "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
    "    @returns bleu_score: corpus-level BLEU score\n",
    "    \"\"\"\n",
    "    if references[0][0] == '<s>':\n",
    "        references = [ref[1:-1] for ref in references]\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
    "                             [hyp.value for hyp in hypotheses])\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def train(args: Dict):\n",
    "    \"\"\" Train the NMT Model.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "    train_data_src = read_corpus(args['--train-src'], source='src')\n",
    "    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n",
    "\n",
    "    dev_data_src = read_corpus(args['--dev-src'], source='src')\n",
    "    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n",
    "\n",
    "    train_data = list(zip(train_data_src, train_data_tgt))\n",
    "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
    "\n",
    "    train_batch_size = int(args['--batch-size'])\n",
    "    clip_grad = float(args['--clip-grad'])\n",
    "    valid_niter = int(args['--valid-niter'])\n",
    "    log_every = int(args['--log-every'])\n",
    "    model_save_path = args['--save-to']\n",
    "\n",
    "    vocab = Vocab.load(args['--vocab'])\n",
    "\n",
    "    model = NMT(embed_size=int(args['--embed-size']),\n",
    "                hidden_size=int(args['--hidden-size']),\n",
    "                dropout_rate=float(args['--dropout']),\n",
    "                vocab=vocab)\n",
    "    model.train()\n",
    "\n",
    "    uniform_init = float(args['--uniform-init'])\n",
    "    if np.abs(uniform_init) > 0.:\n",
    "        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
    "        for p in model.parameters():\n",
    "            p.data.uniform_(-uniform_init, uniform_init)\n",
    "\n",
    "    vocab_mask = torch.ones(len(vocab.tgt))\n",
    "    vocab_mask[vocab.tgt['<pad>']] = 0\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n",
    "    print('use device: %s' % device, file=sys.stderr)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n",
    "\n",
    "    num_trial = 0\n",
    "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
    "    cum_examples = report_examples = epoch = valid_num = 0\n",
    "    hist_valid_scores = []\n",
    "    train_time = begin_time = time.time()\n",
    "    print('begin Maximum Likelihood training')\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "\n",
    "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
    "            train_iter += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size = len(src_sents)\n",
    "\n",
    "            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n",
    "            batch_loss = example_losses.sum()\n",
    "            loss = batch_loss / batch_size\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses_val = batch_loss.item()\n",
    "            report_loss += batch_losses_val\n",
    "            cum_loss += batch_losses_val\n",
    "\n",
    "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
    "            report_tgt_words += tgt_words_num_to_predict\n",
    "            cum_tgt_words += tgt_words_num_to_predict\n",
    "            report_examples += batch_size\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            if train_iter % log_every == 0:\n",
    "                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
    "                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
    "                                                                                         report_loss / report_examples,\n",
    "                                                                                         math.exp(report_loss / report_tgt_words),\n",
    "                                                                                         cum_examples,\n",
    "                                                                                         report_tgt_words / (time.time() - train_time),\n",
    "                                                                                         time.time() - begin_time), file=sys.stderr)\n",
    "\n",
    "                train_time = time.time()\n",
    "                report_loss = report_tgt_words = report_examples = 0.\n",
    "\n",
    "            # perform validation\n",
    "            if train_iter % valid_niter == 0:\n",
    "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
    "                                                                                         cum_loss / cum_examples,\n",
    "                                                                                         np.exp(cum_loss / cum_tgt_words),\n",
    "                                                                                         cum_examples), file=sys.stderr)\n",
    "\n",
    "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
    "                valid_num += 1\n",
    "\n",
    "                print('begin validation ...', file=sys.stderr)\n",
    "\n",
    "                # compute dev. ppl and bleu\n",
    "                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
    "                valid_metric = -dev_ppl\n",
    "\n",
    "                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    patience = 0\n",
    "                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
    "                    model.save(model_save_path)\n",
    "\n",
    "                    # also save the optimizers' state\n",
    "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
    "                elif patience < int(args['--patience']):\n",
    "                    patience += 1\n",
    "                    print('hit patience %d' % patience, file=sys.stderr)\n",
    "\n",
    "                    if patience == int(args['--patience']):\n",
    "                        num_trial += 1\n",
    "                        print('hit #%d trial' % num_trial, file=sys.stderr)\n",
    "                        if num_trial == int(args['--max-num-trial']):\n",
    "                            print('early stop!', file=sys.stderr)\n",
    "                            exit(0)\n",
    "\n",
    "                        # decay lr, and restore from previously best checkpoint\n",
    "                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n",
    "                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
    "\n",
    "                        # load model\n",
    "                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
    "                        model.load_state_dict(params['state_dict'])\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        print('restore parameters of the optimizers', file=sys.stderr)\n",
    "                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
    "\n",
    "                        # set new lr\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "\n",
    "                        # reset patience\n",
    "                        patience = 0\n",
    "\n",
    "                if epoch == int(args['--max-epoch']):\n",
    "                    print('reached maximum number of epochs!', file=sys.stderr)\n",
    "                    exit(0)\n",
    "\n",
    "\n",
    "def decode(args: Dict[str, str]):\n",
    "    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n",
    "    If the target gold-standard sentences are given, the function also computes\n",
    "    corpus-level BLEU score.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n",
    "    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n",
    "    if args['TEST_TARGET_FILE']:\n",
    "        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n",
    "        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n",
    "\n",
    "    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n",
    "    model = NMT.load(args['MODEL_PATH'])\n",
    "\n",
    "    if args['--cuda']:\n",
    "        model = model.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "    hypotheses = beam_search(model, test_data_src,\n",
    "                             beam_size=int(args['--beam-size']),\n",
    "                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
    "\n",
    "    if args['TEST_TARGET_FILE']:\n",
    "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
    "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
    "        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
    "\n",
    "    with open(args['OUTPUT_FILE'], 'w') as f:\n",
    "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
    "            top_hyp = hyps[0]\n",
    "            hyp_sent = ' '.join(top_hyp.value)\n",
    "            f.write(hyp_sent + '\\n')\n",
    "\n",
    "\n",
    "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
    "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
    "    @param model (NMT): NMT Model\n",
    "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
    "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
    "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
    "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    hypotheses = []\n",
    "    with torch.no_grad():\n",
    "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
    "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
    "\n",
    "            hypotheses.append(example_hyps)\n",
    "\n",
    "    if was_training: model.train(was_training)\n",
    "\n",
    "    return hypotheses\n",
    "\n",
    "def main(train_mode=True):\n",
    "    \"\"\" Main func.\n",
    "    \"\"\"\n",
    "    # args = docopt(__doc__)\n",
    "\n",
    "    # Check pytorch version\n",
    "    # assert(torch.__version__ == \"1.0.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n",
    "\n",
    "    # seed the random number generators\n",
    "    seed = int(args['--seed'])\n",
    "    torch.manual_seed(seed)\n",
    "    if args['--cuda']:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed * 13 // 7)\n",
    "\n",
    "    if train_mode is True:\n",
    "        train(args)\n",
    "    else:\n",
    "        decode(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSaPNIuMrOqs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cs224n2019_a4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
