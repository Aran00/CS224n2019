{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (1.16.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (1.3.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (4.33.0)\n",
      "Collecting torch==1.1.0\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/56/9b/a241f6e3d1df013e65e0b03b17e46bd9517036cd9ba9d8cb9384df396e3b/torch-1.1.0-cp35-cp35m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 676.9MB 57kB/s  eta 0:00:01  8% |██▉                             | 59.5MB 16.8MB/s eta 0:00:37234.8MB 51.4MB/s eta 0:00:09��█████▋                    | 246.3MB 51.9MB/s eta 0:00:09eta 0:00:08    38% |████████████▍                   | 262.3MB 61.7MB/s eta 0:00:07eta 0:00:07▍                  | 283.5MB 52.2MB/s eta 0:00:08▊                  | 290.7MB 53.1MB/s eta 0:00:08��█████████████▏                 | 299.3MB 55.0MB/s eta 0:00:079MB 54.4MB/s eta 0:00:07    48% |███████████████▌                | 327.0MB 22.1MB/s eta 0:00:165MB 54.3MB/s eta 0:00:07    48% |███████████████▋                | 330.6MB 32.1MB/s eta 0:00:11    49% |███████████████▉                | 335.0MB 4.7MB/s eta 0:01:13    49% |████████████████                | 337.7MB 4.4MB/s eta 0:01:18��█████████████▎        | 493.5MB 53.7MB/s eta 0:00:04��█████████████▋        | 499.0MB 58.2MB/s eta 0:00:045% |████████████████████████▏       | 510.3MB 62.3MB/s eta 0:00:036% |████████████████████████▌       | 517.3MB 55.2MB/s eta 0:00:036% |████████████████████████▋       | 519.8MB 50.7MB/s eta 0:00:046% |████████████████████████▋       | 520.7MB 56.3MB/s eta 0:00:038% |█████████████████████████       | 528.3MB 56.1MB/s eta 0:00:038% |█████████████████████████       | 529.3MB 35.0MB/s eta 0:00:0579% |█████████████████████████▍      | 537.0MB 79.1MB/s eta 0:00:02MB 53.9MB/s eta 0:00:03MB 54.2MB/s eta 0:00:03MB 59.6MB/s eta 0:00:03█████▏     | 552.5MB 51.7MB/s eta 0:00:03��█████▌     | 560.5MB 55.2MB/s eta 0:00:03    87% |████████████████████████████    | 591.3MB 4.8MB/s eta 0:00:18MB/s eta 0:00:01████▋ | 648.4MB 54.2MB/s eta 0:00:01████▊ | 649.6MB 66.4MB/s eta 0:00:01████▉ | 652.7MB 47.4MB/s eta 0:00:01█████ | 655.1MB 52.0MB/s eta 0:00:01█████ | 656.0MB 58.7MB/s eta 0:00:01�██████████████▎| 661.0MB 54.8MB/s eta 0:00:01�██████████████▎| 662.0MB 51.1MB/s eta 0:00:01�██████████████▍| 663.5MB 50.4MB/s eta 0:00:01�██████████████▍| 664.5MB 54.4MB/s eta 0:00:01�██████████████▌| 667.1MB 58.2MB/s eta 0:00:01�██████████████▋| 668.6MB 54.0MB/s eta 0:00:01�██████████████▋| 669.5MB 53.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (3.4.3)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (0.4.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from nltk) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages (from torchvision) (6.0.0)\n",
      "\u001b[31mtorchvision 0.4.0 has requirement torch==1.2.0, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 1.2.0\n",
      "    Uninstalling torch-1.2.0:\n",
      "      Successfully uninstalled torch-1.2.0\n",
      "Successfully installed torch-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy tqdm torch==1.1.0 nltk torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# This file would merge all the code into one, to run on Google Colaboratory.\n",
    "\n",
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "model_embeddings.py: Embeddings for the NMT model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Anand Dhoot <anandd@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # default values\n",
    "        self.source = None\n",
    "        self.target = None\n",
    "\n",
    "        src_pad_token_idx = vocab.src['<pad>']\n",
    "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
    "\n",
    "        ### YOUR CODE HERE (~2 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.source (Embedding Layer for source language)\n",
    "        ###     self.target (Embedding Layer for target langauge)\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###     1. `vocab` object contains two vocabularies:\n",
    "        ###            `vocab.src` for source\n",
    "        ###            `vocab.tgt` for target\n",
    "        ###     2. You can get the length of a specific vocabulary by running:\n",
    "        ###             `len(vocab.<specific_vocabulary>)`\n",
    "        ###     3. Remember to include the padding token for the specific vocabulary\n",
    "        ###        when creating your Embedding.\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     Embedding Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        self.source = nn.Embedding(len(vocab.src), self.embed_size, src_pad_token_idx)\n",
    "        self.target = nn.Embedding(len(vocab.tgt), self.embed_size, tgt_pad_token_idx)\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "nmt_model.py: NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
    "\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    \"\"\" Simple Neural Machine Translation Model:\n",
    "        - Bidrectional LSTM Encoder\n",
    "        - Unidirection LSTM Decoder\n",
    "        - Global Attention Model (Luong, et al. 2015)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        \"\"\" Init NMT Model.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param hidden_size (int): Hidden Size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        @param dropout_rate (float): Dropout probability, for attention\n",
    "        \"\"\"\n",
    "        super(NMT, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)  # Should we give an initial weight here?\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # default values\n",
    "        self.encoder = None \n",
    "        self.decoder = None\n",
    "        self.h_projection = None\n",
    "        self.c_projection = None\n",
    "        self.att_projection = None\n",
    "        self.combined_output_projection = None\n",
    "        self.target_vocab_projection = None\n",
    "        self.dropout = None\n",
    "\n",
    "\n",
    "        ### YOUR CODE HERE (~8 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.encoder (Bidirectional LSTM with bias)\n",
    "        ###     self.decoder (LSTM Cell with bias)\n",
    "        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n",
    "        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n",
    "        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n",
    "        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n",
    "        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n",
    "        ###     self.dropout (Dropout Layer)\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     LSTM:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "        ###     LSTM Cell:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n",
    "        ###     Linear Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Dropout Layer:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n",
    "        # Why no bias?\n",
    "        self.h_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n",
    "        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
    "        target sentences under the language models learned by the NMT system.\n",
    "\n",
    "        @param source (List[List[str]]): list of source sentence tokens\n",
    "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
    "\n",
    "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
    "                                    log-likelihood of generating the gold-standard target sentence for\n",
    "                                    each example in the input batch. Here b = batch size.\n",
    "        \"\"\"\n",
    "        # Compute sentence lengths\n",
    "        source_lengths = [len(s) for s in source]\n",
    "\n",
    "        # Convert list of lists into tensors\n",
    "        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n",
    "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n",
    "\n",
    "        ###     Run the network forward:\n",
    "        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n",
    "        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n",
    "        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n",
    "        ###     4. Compute log probability distribution over the target vocabulary using the\n",
    "        ###        combined_outputs returned by the `self.decode()` function.\n",
    "\n",
    "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
    "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
    "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n",
    "        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
    "\n",
    "        # Zero out, probabilities for which we have nothing in the target text\n",
    "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
    "        \n",
    "        # Compute log probability of generating true target words\n",
    "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n",
    "        scores = target_gold_words_log_prob.sum(dim=0)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
    "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
    "\n",
    "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
    "                                        b = batch_size, src_len = maximum source sentence length. Note that \n",
    "                                       these have already been sorted in order of longest to shortest sentence.\n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
    "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
    "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
    "                                                hidden state and cell.\n",
    "        \"\"\"\n",
    "        enc_hiddens, dec_init_state = None, None\n",
    "\n",
    "        ### YOUR CODE HERE (~ 8 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n",
    "        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n",
    "        ###         that there is no initial hidden state or cell for the decoder.\n",
    "        ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n",
    "        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n",
    "        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n",
    "        ###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to\n",
    "        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n",
    "        ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n",
    "        ###         - `init_decoder_hidden`:\n",
    "        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n",
    "        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###         - `init_decoder_cell`:\n",
    "        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
    "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
    "        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n",
    "        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
    "        ###\n",
    "        ### See the following docs, as you may need to use some of the following functions in your implementation:\n",
    "        ###     Pack the padded sequence X before passing to the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
    "        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tensor Permute:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n",
    "        '''\n",
    "        How could the sanity test control the initial weight of Linear and embedding, to match the result?\n",
    "        Answer: \n",
    "        \n",
    "        '''\n",
    "        X = self.model_embeddings.source(source_padded)  # (src_len, b, e)\n",
    "        packed_sequence = pack_padded_sequence(X, source_lengths)\n",
    "        output, (last_hidden, last_cell) = self.encoder(packed_sequence)\n",
    "        enc_hiddens, enc_length = pad_packed_sequence(output)\n",
    "        enc_hiddens.transpose_(0, 1)\n",
    "        layer_num, batch_size, _ = last_hidden.size()\n",
    "        ''' \n",
    "        # One method\n",
    "        last_hidden = torch.cat([last_hidden[i] for i in range(layer_num)], 1)\n",
    "        '''\n",
    "        # Another\n",
    "        last_hidden = last_hidden.permute(1, 0, 2)\n",
    "        # print(last_hidden.size())\n",
    "        last_hidden = last_hidden.contiguous().view(batch_size, -1)\n",
    "        last_cell = torch.cat([last_cell[i] for i in range(layer_num)], 1)\n",
    "        init_decoder_hidden = self.h_projection(last_hidden)\n",
    "        init_decoder_cell = self.c_projection(last_cell)\n",
    "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return enc_hiddens, dec_init_state\n",
    "\n",
    "\n",
    "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n",
    "                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute combined output vectors for a batch.\n",
    "\n",
    "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
    "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
    "                                     b = batch size, src_len = maximum source sentence length.\n",
    "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
    "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n",
    "                                       tgt_len = maximum target sentence length, b = batch size. \n",
    "\n",
    "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
    "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
    "        \"\"\"\n",
    "        # Chop of the <END> token for max length sentences.\n",
    "        target_padded = target_padded[:-1]\n",
    "\n",
    "        # Initialize the decoder state (hidden and cell)\n",
    "        dec_state = dec_init_state\n",
    "\n",
    "        # Initialize previous combined output vector o_{t-1} as zero\n",
    "        batch_size = enc_hiddens.size(0)\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        # Initialize a list we will use to collect the combined output o_t on each step\n",
    "        combined_outputs = []\n",
    "\n",
    "        ### YOUR CODE HERE (~9 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n",
    "        ###         which should be shape (b, src_len, h),\n",
    "        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n",
    "        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
    "        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n",
    "        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n",
    "        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n",
    "        ###             - Construct Ybar_t by concatenating Y_t with o_prev.\n",
    "        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n",
    "        ###               as well as the new combined output o_t.\n",
    "        ###             - Append o_t to combined_outputs\n",
    "        ###             - Update o_prev to the new o_t.\n",
    "        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n",
    "        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n",
    "        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###   \n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Zeros Tensor:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n",
    "        ###     Tensor Splitting (iteration):\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n",
    "        ###     Tensor Dimension Squeezing:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tensor Stacking:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n",
    "        enc_hiddens_proj = self.att_projection(enc_hiddens)   # (b, src_len, h)     torch.matmul( ,\n",
    "        Y = self.model_embeddings.target(target_padded)     # (tgt_len, b, e)\n",
    "        Y_splited = torch.split(Y, 1)\n",
    "        output_list = []\n",
    "        for Y_t in Y_splited:\n",
    "            Y_t = torch.squeeze(Y_t, 0)    # (b, e)\n",
    "            Ybar_t = torch.cat((Y_t, o_prev), dim=1)  # (b, h + e)\n",
    "            dec_state, o_t, e_t = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n",
    "            output_list.append(o_t)\n",
    "            o_prev = o_t\n",
    "        combined_outputs = torch.stack(output_list)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return combined_outputs\n",
    "\n",
    "\n",
    "    def step(self, Ybar_t: torch.Tensor,\n",
    "            dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "            enc_hiddens: torch.Tensor,\n",
    "            enc_hiddens_proj: torch.Tensor,\n",
    "            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
    "\n",
    "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
    "                                where b = batch size, e = embedding size, h = hidden size.\n",
    "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
    "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
    "                                    src_len = maximum source length, h = hidden size.\n",
    "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
    "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
    "                                    where b = batch size, src_len is maximum source length. \n",
    "\n",
    "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
    "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
    "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
    "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
    "                                Note: You will not use this outside of this function.\n",
    "                                      We are simply returning this value so that we can sanity check\n",
    "                                      your implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        combined_output = None\n",
    "\n",
    "        ### YOUR CODE HERE (~3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n",
    "        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n",
    "        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). \n",
    "        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###       Hints:\n",
    "        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n",
    "        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n",
    "        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t.\n",
    "        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n",
    "        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
    "        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Batch Multiplication:\n",
    "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        ###     Tensor Unsqueeze:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
    "        ###     Tensor Squeeze:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
    "        dec_state = self.decoder(Ybar_t, dec_state)\n",
    "        dec_hidden, dec_cell = dec_state    # (b, h)\n",
    "        e_t = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(dim=2))    # (b, src_len, h) @ (b, h, 1) -> (b, src_len, 1)\n",
    "        e_t.squeeze_(dim=2)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Set e_t to -inf where enc_masks has 1\n",
    "        if enc_masks is not None:\n",
    "            e_t.data.masked_fill_(enc_masks.byte(), -float('inf'))    # for softmax\n",
    "\n",
    "        ### YOUR CODE HERE (~6 Lines)\n",
    "        ### TODO:\n",
    "        ###     1. Apply softmax to e_t to yield alpha_t\n",
    "        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n",
    "        ###         attention output vector, a_t.\n",
    "        #$$     Hints:\n",
    "        ###           - alpha_t is shape (b, src_len)\n",
    "        ###           - enc_hiddens is shape (b, src_len, 2h)\n",
    "        ###           - a_t should be shape (b, 2h)\n",
    "        ###           - You will need to do some squeezing and unsqueezing.\n",
    "        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n",
    "        ###\n",
    "        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n",
    "        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n",
    "        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n",
    "        ###\n",
    "        ### Use the following docs to implement this functionality:\n",
    "        ###     Softmax:\n",
    "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n",
    "        ###     Batch Multiplication:\n",
    "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "        ###     Tensor View:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        ###     Tensor Concatenation:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "        ###     Tanh:\n",
    "        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n",
    "        alpha_t = F.softmax(e_t, dim=1)   # (b, src_len)\n",
    "        a_t = torch.bmm(alpha_t.unsqueeze(dim=1), enc_hiddens)   # (b, 1, 2h)\n",
    "        a_t.squeeze_(dim=1)  # (b, h)\n",
    "        U_t = torch.cat((dec_hidden, a_t), dim=1)          # (b, 3h)\n",
    "        V_t = self.combined_output_projection(U_t)         # (b, h)\n",
    "        O_t = self.dropout(torch.tanh(V_t))\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        combined_output = O_t\n",
    "        return dec_state, combined_output, e_t\n",
    "\n",
    "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
    "        \"\"\" Generate sentence masks for encoder hidden states.\n",
    "\n",
    "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
    "                                     src_len = max source length, h = hidden size. \n",
    "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
    "        \n",
    "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
    "                                    where src_len = max source length, h = hidden size.\n",
    "        \"\"\"\n",
    "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
    "        for e_id, src_len in enumerate(source_lengths):\n",
    "            enc_masks[e_id, src_len:] = 1\n",
    "        return enc_masks.to(self.device)\n",
    "\n",
    "\n",
    "    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n",
    "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
    "        @param src_sent (List[str]): a single source sentence (words)\n",
    "        @param beam_size (int): beam size\n",
    "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
    "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
    "                value: List[str]: the decoded target sentence, represented as a list of words\n",
    "                score: float: the log-likelihood of the target sentence\n",
    "        \"\"\"\n",
    "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
    "\n",
    "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
    "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
    "\n",
    "        h_tm1 = dec_init_vec\n",
    "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
    "\n",
    "        eos_id = self.vocab.tgt['</s>']\n",
    "\n",
    "        hypotheses = [['<s>']]\n",
    "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
    "        completed_hypotheses = []\n",
    "\n",
    "        t = 0\n",
    "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
    "            t += 1\n",
    "            hyp_num = len(hypotheses)\n",
    "\n",
    "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
    "                                                     src_encodings.size(1),   # batch: 1\n",
    "                                                     src_encodings.size(2))   # 2 * h\n",
    "\n",
    "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
    "                                                                           src_encodings_att_linear.size(1),\n",
    "                                                                           src_encodings_att_linear.size(2))\n",
    "\n",
    "            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
    "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
    "\n",
    "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
    "\n",
    "            (h_t, cell_t), att_t, _ = self.step(x, h_tm1,\n",
    "                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
    "\n",
    "            # log probabilities over target words\n",
    "            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
    "\n",
    "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
    "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
    "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
    "\n",
    "            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n",
    "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
    "\n",
    "            new_hypotheses = []\n",
    "            live_hyp_ids = []\n",
    "            new_hyp_scores = []\n",
    "\n",
    "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
    "                prev_hyp_id = prev_hyp_id.item()\n",
    "                hyp_word_id = hyp_word_id.item()\n",
    "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
    "\n",
    "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
    "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
    "                if hyp_word == '</s>':\n",
    "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
    "                                                           score=cand_new_hyp_score))\n",
    "                else:\n",
    "                    new_hypotheses.append(new_hyp_sent)\n",
    "                    live_hyp_ids.append(prev_hyp_id)\n",
    "                    new_hyp_scores.append(cand_new_hyp_score)\n",
    "\n",
    "            if len(completed_hypotheses) == beam_size:\n",
    "                break\n",
    "\n",
    "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
    "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
    "            att_tm1 = att_t[live_hyp_ids]\n",
    "\n",
    "            hypotheses = new_hypotheses\n",
    "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
    "\n",
    "        if len(completed_hypotheses) == 0:\n",
    "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
    "                                                   score=hyp_scores[0].item()))\n",
    "\n",
    "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
    "\n",
    "        return completed_hypotheses\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
    "        \"\"\"\n",
    "        return self.model_embeddings.source.weight.device\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model_path: str):\n",
    "        \"\"\" Load the model from a file.\n",
    "        @param model_path (str): path to model\n",
    "        \"\"\"\n",
    "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "        args = params['args']\n",
    "        model = NMT(vocab=params['vocab'], **args)\n",
    "        model.load_state_dict(params['state_dict'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\" Save the odel to a file.\n",
    "        @param path (str): path to the model\n",
    "        \"\"\"\n",
    "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
    "\n",
    "        params = {\n",
    "            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n",
    "            'vocab': self.vocab,\n",
    "            'state_dict': self.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(params, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "nmt.py: NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import sys\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def pad_sents(sents, pad_token):\n",
    "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
    "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
    "                                    is represented as a list of words\n",
    "    @param pad_token (str): padding token\n",
    "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
    "        than the max length sentence are padded out with the pad_token, such that\n",
    "        each sentences in the batch now has equal length.\n",
    "    \"\"\"\n",
    "    sents_padded = []\n",
    "\n",
    "    ### YOUR CODE HERE (~6 Lines)\n",
    "    sents_len = list(map(lambda x: len(x), sents))\n",
    "    max_len = reduce(lambda x, y: x if x > y else y, sents_len, 0)\n",
    "    sents_padded = [sent + [pad_token] * (max_len - len(sent)) for sent in sents]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return sents_padded\n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(file_path, source):\n",
    "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
    "    @param file_path (str): path to file containing corpus\n",
    "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
    "        is of the source language or target language\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in open(file_path):\n",
    "        sent = line.strip().split(' ')\n",
    "        # only append <s> and </s> to the target sentence\n",
    "        if source == 'tgt':\n",
    "            sent = ['<s>'] + sent + ['</s>']\n",
    "        data.append(sent)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(data) / batch_size)\n",
    "    index_array = list(range(len(data)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
    "        examples = [data[idx] for idx in indices]\n",
    "\n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "        src_sents = [e[0] for e in examples]\n",
    "        tgt_sents = [e[1] for e in examples]\n",
    "\n",
    "        yield src_sents, tgt_sents\n",
    "\n",
    "\n",
    "def pad_sents_sanity_check():\n",
    "    sents = [['I', 'want', 'to', 'eat'],\n",
    "             ['Good', 'day'],\n",
    "             ['What']]\n",
    "    print(pad_sents(sents, '<UNK>'))\n",
    "    print('pad_sents sanity check passed!')\n",
    "\n",
    "# pad_sents_sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "vocab.py: Vocabulary Generation\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\n",
    "Usage:\n",
    "    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n",
    "\n",
    "Options:\n",
    "    -h --help                  Show this screen.\n",
    "    --train-src=<file>         File of training source sentences\n",
    "    --train-tgt=<file>         File of training target sentences\n",
    "    --size=<int>               vocab size [default: 50000]\n",
    "    --freq-cutoff=<int>        frequency cutoff [default: 2]\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0   # Pad Token\n",
    "            self.word2id['<s>'] = 1 # Start Token\n",
    "            self.word2id['</s>'] = 2    # End Token\n",
    "            self.word2id['<unk>'] = 3   # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word \n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained    \n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of words or list of sentences of words\n",
    "        into list or list of list of indices.\n",
    "        @param sents (list[str] or list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        if type(sents[0]) == list:\n",
    "            return [[self[w] for w in s] for s in sents]\n",
    "        else:\n",
    "            return [self[w] for w in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\" Vocab encapsulating src and target langauges.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
    "        \"\"\" Init Vocab.\n",
    "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
    "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
    "        \"\"\"\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> 'Vocab':\n",
    "        \"\"\" Build Vocabulary.\n",
    "        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n",
    "        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n",
    "        @param vocab_size (int): Size of vocabulary for both source and target languages\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n",
    "        \"\"\"\n",
    "        assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "        print('initialize source vocabulary ..')\n",
    "        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
    "\n",
    "        print('initialize target vocabulary ..')\n",
    "        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
    "\n",
    "        return Vocab(src, tgt)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\" Save Vocab to file as JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        \"\"\"\n",
    "        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, 'w'), indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\" Load vocabulary from JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        @returns Vocab object loaded from JSON dump\n",
    "        \"\"\"\n",
    "        entry = json.load(open(file_path, 'r'))\n",
    "        src_word2id = entry['src_word2id']\n",
    "        tgt_word2id = entry['tgt_word2id']\n",
    "\n",
    "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = '/home/aistudio/data/data10516/'\n",
    "\n",
    "args = {\n",
    "    \"--cuda\": True,\n",
    "    \"--train-src\": file_path + \"train.es\",\n",
    "    \"--train-tgt\": file_path + \"train.en\",\n",
    "    \"--dev-src\": file_path + \"dev.es\",\n",
    "    \"--dev-tgt\": file_path + \"dev.en\",\n",
    "    \"--vocab\": file_path + \"vocab.json\",\n",
    "    \"--seed\": 0,\n",
    "    \"--batch-size\": 32,\n",
    "    \"--embed-size\": 256,\n",
    "    \"--hidden-size\": 256,\n",
    "    \"--clip-grad\": 5.0,\n",
    "    \"--log-every\": 10,\n",
    "    \"--max-epoch\": 10,\n",
    "    \"--input-feed\": True,\n",
    "    \"--patience\": 5,\n",
    "    \"--max-num-trial\": 5,\n",
    "    \"--lr-decay\": 0.5,\n",
    "    \"--beam-size\": 5,\n",
    "    \"--sample-size\": 5,\n",
    "    \"--lr\": 0.001,\n",
    "    \"--uniform-init\": 0.1,\n",
    "    \"--save-to\": \"/home/aistudio/work/model.bin\",\n",
    "    \"--valid-niter\": 2000,\n",
    "    \"--dropout\": 0.3,\n",
    "    \"--max-decoding-time-step\": 70,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CS224N 2018-19: Homework 4\n",
    "run.py: Run Script for Simple NMT Model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "\n",
    "Usage:\n",
    "    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n",
    "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n",
    "    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n",
    "\n",
    "Options:\n",
    "    -h --help                               show this screen.\n",
    "    --cuda                                  use GPU\n",
    "    --train-src=<file>                      train source file\n",
    "    --train-tgt=<file>                      train target file\n",
    "    --dev-src=<file>                        dev source file\n",
    "    --dev-tgt=<file>                        dev target file\n",
    "    --vocab=<file>                          vocab file\n",
    "    --seed=<int>                            seed [default: 0]\n",
    "    --batch-size=<int>                      batch size [default: 32]\n",
    "    --embed-size=<int>                      embedding size [default: 256]\n",
    "    --hidden-size=<int>                     hidden size [default: 256]\n",
    "    --clip-grad=<float>                     gradient clipping [default: 5.0]\n",
    "    --log-every=<int>                       log every [default: 10]\n",
    "    --max-epoch=<int>                       max epoch [default: 30]\n",
    "    --input-feed                            use input feeding\n",
    "    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n",
    "    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n",
    "    --lr-decay=<float>                      learning rate decay [default: 0.5]\n",
    "    --beam-size=<int>                       beam size [default: 5]\n",
    "    --sample-size=<int>                     sample size [default: 5]\n",
    "    --lr=<float>                            learning rate [default: 0.001]\n",
    "    --uniform-init=<float>       `           uniformly initialize all parameters [default: 0.1]\n",
    "    --save-to=<file>                        model save path [default: model.bin]\n",
    "    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n",
    "    --dropout=<float>                       dropout [default: 0.3]\n",
    "    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n",
    "\"\"\"\n",
    "import math\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "\n",
    "\n",
    "def evaluate_ppl(model, dev_data, batch_size=32):\n",
    "    \"\"\" Evaluate perplexity on dev sentences\n",
    "    @param model (NMT): NMT Model\n",
    "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (batch size)\n",
    "    @returns ppl (perplixty on dev sentences)\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    cum_loss = 0.\n",
    "    cum_tgt_words = 0.\n",
    "\n",
    "    # no_grad() signals backend to throw away all gradients\n",
    "    with torch.no_grad():\n",
    "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
    "            loss = -model(src_sents, tgt_sents).sum()\n",
    "\n",
    "            cum_loss += loss.item()\n",
    "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
    "            cum_tgt_words += tgt_word_num_to_predict\n",
    "\n",
    "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
    "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
    "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
    "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
    "    @returns bleu_score: corpus-level BLEU score\n",
    "    \"\"\"\n",
    "    if references[0][0] == '<s>':\n",
    "        references = [ref[1:-1] for ref in references]\n",
    "    bleu_score = corpus_bleu([[ref] for ref in references],\n",
    "                             [hyp.value for hyp in hypotheses])\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def train(args: Dict):\n",
    "    \"\"\" Train the NMT Model.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "    train_data_src = read_corpus(args['--train-src'], source='src')\n",
    "    train_data_tgt = read_corpus(args['--train-tgt'], source='tgt')\n",
    "\n",
    "    dev_data_src = read_corpus(args['--dev-src'], source='src')\n",
    "    dev_data_tgt = read_corpus(args['--dev-tgt'], source='tgt')\n",
    "\n",
    "    train_data = list(zip(train_data_src, train_data_tgt))\n",
    "    dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
    "    \n",
    "    # train_data = train_data[:1500]    # A mini batch to try to overfit, to see if the model is right. Could we reduce the error to near 0?\n",
    "\n",
    "    train_batch_size = int(args['--batch-size'])\n",
    "    clip_grad = float(args['--clip-grad'])\n",
    "    valid_niter = int(args['--valid-niter'])\n",
    "    log_every = int(args['--log-every'])\n",
    "    model_save_path = args['--save-to']\n",
    "\n",
    "    vocab = Vocab.load(args['--vocab'])\n",
    "\n",
    "    model = NMT(embed_size=int(args['--embed-size']),\n",
    "                hidden_size=int(args['--hidden-size']),\n",
    "                dropout_rate=float(args['--dropout']),\n",
    "                vocab=vocab)\n",
    "    model.train()\n",
    "\n",
    "    uniform_init = float(args['--uniform-init'])\n",
    "    if np.abs(uniform_init) > 0.:\n",
    "        print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
    "        for p in model.parameters():\n",
    "            p.data.uniform_(-uniform_init, uniform_init)\n",
    "\n",
    "    vocab_mask = torch.ones(len(vocab.tgt))\n",
    "    vocab_mask[vocab.tgt['<pad>']] = 0\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if args['--cuda'] else \"cpu\")\n",
    "    print('use device: %s' % device, file=sys.stderr)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(args['--lr']))\n",
    "\n",
    "    num_trial = 0\n",
    "    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
    "    cum_examples = report_examples = epoch = valid_num = 0\n",
    "    hist_valid_scores = []\n",
    "    train_time = begin_time = time.time()\n",
    "    print('begin Maximum Likelihood training')\n",
    "\n",
    "    while True:\n",
    "        epoch += 1\n",
    "\n",
    "        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n",
    "            train_iter += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size = len(src_sents)\n",
    "\n",
    "            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n",
    "            batch_loss = example_losses.sum()\n",
    "            loss = batch_loss / batch_size\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses_val = batch_loss.item()\n",
    "            report_loss += batch_losses_val\n",
    "            cum_loss += batch_losses_val\n",
    "\n",
    "            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
    "            report_tgt_words += tgt_words_num_to_predict\n",
    "            cum_tgt_words += tgt_words_num_to_predict\n",
    "            report_examples += batch_size\n",
    "            cum_examples += batch_size\n",
    "\n",
    "            if train_iter % log_every == 0:\n",
    "                print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
    "                      'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
    "                                                                                         report_loss / report_examples,\n",
    "                                                                                         math.exp(report_loss / report_tgt_words),\n",
    "                                                                                         cum_examples,\n",
    "                                                                                         report_tgt_words / (time.time() - train_time),\n",
    "                                                                                         time.time() - begin_time), file=sys.stderr)\n",
    "\n",
    "                train_time = time.time()\n",
    "                report_loss = report_tgt_words = report_examples = 0.\n",
    "\n",
    "            # perform validation\n",
    "            if train_iter % valid_niter == 0:\n",
    "                print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
    "                                                                                         cum_loss / cum_examples,\n",
    "                                                                                         np.exp(cum_loss / cum_tgt_words),\n",
    "                                                                                         cum_examples), file=sys.stderr)\n",
    "\n",
    "                cum_loss = cum_examples = cum_tgt_words = 0.\n",
    "                valid_num += 1\n",
    "\n",
    "                print('begin validation ...', file=sys.stderr)\n",
    "\n",
    "                # compute dev. ppl and bleu\n",
    "                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
    "                valid_metric = -dev_ppl\n",
    "\n",
    "                print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n",
    "\n",
    "                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
    "                hist_valid_scores.append(valid_metric)\n",
    "\n",
    "                if is_better:\n",
    "                    patience = 0\n",
    "                    print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
    "                    model.save(model_save_path)\n",
    "\n",
    "                    # also save the optimizers' state\n",
    "                    torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
    "                elif patience < int(args['--patience']):\n",
    "                    patience += 1\n",
    "                    print('hit patience %d' % patience, file=sys.stderr)\n",
    "\n",
    "                    if patience == int(args['--patience']):\n",
    "                        num_trial += 1\n",
    "                        print('hit #%d trial' % num_trial, file=sys.stderr)\n",
    "                        if num_trial == int(args['--max-num-trial']):\n",
    "                            print('early stop!', file=sys.stderr)\n",
    "                            exit(0)\n",
    "\n",
    "                        # decay lr, and restore from previously best checkpoint\n",
    "                        lr = optimizer.param_groups[0]['lr'] * float(args['--lr-decay'])\n",
    "                        print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
    "\n",
    "                        # load model\n",
    "                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
    "                        model.load_state_dict(params['state_dict'])\n",
    "                        model = model.to(device)\n",
    "\n",
    "                        print('restore parameters of the optimizers', file=sys.stderr)\n",
    "                        optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
    "\n",
    "                        # set new lr\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "\n",
    "                        # reset patience\n",
    "                        patience = 0\n",
    "\n",
    "                if epoch == int(args['--max-epoch']):\n",
    "                    print('reached maximum number of epochs!', file=sys.stderr)\n",
    "                    raise\n",
    "\n",
    "\n",
    "def decode(args: Dict[str, str]):\n",
    "    \"\"\" Performs decoding on a test set, and save the best-scoring decoding results.\n",
    "    If the target gold-standard sentences are given, the function also computes\n",
    "    corpus-level BLEU score.\n",
    "    @param args (Dict): args from cmd line\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"load test source sentences from [{}]\".format(args['TEST_SOURCE_FILE']), file=sys.stderr)\n",
    "    test_data_src = read_corpus(args['TEST_SOURCE_FILE'], source='src')\n",
    "    if args['TEST_TARGET_FILE']:\n",
    "        print(\"load test target sentences from [{}]\".format(args['TEST_TARGET_FILE']), file=sys.stderr)\n",
    "        test_data_tgt = read_corpus(args['TEST_TARGET_FILE'], source='tgt')\n",
    "\n",
    "    print(\"load model from {}\".format(args['MODEL_PATH']), file=sys.stderr)\n",
    "    model = NMT.load(args['MODEL_PATH'])\n",
    "\n",
    "    if args['--cuda']:\n",
    "        model = model.to(torch.device(\"cuda:0\"))\n",
    "\n",
    "    hypotheses = beam_search(model, test_data_src,\n",
    "                             beam_size=int(args['--beam-size']),\n",
    "                             max_decoding_time_step=int(args['--max-decoding-time-step']))\n",
    "\n",
    "    if args['TEST_TARGET_FILE']:\n",
    "        top_hypotheses = [hyps[0] for hyps in hypotheses]\n",
    "        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n",
    "        print('Corpus BLEU: {}'.format(bleu_score * 100), file=sys.stderr)\n",
    "\n",
    "    with open(args['OUTPUT_FILE'], 'w') as f:\n",
    "        for src_sent, hyps in zip(test_data_src, hypotheses):\n",
    "            top_hyp = hyps[0]\n",
    "            hyp_sent = ' '.join(top_hyp.value)\n",
    "            f.write(hyp_sent + '\\n')\n",
    "\n",
    "\n",
    "def beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n",
    "    \"\"\" Run beam search to construct hypotheses for a list of src-language sentences.\n",
    "    @param model (NMT): NMT Model\n",
    "    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n",
    "    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n",
    "    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n",
    "    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    hypotheses = []\n",
    "    with torch.no_grad():\n",
    "        for src_sent in tqdm(test_data_src, desc='Decoding', file=sys.stdout):\n",
    "            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n",
    "\n",
    "            hypotheses.append(example_hyps)\n",
    "\n",
    "    if was_training: model.train(was_training)\n",
    "\n",
    "    return hypotheses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'--max-epoch': 10, '--lr-decay': 0.5, '--clip-grad': 5.0, '--hidden-size': 256, '--max-num-trial': 5, '--train-src': '/home/aistudio/data/data10516/train.es', '--batch-size': 32, '--seed': 0, '--valid-niter': 2000, '--uniform-init': 0.1, '--dev-src': '/home/aistudio/data/data10516/dev.es', '--cuda': True, '--dropout': 0.3, '--max-decoding-time-step': 70, '--lr': 0.001, '--beam-size': 5, '--log-every': 10, '--vocab': '/home/aistudio/data/data10516/vocab.json', '--dev-tgt': '/home/aistudio/data/data10516/dev.en', '--save-to': '/home/aistudio/work/model.bin', '--patience': 5, '--train-tgt': '/home/aistudio/data/data10516/train.en', '--embed-size': 256, '--input-feed': True, '--sample-size': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uniformly initialize parameters [-0.100000, +0.100000]\n",
      "use device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin Maximum Likelihood training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1, iter 10, avg. loss 194.18, avg. ppl 22964.15 cum. examples 320, speed 8325.85 words/sec, time elapsed 0.74 sec\n",
      "epoch 1, iter 20, avg. loss 160.31, avg. ppl 2712.09 cum. examples 640, speed 8551.00 words/sec, time elapsed 1.50 sec\n",
      "epoch 1, iter 30, avg. loss 155.54, avg. ppl 1629.10 cum. examples 960, speed 8140.31 words/sec, time elapsed 2.33 sec\n",
      "epoch 1, iter 40, avg. loss 154.50, avg. ppl 1370.21 cum. examples 1280, speed 7621.50 words/sec, time elapsed 3.23 sec\n",
      "epoch 2, iter 50, avg. loss 136.84, avg. ppl 1104.35 cum. examples 1596, speed 8236.76 words/sec, time elapsed 3.98 sec\n",
      "epoch 2, iter 60, avg. loss 137.17, avg. ppl 846.15 cum. examples 1916, speed 8068.88 words/sec, time elapsed 4.78 sec\n",
      "epoch 2, iter 70, avg. loss 132.68, avg. ppl 844.10 cum. examples 2236, speed 8562.08 words/sec, time elapsed 5.52 sec\n",
      "epoch 2, iter 80, avg. loss 138.20, avg. ppl 821.03 cum. examples 2556, speed 8710.73 words/sec, time elapsed 6.28 sec\n",
      "epoch 2, iter 90, avg. loss 136.29, avg. ppl 787.47 cum. examples 2876, speed 8718.90 words/sec, time elapsed 7.03 sec\n",
      "epoch 3, iter 100, avg. loss 138.50, avg. ppl 728.06 cum. examples 3192, speed 8749.50 words/sec, time elapsed 7.79 sec\n",
      "epoch 3, iter 110, avg. loss 129.25, avg. ppl 621.50 cum. examples 3512, speed 8605.79 words/sec, time elapsed 8.53 sec\n",
      "epoch 3, iter 120, avg. loss 136.07, avg. ppl 653.41 cum. examples 3832, speed 7113.62 words/sec, time elapsed 9.48 sec\n",
      "epoch 3, iter 130, avg. loss 126.68, avg. ppl 606.66 cum. examples 4152, speed 8694.54 words/sec, time elapsed 10.21 sec\n",
      "epoch 3, iter 140, avg. loss 132.14, avg. ppl 580.83 cum. examples 4472, speed 8249.16 words/sec, time elapsed 11.01 sec\n",
      "epoch 4, iter 150, avg. loss 125.70, avg. ppl 493.51 cum. examples 4788, speed 8483.69 words/sec, time elapsed 11.77 sec\n",
      "epoch 4, iter 160, avg. loss 124.62, avg. ppl 463.10 cum. examples 5108, speed 8390.00 words/sec, time elapsed 12.54 sec\n",
      "epoch 4, iter 170, avg. loss 126.31, avg. ppl 488.61 cum. examples 5428, speed 7564.70 words/sec, time elapsed 13.40 sec\n",
      "epoch 4, iter 180, avg. loss 122.55, avg. ppl 453.08 cum. examples 5748, speed 7191.81 words/sec, time elapsed 14.30 sec\n",
      "epoch 5, iter 190, avg. loss 129.07, avg. ppl 432.02 cum. examples 6064, speed 8857.97 words/sec, time elapsed 15.06 sec\n",
      "epoch 5, iter 200, avg. loss 120.45, avg. ppl 350.86 cum. examples 6384, speed 8672.32 words/sec, time elapsed 15.81 sec\n",
      "epoch 5, iter 210, avg. loss 118.24, avg. ppl 343.67 cum. examples 6704, speed 8366.11 words/sec, time elapsed 16.59 sec\n",
      "epoch 5, iter 220, avg. loss 114.14, avg. ppl 347.44 cum. examples 7024, speed 8576.16 words/sec, time elapsed 17.32 sec\n",
      "epoch 5, iter 230, avg. loss 121.24, avg. ppl 336.75 cum. examples 7344, speed 8803.27 words/sec, time elapsed 18.07 sec\n",
      "epoch 6, iter 240, avg. loss 114.83, avg. ppl 302.14 cum. examples 7660, speed 8779.47 words/sec, time elapsed 18.80 sec\n",
      "epoch 6, iter 250, avg. loss 116.16, avg. ppl 263.38 cum. examples 7980, speed 8715.07 words/sec, time elapsed 19.56 sec\n",
      "epoch 6, iter 260, avg. loss 111.16, avg. ppl 235.88 cum. examples 8300, speed 8861.36 words/sec, time elapsed 20.30 sec\n",
      "epoch 6, iter 270, avg. loss 111.96, avg. ppl 251.85 cum. examples 8620, speed 8684.53 words/sec, time elapsed 21.04 sec\n",
      "epoch 6, iter 280, avg. loss 115.12, avg. ppl 250.98 cum. examples 8940, speed 8598.40 words/sec, time elapsed 21.82 sec\n",
      "epoch 7, iter 290, avg. loss 108.75, avg. ppl 201.93 cum. examples 9256, speed 9002.03 words/sec, time elapsed 22.54 sec\n",
      "epoch 7, iter 300, avg. loss 102.05, avg. ppl 178.60 cum. examples 9576, speed 8517.45 words/sec, time elapsed 23.28 sec\n",
      "epoch 7, iter 310, avg. loss 103.00, avg. ppl 188.64 cum. examples 9896, speed 8484.57 words/sec, time elapsed 24.02 sec\n",
      "epoch 7, iter 320, avg. loss 110.90, avg. ppl 185.85 cum. examples 10216, speed 8655.27 words/sec, time elapsed 24.81 sec\n",
      "epoch 8, iter 330, avg. loss 107.61, avg. ppl 172.81 cum. examples 10532, speed 8227.34 words/sec, time elapsed 25.61 sec\n",
      "epoch 8, iter 340, avg. loss 100.32, avg. ppl 141.23 cum. examples 10852, speed 9102.24 words/sec, time elapsed 26.32 sec\n",
      "epoch 8, iter 350, avg. loss 102.48, avg. ppl 142.68 cum. examples 11172, speed 8810.43 words/sec, time elapsed 27.07 sec\n",
      "epoch 8, iter 360, avg. loss 97.82, avg. ppl 130.40 cum. examples 11492, speed 8606.79 words/sec, time elapsed 27.82 sec\n",
      "epoch 8, iter 370, avg. loss 101.51, avg. ppl 136.13 cum. examples 11812, speed 8393.31 words/sec, time elapsed 28.61 sec\n",
      "epoch 9, iter 380, avg. loss 98.23, avg. ppl 130.71 cum. examples 12128, speed 8103.62 words/sec, time elapsed 29.39 sec\n",
      "epoch 9, iter 390, avg. loss 93.15, avg. ppl 100.57 cum. examples 12448, speed 8628.13 words/sec, time elapsed 30.14 sec\n",
      "epoch 9, iter 400, avg. loss 97.24, avg. ppl 105.30 cum. examples 12768, speed 8819.74 words/sec, time elapsed 30.90 sec\n",
      "epoch 9, iter 410, avg. loss 93.94, avg. ppl 107.57 cum. examples 13088, speed 8404.97 words/sec, time elapsed 31.66 sec\n",
      "epoch 9, iter 420, avg. loss 96.95, avg. ppl 109.21 cum. examples 13408, speed 8767.16 words/sec, time elapsed 32.42 sec\n",
      "epoch 10, iter 430, avg. loss 91.78, avg. ppl 85.13 cum. examples 13724, speed 8602.11 words/sec, time elapsed 33.18 sec\n",
      "epoch 10, iter 440, avg. loss 89.81, avg. ppl 81.08 cum. examples 14044, speed 8425.44 words/sec, time elapsed 33.95 sec\n",
      "epoch 10, iter 450, avg. loss 83.74, avg. ppl 80.77 cum. examples 14364, speed 8379.53 words/sec, time elapsed 34.68 sec\n",
      "epoch 10, iter 460, avg. loss 89.07, avg. ppl 82.61 cum. examples 14684, speed 8432.04 words/sec, time elapsed 35.45 sec\n",
      "epoch 10, iter 470, avg. loss 95.54, avg. ppl 86.89 cum. examples 15000, speed 8328.47 words/sec, time elapsed 36.26 sec\n",
      "epoch 11, iter 480, avg. loss 87.23, avg. ppl 65.38 cum. examples 15320, speed 8388.83 words/sec, time elapsed 37.06 sec\n",
      "epoch 11, iter 490, avg. loss 80.29, avg. ppl 58.67 cum. examples 15640, speed 8576.98 words/sec, time elapsed 37.79 sec\n",
      "epoch 11, iter 500, avg. loss 87.90, avg. ppl 64.64 cum. examples 15960, speed 8350.07 words/sec, time elapsed 38.60 sec\n",
      "epoch 11, iter 510, avg. loss 83.98, avg. ppl 65.65 cum. examples 16280, speed 8267.36 words/sec, time elapsed 39.38 sec\n",
      "epoch 12, iter 520, avg. loss 81.88, avg. ppl 59.03 cum. examples 16596, speed 8429.21 words/sec, time elapsed 40.13 sec\n",
      "epoch 12, iter 530, avg. loss 84.01, avg. ppl 50.74 cum. examples 16916, speed 8692.21 words/sec, time elapsed 40.92 sec\n",
      "epoch 12, iter 540, avg. loss 77.82, avg. ppl 49.27 cum. examples 17236, speed 7879.01 words/sec, time elapsed 41.73 sec\n",
      "epoch 12, iter 550, avg. loss 79.20, avg. ppl 49.18 cum. examples 17556, speed 8526.92 words/sec, time elapsed 42.49 sec\n",
      "epoch 12, iter 560, avg. loss 81.45, avg. ppl 52.83 cum. examples 17876, speed 8750.38 words/sec, time elapsed 43.24 sec\n",
      "epoch 13, iter 570, avg. loss 77.86, avg. ppl 42.61 cum. examples 18192, speed 8644.72 words/sec, time elapsed 44.00 sec\n",
      "epoch 13, iter 580, avg. loss 68.35, avg. ppl 37.81 cum. examples 18512, speed 8315.30 words/sec, time elapsed 44.73 sec\n",
      "epoch 13, iter 590, avg. loss 80.52, avg. ppl 42.91 cum. examples 18832, speed 8598.59 words/sec, time elapsed 45.52 sec\n",
      "epoch 13, iter 600, avg. loss 74.43, avg. ppl 38.17 cum. examples 19152, speed 8882.24 words/sec, time elapsed 46.26 sec\n",
      "epoch 13, iter 610, avg. loss 75.63, avg. ppl 40.94 cum. examples 19472, speed 8665.63 words/sec, time elapsed 47.01 sec\n",
      "epoch 14, iter 620, avg. loss 72.10, avg. ppl 31.21 cum. examples 19788, speed 8422.58 words/sec, time elapsed 47.80 sec\n",
      "epoch 14, iter 630, avg. loss 70.36, avg. ppl 31.33 cum. examples 20108, speed 8548.86 words/sec, time elapsed 48.56 sec\n",
      "epoch 14, iter 640, avg. loss 69.93, avg. ppl 30.62 cum. examples 20428, speed 8606.49 words/sec, time elapsed 49.32 sec\n",
      "epoch 14, iter 650, avg. loss 70.74, avg. ppl 31.92 cum. examples 20748, speed 8283.19 words/sec, time elapsed 50.11 sec\n",
      "epoch 15, iter 660, avg. loss 67.36, avg. ppl 30.18 cum. examples 21064, speed 8276.28 words/sec, time elapsed 50.87 sec\n",
      "epoch 15, iter 670, avg. loss 67.90, avg. ppl 24.57 cum. examples 21384, speed 8400.09 words/sec, time elapsed 51.68 sec\n",
      "epoch 15, iter 680, avg. loss 64.11, avg. ppl 24.31 cum. examples 21704, speed 8467.83 words/sec, time elapsed 52.44 sec\n",
      "epoch 15, iter 690, avg. loss 65.44, avg. ppl 25.83 cum. examples 22024, speed 8424.56 words/sec, time elapsed 53.20 sec\n",
      "epoch 15, iter 700, avg. loss 67.71, avg. ppl 26.13 cum. examples 22344, speed 8338.39 words/sec, time elapsed 54.00 sec\n",
      "epoch 16, iter 710, avg. loss 57.92, avg. ppl 21.44 cum. examples 22660, speed 8777.04 words/sec, time elapsed 54.68 sec\n",
      "epoch 16, iter 720, avg. loss 57.23, avg. ppl 19.40 cum. examples 22980, speed 8219.52 words/sec, time elapsed 55.43 sec\n",
      "epoch 16, iter 730, avg. loss 65.93, avg. ppl 21.28 cum. examples 23300, speed 8656.75 words/sec, time elapsed 56.23 sec\n",
      "epoch 16, iter 740, avg. loss 63.26, avg. ppl 20.73 cum. examples 23620, speed 8807.85 words/sec, time elapsed 56.99 sec\n",
      "epoch 16, iter 750, avg. loss 62.15, avg. ppl 21.36 cum. examples 23940, speed 8560.30 words/sec, time elapsed 57.74 sec\n",
      "epoch 17, iter 760, avg. loss 58.65, avg. ppl 17.20 cum. examples 24256, speed 8513.88 words/sec, time elapsed 58.51 sec\n",
      "epoch 17, iter 770, avg. loss 60.09, avg. ppl 16.76 cum. examples 24576, speed 8602.51 words/sec, time elapsed 59.30 sec\n",
      "epoch 17, iter 780, avg. loss 56.53, avg. ppl 16.33 cum. examples 24896, speed 8390.97 words/sec, time elapsed 60.07 sec\n",
      "epoch 17, iter 790, avg. loss 56.21, avg. ppl 16.55 cum. examples 25216, speed 8290.74 words/sec, time elapsed 60.85 sec\n",
      "epoch 18, iter 800, avg. loss 54.77, avg. ppl 16.72 cum. examples 25532, speed 8622.82 words/sec, time elapsed 61.56 sec\n",
      "epoch 18, iter 810, avg. loss 55.32, avg. ppl 13.51 cum. examples 25852, speed 8246.06 words/sec, time elapsed 62.39 sec\n",
      "epoch 18, iter 820, avg. loss 50.93, avg. ppl 12.83 cum. examples 26172, speed 8885.06 words/sec, time elapsed 63.10 sec\n",
      "epoch 18, iter 830, avg. loss 50.94, avg. ppl 13.49 cum. examples 26492, speed 8161.89 words/sec, time elapsed 63.87 sec\n",
      "epoch 18, iter 840, avg. loss 56.62, avg. ppl 14.64 cum. examples 26812, speed 8825.40 words/sec, time elapsed 64.64 sec\n",
      "epoch 19, iter 850, avg. loss 52.06, avg. ppl 12.76 cum. examples 27128, speed 8934.67 words/sec, time elapsed 65.36 sec\n",
      "epoch 19, iter 860, avg. loss 48.65, avg. ppl 11.13 cum. examples 27448, speed 8924.21 words/sec, time elapsed 66.08 sec\n",
      "epoch 19, iter 870, avg. loss 47.38, avg. ppl 10.87 cum. examples 27768, speed 8323.09 words/sec, time elapsed 66.85 sec\n",
      "epoch 19, iter 880, avg. loss 51.86, avg. ppl 11.65 cum. examples 28088, speed 8454.15 words/sec, time elapsed 67.65 sec\n",
      "epoch 19, iter 890, avg. loss 50.97, avg. ppl 11.82 cum. examples 28408, speed 8535.71 words/sec, time elapsed 68.42 sec\n",
      "epoch 20, iter 900, avg. loss 46.51, avg. ppl 9.58 cum. examples 28724, speed 8684.93 words/sec, time elapsed 69.17 sec\n",
      "epoch 20, iter 910, avg. loss 45.04, avg. ppl 8.90 cum. examples 29044, speed 8820.59 words/sec, time elapsed 69.92 sec\n",
      "epoch 20, iter 920, avg. loss 45.77, avg. ppl 9.77 cum. examples 29364, speed 8812.91 words/sec, time elapsed 70.65 sec\n",
      "epoch 20, iter 930, avg. loss 46.40, avg. ppl 9.82 cum. examples 29684, speed 8508.40 words/sec, time elapsed 71.41 sec\n",
      "epoch 20, iter 940, avg. loss 45.68, avg. ppl 9.81 cum. examples 30000, speed 8333.67 words/sec, time elapsed 72.17 sec\n",
      "epoch 21, iter 950, avg. loss 39.85, avg. ppl 7.67 cum. examples 30320, speed 8722.71 words/sec, time elapsed 72.89 sec\n",
      "epoch 21, iter 960, avg. loss 45.61, avg. ppl 8.23 cum. examples 30640, speed 8791.48 words/sec, time elapsed 73.68 sec\n",
      "epoch 21, iter 970, avg. loss 42.04, avg. ppl 8.17 cum. examples 30960, speed 8423.98 words/sec, time elapsed 74.44 sec\n",
      "epoch 21, iter 980, avg. loss 42.84, avg. ppl 8.29 cum. examples 31280, speed 8485.91 words/sec, time elapsed 75.20 sec\n",
      "epoch 22, iter 990, avg. loss 43.30, avg. ppl 7.94 cum. examples 31596, speed 8415.56 words/sec, time elapsed 75.99 sec\n",
      "epoch 22, iter 1000, avg. loss 35.56, avg. ppl 6.40 cum. examples 31916, speed 8100.67 words/sec, time elapsed 76.74 sec\n",
      "epoch 22, iter 1010, avg. loss 40.58, avg. ppl 6.98 cum. examples 32236, speed 8612.30 words/sec, time elapsed 77.52 sec\n",
      "epoch 22, iter 1020, avg. loss 40.02, avg. ppl 7.09 cum. examples 32556, speed 8496.40 words/sec, time elapsed 78.29 sec\n",
      "epoch 22, iter 1030, avg. loss 41.39, avg. ppl 7.44 cum. examples 32876, speed 8711.62 words/sec, time elapsed 79.05 sec\n",
      "epoch 23, iter 1040, avg. loss 39.94, avg. ppl 6.48 cum. examples 33192, speed 8528.28 words/sec, time elapsed 79.84 sec\n",
      "epoch 23, iter 1050, avg. loss 38.58, avg. ppl 6.08 cum. examples 33512, speed 8932.96 words/sec, time elapsed 80.61 sec\n",
      "epoch 23, iter 1060, avg. loss 34.17, avg. ppl 5.82 cum. examples 33832, speed 7881.57 words/sec, time elapsed 81.39 sec\n",
      "epoch 23, iter 1070, avg. loss 36.75, avg. ppl 6.06 cum. examples 34152, speed 8571.62 words/sec, time elapsed 82.16 sec\n",
      "epoch 23, iter 1080, avg. loss 35.41, avg. ppl 6.17 cum. examples 34472, speed 8030.76 words/sec, time elapsed 82.93 sec\n",
      "epoch 24, iter 1090, avg. loss 31.92, avg. ppl 5.13 cum. examples 34788, speed 8079.69 words/sec, time elapsed 83.69 sec\n",
      "epoch 24, iter 1100, avg. loss 36.39, avg. ppl 5.47 cum. examples 35108, speed 8993.74 words/sec, time elapsed 84.46 sec\n",
      "epoch 24, iter 1110, avg. loss 33.72, avg. ppl 5.26 cum. examples 35428, speed 8537.07 words/sec, time elapsed 85.22 sec\n",
      "epoch 24, iter 1120, avg. loss 35.31, avg. ppl 5.58 cum. examples 35748, speed 8424.34 words/sec, time elapsed 86.00 sec\n",
      "epoch 25, iter 1130, avg. loss 35.03, avg. ppl 5.61 cum. examples 36064, speed 8429.25 words/sec, time elapsed 86.76 sec\n",
      "epoch 25, iter 1140, avg. loss 31.93, avg. ppl 4.60 cum. examples 36384, speed 9067.92 words/sec, time elapsed 87.50 sec\n",
      "epoch 25, iter 1150, avg. loss 30.09, avg. ppl 4.75 cum. examples 36704, speed 8715.51 words/sec, time elapsed 88.21 sec\n",
      "epoch 25, iter 1160, avg. loss 31.01, avg. ppl 4.72 cum. examples 37024, speed 8272.22 words/sec, time elapsed 88.98 sec\n",
      "epoch 25, iter 1170, avg. loss 32.41, avg. ppl 4.80 cum. examples 37344, speed 8688.04 words/sec, time elapsed 89.74 sec\n",
      "epoch 26, iter 1180, avg. loss 30.61, avg. ppl 4.48 cum. examples 37660, speed 8480.01 words/sec, time elapsed 90.50 sec\n",
      "epoch 26, iter 1190, avg. loss 28.47, avg. ppl 4.11 cum. examples 37980, speed 8565.36 words/sec, time elapsed 91.25 sec\n",
      "epoch 26, iter 1200, avg. loss 30.79, avg. ppl 4.36 cum. examples 38300, speed 8676.90 words/sec, time elapsed 92.02 sec\n",
      "epoch 26, iter 1210, avg. loss 30.45, avg. ppl 4.34 cum. examples 38620, speed 8539.67 words/sec, time elapsed 92.80 sec\n",
      "epoch 26, iter 1220, avg. loss 30.52, avg. ppl 4.49 cum. examples 38940, speed 8538.88 words/sec, time elapsed 93.56 sec\n",
      "epoch 27, iter 1230, avg. loss 26.99, avg. ppl 3.80 cum. examples 39256, speed 8353.17 words/sec, time elapsed 94.33 sec\n",
      "epoch 27, iter 1240, avg. loss 29.27, avg. ppl 3.98 cum. examples 39576, speed 8722.93 words/sec, time elapsed 95.11 sec\n",
      "epoch 27, iter 1250, avg. loss 27.40, avg. ppl 3.86 cum. examples 39896, speed 9081.83 words/sec, time elapsed 95.82 sec\n",
      "epoch 27, iter 1260, avg. loss 27.49, avg. ppl 4.01 cum. examples 40216, speed 8498.40 words/sec, time elapsed 96.57 sec\n",
      "epoch 28, iter 1270, avg. loss 28.37, avg. ppl 3.96 cum. examples 40532, speed 8990.31 words/sec, time elapsed 97.29 sec\n",
      "epoch 28, iter 1280, avg. loss 22.94, avg. ppl 3.32 cum. examples 40852, speed 8748.72 words/sec, time elapsed 97.99 sec\n",
      "epoch 28, iter 1290, avg. loss 25.31, avg. ppl 3.49 cum. examples 41172, speed 8316.94 words/sec, time elapsed 98.77 sec\n",
      "epoch 28, iter 1300, avg. loss 28.34, avg. ppl 3.76 cum. examples 41492, speed 8833.44 words/sec, time elapsed 99.55 sec\n",
      "epoch 28, iter 1310, avg. loss 25.60, avg. ppl 3.56 cum. examples 41812, speed 8482.51 words/sec, time elapsed 100.31 sec\n",
      "epoch 29, iter 1320, avg. loss 26.18, avg. ppl 3.53 cum. examples 42128, speed 8512.45 words/sec, time elapsed 101.08 sec\n",
      "epoch 29, iter 1330, avg. loss 23.45, avg. ppl 3.21 cum. examples 42448, speed 8482.44 words/sec, time elapsed 101.84 sec\n",
      "epoch 29, iter 1340, avg. loss 23.78, avg. ppl 3.21 cum. examples 42768, speed 8276.85 words/sec, time elapsed 102.62 sec\n",
      "epoch 29, iter 1350, avg. loss 24.30, avg. ppl 3.33 cum. examples 43088, speed 8247.84 words/sec, time elapsed 103.41 sec\n",
      "epoch 29, iter 1360, avg. loss 25.53, avg. ppl 3.47 cum. examples 43408, speed 8570.16 words/sec, time elapsed 104.17 sec\n",
      "epoch 30, iter 1370, avg. loss 22.21, avg. ppl 3.06 cum. examples 43724, speed 8815.19 words/sec, time elapsed 104.88 sec\n",
      "epoch 30, iter 1380, avg. loss 22.63, avg. ppl 3.01 cum. examples 44044, speed 8847.40 words/sec, time elapsed 105.63 sec\n",
      "epoch 30, iter 1390, avg. loss 23.33, avg. ppl 3.07 cum. examples 44364, speed 8738.10 words/sec, time elapsed 106.39 sec\n",
      "epoch 30, iter 1400, avg. loss 24.15, avg. ppl 3.15 cum. examples 44684, speed 8658.78 words/sec, time elapsed 107.17 sec\n",
      "epoch 30, iter 1410, avg. loss 23.10, avg. ppl 3.12 cum. examples 45000, speed 8389.76 words/sec, time elapsed 107.93 sec\n",
      "epoch 31, iter 1420, avg. loss 20.06, avg. ppl 2.74 cum. examples 45320, speed 8366.66 words/sec, time elapsed 108.69 sec\n",
      "epoch 31, iter 1430, avg. loss 20.94, avg. ppl 2.85 cum. examples 45640, speed 8508.30 words/sec, time elapsed 109.44 sec\n",
      "epoch 31, iter 1440, avg. loss 21.77, avg. ppl 2.88 cum. examples 45960, speed 8426.43 words/sec, time elapsed 110.22 sec\n",
      "epoch 31, iter 1450, avg. loss 23.29, avg. ppl 2.98 cum. examples 46280, speed 8019.62 words/sec, time elapsed 111.07 sec\n",
      "epoch 32, iter 1460, avg. loss 20.34, avg. ppl 2.80 cum. examples 46596, speed 7721.59 words/sec, time elapsed 111.88 sec\n",
      "epoch 32, iter 1470, avg. loss 19.60, avg. ppl 2.62 cum. examples 46916, speed 7782.95 words/sec, time elapsed 112.72 sec\n",
      "epoch 32, iter 1480, avg. loss 19.28, avg. ppl 2.63 cum. examples 47236, speed 8494.29 words/sec, time elapsed 113.47 sec\n",
      "epoch 32, iter 1490, avg. loss 19.88, avg. ppl 2.74 cum. examples 47556, speed 8501.74 words/sec, time elapsed 114.21 sec\n",
      "epoch 32, iter 1500, avg. loss 22.23, avg. ppl 2.79 cum. examples 47876, speed 9260.85 words/sec, time elapsed 114.96 sec\n",
      "epoch 33, iter 1510, avg. loss 18.39, avg. ppl 2.47 cum. examples 48192, speed 8447.14 words/sec, time elapsed 115.72 sec\n",
      "epoch 33, iter 1520, avg. loss 18.33, avg. ppl 2.44 cum. examples 48512, speed 8437.95 words/sec, time elapsed 116.50 sec\n",
      "epoch 33, iter 1530, avg. loss 19.99, avg. ppl 2.61 cum. examples 48832, speed 8887.91 words/sec, time elapsed 117.25 sec\n",
      "epoch 33, iter 1540, avg. loss 19.60, avg. ppl 2.60 cum. examples 49152, speed 8288.41 words/sec, time elapsed 118.04 sec\n",
      "epoch 33, iter 1550, avg. loss 19.13, avg. ppl 2.57 cum. examples 49472, speed 8623.09 words/sec, time elapsed 118.80 sec\n",
      "epoch 34, iter 1560, avg. loss 15.82, avg. ppl 2.28 cum. examples 49788, speed 8227.43 words/sec, time elapsed 119.53 sec\n",
      "epoch 34, iter 1570, avg. loss 18.69, avg. ppl 2.41 cum. examples 50108, speed 8935.53 words/sec, time elapsed 120.30 sec\n",
      "epoch 34, iter 1580, avg. loss 19.06, avg. ppl 2.45 cum. examples 50428, speed 8377.44 words/sec, time elapsed 121.11 sec\n",
      "epoch 34, iter 1590, avg. loss 17.55, avg. ppl 2.42 cum. examples 50748, speed 8255.54 words/sec, time elapsed 121.88 sec\n",
      "epoch 35, iter 1600, avg. loss 17.23, avg. ppl 2.33 cum. examples 51064, speed 8268.65 words/sec, time elapsed 122.66 sec\n",
      "epoch 35, iter 1610, avg. loss 14.98, avg. ppl 2.15 cum. examples 51384, speed 8332.93 words/sec, time elapsed 123.41 sec\n",
      "epoch 35, iter 1620, avg. loss 16.27, avg. ppl 2.21 cum. examples 51704, speed 8994.74 words/sec, time elapsed 124.14 sec\n",
      "epoch 35, iter 1630, avg. loss 15.89, avg. ppl 2.20 cum. examples 52024, speed 8502.45 words/sec, time elapsed 124.90 sec\n",
      "epoch 35, iter 1640, avg. loss 18.30, avg. ppl 2.40 cum. examples 52344, speed 8951.08 words/sec, time elapsed 125.65 sec\n",
      "epoch 36, iter 1650, avg. loss 18.00, avg. ppl 2.34 cum. examples 52660, speed 8854.85 words/sec, time elapsed 126.40 sec\n",
      "epoch 36, iter 1660, avg. loss 14.55, avg. ppl 2.06 cum. examples 52980, speed 8759.57 words/sec, time elapsed 127.13 sec\n",
      "epoch 36, iter 1670, avg. loss 14.60, avg. ppl 2.07 cum. examples 53300, speed 8481.13 words/sec, time elapsed 127.89 sec\n",
      "epoch 36, iter 1680, avg. loss 15.63, avg. ppl 2.19 cum. examples 53620, speed 8580.01 words/sec, time elapsed 128.64 sec\n",
      "epoch 36, iter 1690, avg. loss 16.57, avg. ppl 2.21 cum. examples 53940, speed 8567.13 words/sec, time elapsed 129.42 sec\n",
      "epoch 37, iter 1700, avg. loss 15.16, avg. ppl 2.05 cum. examples 54256, speed 8610.16 words/sec, time elapsed 130.19 sec\n",
      "epoch 37, iter 1710, avg. loss 13.60, avg. ppl 2.01 cum. examples 54576, speed 8688.34 words/sec, time elapsed 130.91 sec\n",
      "epoch 37, iter 1720, avg. loss 13.54, avg. ppl 1.99 cum. examples 54896, speed 8648.86 words/sec, time elapsed 131.64 sec\n",
      "epoch 37, iter 1730, avg. loss 16.03, avg. ppl 2.14 cum. examples 55216, speed 8963.62 words/sec, time elapsed 132.39 sec\n",
      "epoch 38, iter 1740, avg. loss 14.90, avg. ppl 2.08 cum. examples 55532, speed 8273.69 words/sec, time elapsed 133.17 sec\n",
      "epoch 38, iter 1750, avg. loss 14.70, avg. ppl 1.98 cum. examples 55852, speed 9036.74 words/sec, time elapsed 133.93 sec\n",
      "epoch 38, iter 1760, avg. loss 13.79, avg. ppl 1.96 cum. examples 56172, speed 8828.59 words/sec, time elapsed 134.68 sec\n",
      "epoch 38, iter 1770, avg. loss 14.07, avg. ppl 1.98 cum. examples 56492, speed 8598.07 words/sec, time elapsed 135.44 sec\n",
      "epoch 38, iter 1780, avg. loss 12.62, avg. ppl 1.89 cum. examples 56812, speed 7886.42 words/sec, time elapsed 136.24 sec\n",
      "epoch 39, iter 1790, avg. loss 13.21, avg. ppl 1.95 cum. examples 57128, speed 7613.04 words/sec, time elapsed 137.07 sec\n",
      "epoch 39, iter 1800, avg. loss 12.10, avg. ppl 1.82 cum. examples 57448, speed 7986.68 words/sec, time elapsed 137.88 sec\n",
      "epoch 39, iter 1810, avg. loss 13.88, avg. ppl 1.92 cum. examples 57768, speed 8598.05 words/sec, time elapsed 138.67 sec\n",
      "epoch 39, iter 1820, avg. loss 12.85, avg. ppl 1.89 cum. examples 58088, speed 8217.15 words/sec, time elapsed 139.46 sec\n",
      "epoch 39, iter 1830, avg. loss 13.14, avg. ppl 1.95 cum. examples 58408, speed 8146.59 words/sec, time elapsed 140.23 sec\n",
      "epoch 40, iter 1840, avg. loss 12.88, avg. ppl 1.87 cum. examples 58724, speed 8851.41 words/sec, time elapsed 140.97 sec\n",
      "epoch 40, iter 1850, avg. loss 11.22, avg. ppl 1.74 cum. examples 59044, speed 8754.54 words/sec, time elapsed 141.71 sec\n",
      "epoch 40, iter 1860, avg. loss 11.57, avg. ppl 1.78 cum. examples 59364, speed 8344.15 words/sec, time elapsed 142.48 sec\n",
      "epoch 40, iter 1870, avg. loss 13.10, avg. ppl 1.86 cum. examples 59684, speed 8481.23 words/sec, time elapsed 143.27 sec\n",
      "epoch 40, iter 1880, avg. loss 12.36, avg. ppl 1.85 cum. examples 60000, speed 8694.92 words/sec, time elapsed 144.00 sec\n",
      "epoch 41, iter 1890, avg. loss 10.63, avg. ppl 1.70 cum. examples 60320, speed 9021.23 words/sec, time elapsed 144.71 sec\n",
      "epoch 41, iter 1900, avg. loss 11.35, avg. ppl 1.74 cum. examples 60640, speed 8797.83 words/sec, time elapsed 145.46 sec\n",
      "epoch 41, iter 1910, avg. loss 12.45, avg. ppl 1.80 cum. examples 60960, speed 8691.83 words/sec, time elapsed 146.24 sec\n",
      "epoch 41, iter 1920, avg. loss 11.43, avg. ppl 1.76 cum. examples 61280, speed 8386.99 words/sec, time elapsed 147.01 sec\n",
      "epoch 42, iter 1930, avg. loss 11.76, avg. ppl 1.77 cum. examples 61596, speed 8384.87 words/sec, time elapsed 147.79 sec\n",
      "epoch 42, iter 1940, avg. loss 10.36, avg. ppl 1.66 cum. examples 61916, speed 8673.15 words/sec, time elapsed 148.54 sec\n",
      "epoch 42, iter 1950, avg. loss 9.27, avg. ppl 1.64 cum. examples 62236, speed 8290.49 words/sec, time elapsed 149.27 sec\n",
      "epoch 42, iter 1960, avg. loss 12.49, avg. ppl 1.76 cum. examples 62556, speed 8702.47 words/sec, time elapsed 150.08 sec\n",
      "epoch 42, iter 1970, avg. loss 10.70, avg. ppl 1.71 cum. examples 62876, speed 8355.65 words/sec, time elapsed 150.84 sec\n",
      "epoch 43, iter 1980, avg. loss 10.52, avg. ppl 1.66 cum. examples 63192, speed 8108.06 words/sec, time elapsed 151.65 sec\n",
      "epoch 43, iter 1990, avg. loss 9.97, avg. ppl 1.63 cum. examples 63512, speed 8353.22 words/sec, time elapsed 152.43 sec\n",
      "epoch 43, iter 2000, avg. loss 10.14, avg. ppl 1.67 cum. examples 63832, speed 8184.08 words/sec, time elapsed 153.21 sec\n",
      "epoch 43, iter 2000, cum. loss 54.77, cum. ppl 14.67 cum. examples 63832\n",
      "begin validation ...\n",
      "validation: iter 2000, dev. ppl 12497.469741\n",
      "save currently the best model to [/home/aistudio/work/model.bin]\n",
      "save model parameters to [/home/aistudio/work/model.bin]\n",
      "epoch 43, iter 2010, avg. loss 10.72, avg. ppl 1.67 cum. examples 320, speed 1062.09 words/sec, time elapsed 159.53 sec\n",
      "epoch 43, iter 2020, avg. loss 9.49, avg. ppl 1.63 cum. examples 640, speed 8446.25 words/sec, time elapsed 160.27 sec\n",
      "epoch 44, iter 2030, avg. loss 9.92, avg. ppl 1.60 cum. examples 956, speed 8236.25 words/sec, time elapsed 161.07 sec\n",
      "epoch 44, iter 2040, avg. loss 8.73, avg. ppl 1.57 cum. examples 1276, speed 8422.49 words/sec, time elapsed 161.81 sec\n",
      "epoch 44, iter 2050, avg. loss 9.44, avg. ppl 1.60 cum. examples 1596, speed 8115.15 words/sec, time elapsed 162.60 sec\n",
      "epoch 44, iter 2060, avg. loss 9.56, avg. ppl 1.59 cum. examples 1916, speed 8794.85 words/sec, time elapsed 163.35 sec\n",
      "epoch 45, iter 2070, avg. loss 10.38, avg. ppl 1.63 cum. examples 2232, speed 8676.58 words/sec, time elapsed 164.12 sec\n",
      "epoch 45, iter 2080, avg. loss 8.47, avg. ppl 1.53 cum. examples 2552, speed 8725.53 words/sec, time elapsed 164.85 sec\n",
      "epoch 45, iter 2090, avg. loss 9.38, avg. ppl 1.56 cum. examples 2872, speed 9183.74 words/sec, time elapsed 165.59 sec\n",
      "epoch 45, iter 2100, avg. loss 8.94, avg. ppl 1.55 cum. examples 3192, speed 8890.93 words/sec, time elapsed 166.31 sec\n",
      "epoch 45, iter 2110, avg. loss 8.41, avg. ppl 1.54 cum. examples 3512, speed 8428.58 words/sec, time elapsed 167.06 sec\n",
      "epoch 46, iter 2120, avg. loss 9.90, avg. ppl 1.58 cum. examples 3828, speed 8829.32 words/sec, time elapsed 167.83 sec\n",
      "epoch 46, iter 2130, avg. loss 7.89, avg. ppl 1.48 cum. examples 4148, speed 8437.86 words/sec, time elapsed 168.59 sec\n",
      "epoch 46, iter 2140, avg. loss 7.75, avg. ppl 1.48 cum. examples 4468, speed 8380.99 words/sec, time elapsed 169.35 sec\n",
      "epoch 46, iter 2150, avg. loss 8.99, avg. ppl 1.55 cum. examples 4788, speed 8448.77 words/sec, time elapsed 170.13 sec\n",
      "epoch 46, iter 2160, avg. loss 9.66, avg. ppl 1.58 cum. examples 5108, speed 8716.21 words/sec, time elapsed 170.90 sec\n",
      "epoch 47, iter 2170, avg. loss 8.06, avg. ppl 1.49 cum. examples 5424, speed 8665.98 words/sec, time elapsed 171.64 sec\n",
      "epoch 47, iter 2180, avg. loss 7.75, avg. ppl 1.46 cum. examples 5744, speed 7859.58 words/sec, time elapsed 172.47 sec\n",
      "epoch 47, iter 2190, avg. loss 8.44, avg. ppl 1.49 cum. examples 6064, speed 8903.00 words/sec, time elapsed 173.22 sec\n",
      "epoch 47, iter 2200, avg. loss 8.07, avg. ppl 1.51 cum. examples 6384, speed 8458.29 words/sec, time elapsed 173.97 sec\n",
      "epoch 48, iter 2210, avg. loss 8.10, avg. ppl 1.48 cum. examples 6700, speed 8438.68 words/sec, time elapsed 174.73 sec\n",
      "epoch 48, iter 2220, avg. loss 7.99, avg. ppl 1.46 cum. examples 7020, speed 8747.35 words/sec, time elapsed 175.51 sec\n",
      "epoch 48, iter 2230, avg. loss 7.61, avg. ppl 1.46 cum. examples 7340, speed 7761.14 words/sec, time elapsed 176.33 sec\n",
      "epoch 48, iter 2240, avg. loss 7.52, avg. ppl 1.46 cum. examples 7660, speed 8270.37 words/sec, time elapsed 177.10 sec\n",
      "epoch 48, iter 2250, avg. loss 8.26, avg. ppl 1.49 cum. examples 7980, speed 8635.22 words/sec, time elapsed 177.87 sec\n",
      "epoch 49, iter 2260, avg. loss 6.85, avg. ppl 1.41 cum. examples 8296, speed 8171.38 words/sec, time elapsed 178.65 sec\n",
      "epoch 49, iter 2270, avg. loss 7.52, avg. ppl 1.43 cum. examples 8616, speed 8901.18 words/sec, time elapsed 179.40 sec\n",
      "epoch 49, iter 2280, avg. loss 7.50, avg. ppl 1.43 cum. examples 8936, speed 8542.62 words/sec, time elapsed 180.18 sec\n",
      "epoch 49, iter 2290, avg. loss 7.93, avg. ppl 1.45 cum. examples 9256, speed 8038.87 words/sec, time elapsed 181.03 sec\n",
      "epoch 49, iter 2300, avg. loss 6.78, avg. ppl 1.43 cum. examples 9576, speed 8565.23 words/sec, time elapsed 181.74 sec\n",
      "epoch 50, iter 2310, avg. loss 6.39, avg. ppl 1.39 cum. examples 9892, speed 8418.60 words/sec, time elapsed 182.48 sec\n",
      "epoch 50, iter 2320, avg. loss 7.44, avg. ppl 1.42 cum. examples 10212, speed 8740.88 words/sec, time elapsed 183.26 sec\n",
      "epoch 50, iter 2330, avg. loss 7.02, avg. ppl 1.41 cum. examples 10532, speed 8388.54 words/sec, time elapsed 184.04 sec\n",
      "epoch 50, iter 2340, avg. loss 6.64, avg. ppl 1.40 cum. examples 10852, speed 8431.59 words/sec, time elapsed 184.78 sec\n",
      "epoch 50, iter 2350, avg. loss 7.30, avg. ppl 1.43 cum. examples 11168, speed 8906.66 words/sec, time elapsed 185.51 sec\n",
      "epoch 51, iter 2360, avg. loss 6.21, avg. ppl 1.37 cum. examples 11488, speed 8393.83 words/sec, time elapsed 186.26 sec\n",
      "epoch 51, iter 2370, avg. loss 6.35, avg. ppl 1.37 cum. examples 11808, speed 8302.53 words/sec, time elapsed 187.05 sec\n",
      "epoch 51, iter 2380, avg. loss 6.45, avg. ppl 1.37 cum. examples 12128, speed 8597.75 words/sec, time elapsed 187.80 sec\n",
      "epoch 51, iter 2390, avg. loss 6.90, avg. ppl 1.39 cum. examples 12448, speed 8544.14 words/sec, time elapsed 188.58 sec\n",
      "epoch 52, iter 2400, avg. loss 6.12, avg. ppl 1.35 cum. examples 12764, speed 9091.13 words/sec, time elapsed 189.29 sec\n",
      "epoch 52, iter 2410, avg. loss 5.72, avg. ppl 1.33 cum. examples 13084, speed 8585.12 words/sec, time elapsed 190.04 sec\n",
      "epoch 52, iter 2420, avg. loss 6.25, avg. ppl 1.35 cum. examples 13404, speed 8468.77 words/sec, time elapsed 190.82 sec\n",
      "epoch 52, iter 2430, avg. loss 6.68, avg. ppl 1.39 cum. examples 13724, speed 8603.11 words/sec, time elapsed 191.58 sec\n",
      "epoch 52, iter 2440, avg. loss 6.46, avg. ppl 1.37 cum. examples 14044, speed 9021.58 words/sec, time elapsed 192.30 sec\n",
      "epoch 53, iter 2450, avg. loss 5.75, avg. ppl 1.32 cum. examples 14360, speed 9207.85 words/sec, time elapsed 193.01 sec\n",
      "epoch 53, iter 2460, avg. loss 6.14, avg. ppl 1.34 cum. examples 14680, speed 8589.68 words/sec, time elapsed 193.78 sec\n",
      "epoch 53, iter 2470, avg. loss 6.18, avg. ppl 1.35 cum. examples 15000, speed 9036.41 words/sec, time elapsed 194.52 sec\n",
      "epoch 53, iter 2480, avg. loss 5.92, avg. ppl 1.34 cum. examples 15320, speed 8637.95 words/sec, time elapsed 195.27 sec\n",
      "epoch 53, iter 2490, avg. loss 5.91, avg. ppl 1.35 cum. examples 15640, speed 8718.75 words/sec, time elapsed 196.00 sec\n",
      "epoch 54, iter 2500, avg. loss 5.65, avg. ppl 1.30 cum. examples 15956, speed 9016.68 words/sec, time elapsed 196.75 sec\n",
      "epoch 54, iter 2510, avg. loss 5.39, avg. ppl 1.31 cum. examples 16276, speed 8142.21 words/sec, time elapsed 197.54 sec\n",
      "epoch 54, iter 2520, avg. loss 5.23, avg. ppl 1.30 cum. examples 16596, speed 8432.33 words/sec, time elapsed 198.29 sec\n",
      "epoch 54, iter 2530, avg. loss 5.94, avg. ppl 1.34 cum. examples 16916, speed 8914.98 words/sec, time elapsed 199.03 sec\n",
      "epoch 55, iter 2540, avg. loss 5.93, avg. ppl 1.34 cum. examples 17232, speed 8503.44 words/sec, time elapsed 199.77 sec\n",
      "epoch 55, iter 2550, avg. loss 4.88, avg. ppl 1.27 cum. examples 17552, speed 8717.67 words/sec, time elapsed 200.52 sec\n",
      "epoch 55, iter 2560, avg. loss 4.70, avg. ppl 1.27 cum. examples 17872, speed 8500.12 words/sec, time elapsed 201.25 sec\n",
      "epoch 55, iter 2570, avg. loss 5.51, avg. ppl 1.31 cum. examples 18192, speed 8497.76 words/sec, time elapsed 202.01 sec\n",
      "epoch 55, iter 2580, avg. loss 5.87, avg. ppl 1.33 cum. examples 18512, speed 8345.73 words/sec, time elapsed 202.81 sec\n",
      "epoch 56, iter 2590, avg. loss 5.64, avg. ppl 1.31 cum. examples 18828, speed 8665.07 words/sec, time elapsed 203.58 sec\n",
      "epoch 56, iter 2600, avg. loss 5.06, avg. ppl 1.28 cum. examples 19148, speed 8666.60 words/sec, time elapsed 204.34 sec\n",
      "epoch 56, iter 2610, avg. loss 5.45, avg. ppl 1.30 cum. examples 19468, speed 9045.81 words/sec, time elapsed 205.07 sec\n",
      "epoch 56, iter 2620, avg. loss 5.05, avg. ppl 1.29 cum. examples 19788, speed 7877.48 words/sec, time elapsed 205.88 sec\n",
      "epoch 56, iter 2630, avg. loss 5.68, avg. ppl 1.31 cum. examples 20108, speed 8785.64 words/sec, time elapsed 206.64 sec\n",
      "epoch 57, iter 2640, avg. loss 4.56, avg. ppl 1.26 cum. examples 20424, speed 8671.77 words/sec, time elapsed 207.36 sec\n",
      "epoch 57, iter 2650, avg. loss 5.47, avg. ppl 1.29 cum. examples 20744, speed 7864.15 words/sec, time elapsed 208.24 sec\n",
      "epoch 57, iter 2660, avg. loss 4.61, avg. ppl 1.26 cum. examples 21064, speed 7898.01 words/sec, time elapsed 209.05 sec\n",
      "epoch 57, iter 2670, avg. loss 4.55, avg. ppl 1.26 cum. examples 21384, speed 8572.63 words/sec, time elapsed 209.79 sec\n",
      "epoch 58, iter 2680, avg. loss 5.01, avg. ppl 1.27 cum. examples 21700, speed 8314.68 words/sec, time elapsed 210.58 sec\n",
      "epoch 58, iter 2690, avg. loss 4.67, avg. ppl 1.24 cum. examples 22020, speed 9095.31 words/sec, time elapsed 211.34 sec\n",
      "epoch 58, iter 2700, avg. loss 4.25, avg. ppl 1.24 cum. examples 22340, speed 8783.66 words/sec, time elapsed 212.06 sec\n",
      "epoch 58, iter 2710, avg. loss 4.55, avg. ppl 1.26 cum. examples 22660, speed 8633.99 words/sec, time elapsed 212.79 sec\n",
      "epoch 58, iter 2720, avg. loss 4.56, avg. ppl 1.26 cum. examples 22980, speed 8443.37 words/sec, time elapsed 213.54 sec\n",
      "epoch 59, iter 2730, avg. loss 5.25, avg. ppl 1.28 cum. examples 23296, speed 8600.33 words/sec, time elapsed 214.33 sec\n",
      "epoch 59, iter 2740, avg. loss 4.23, avg. ppl 1.23 cum. examples 23616, speed 8738.97 words/sec, time elapsed 215.08 sec\n",
      "epoch 59, iter 2750, avg. loss 4.68, avg. ppl 1.25 cum. examples 23936, speed 9023.35 words/sec, time elapsed 215.82 sec\n",
      "epoch 59, iter 2760, avg. loss 4.58, avg. ppl 1.25 cum. examples 24256, speed 8728.11 words/sec, time elapsed 216.56 sec\n",
      "epoch 59, iter 2770, avg. loss 4.41, avg. ppl 1.24 cum. examples 24576, speed 8457.55 words/sec, time elapsed 217.33 sec\n",
      "epoch 60, iter 2780, avg. loss 4.07, avg. ppl 1.23 cum. examples 24892, speed 7961.95 words/sec, time elapsed 218.10 sec\n",
      "epoch 60, iter 2790, avg. loss 4.10, avg. ppl 1.22 cum. examples 25212, speed 7690.65 words/sec, time elapsed 218.95 sec\n",
      "epoch 60, iter 2800, avg. loss 4.37, avg. ppl 1.24 cum. examples 25532, speed 7654.82 words/sec, time elapsed 219.80 sec\n",
      "epoch 60, iter 2810, avg. loss 4.54, avg. ppl 1.24 cum. examples 25852, speed 8050.81 words/sec, time elapsed 220.63 sec\n",
      "epoch 60, iter 2820, avg. loss 4.34, avg. ppl 1.24 cum. examples 26168, speed 8474.61 words/sec, time elapsed 221.38 sec\n",
      "epoch 61, iter 2830, avg. loss 4.11, avg. ppl 1.21 cum. examples 26488, speed 9028.11 words/sec, time elapsed 222.14 sec\n",
      "epoch 61, iter 2840, avg. loss 4.41, avg. ppl 1.23 cum. examples 26808, speed 8659.22 words/sec, time elapsed 222.93 sec\n",
      "epoch 61, iter 2850, avg. loss 3.73, avg. ppl 1.22 cum. examples 27128, speed 8469.44 words/sec, time elapsed 223.64 sec\n",
      "epoch 61, iter 2860, avg. loss 4.13, avg. ppl 1.23 cum. examples 27448, speed 8658.90 words/sec, time elapsed 224.37 sec\n",
      "epoch 62, iter 2870, avg. loss 4.00, avg. ppl 1.22 cum. examples 27764, speed 8652.11 words/sec, time elapsed 225.12 sec\n",
      "epoch 62, iter 2880, avg. loss 4.00, avg. ppl 1.21 cum. examples 28084, speed 8499.56 words/sec, time elapsed 225.90 sec\n",
      "epoch 62, iter 2890, avg. loss 3.66, avg. ppl 1.20 cum. examples 28404, speed 8410.99 words/sec, time elapsed 226.67 sec\n",
      "epoch 62, iter 2900, avg. loss 4.13, avg. ppl 1.22 cum. examples 28724, speed 8838.77 words/sec, time elapsed 227.42 sec\n",
      "epoch 62, iter 2910, avg. loss 3.79, avg. ppl 1.21 cum. examples 29044, speed 8048.50 words/sec, time elapsed 228.21 sec\n",
      "epoch 63, iter 2920, avg. loss 3.73, avg. ppl 1.20 cum. examples 29360, speed 8240.26 words/sec, time elapsed 228.99 sec\n",
      "epoch 63, iter 2930, avg. loss 4.07, avg. ppl 1.21 cum. examples 29680, speed 8586.00 words/sec, time elapsed 229.79 sec\n",
      "epoch 63, iter 2940, avg. loss 3.93, avg. ppl 1.21 cum. examples 30000, speed 8563.58 words/sec, time elapsed 230.56 sec\n",
      "epoch 63, iter 2950, avg. loss 3.75, avg. ppl 1.21 cum. examples 30320, speed 8055.33 words/sec, time elapsed 231.34 sec\n",
      "epoch 63, iter 2960, avg. loss 3.93, avg. ppl 1.21 cum. examples 30640, speed 8193.38 words/sec, time elapsed 232.13 sec\n",
      "epoch 64, iter 2970, avg. loss 3.56, avg. ppl 1.19 cum. examples 30956, speed 8303.08 words/sec, time elapsed 232.90 sec\n",
      "epoch 64, iter 2980, avg. loss 3.27, avg. ppl 1.19 cum. examples 31276, speed 8009.55 words/sec, time elapsed 233.66 sec\n",
      "epoch 64, iter 2990, avg. loss 3.80, avg. ppl 1.20 cum. examples 31596, speed 8467.67 words/sec, time elapsed 234.47 sec\n",
      "epoch 64, iter 3000, avg. loss 3.56, avg. ppl 1.19 cum. examples 31916, speed 8087.02 words/sec, time elapsed 235.27 sec\n",
      "epoch 65, iter 3010, avg. loss 3.67, avg. ppl 1.19 cum. examples 32232, speed 8345.42 words/sec, time elapsed 236.06 sec\n",
      "epoch 65, iter 3020, avg. loss 3.35, avg. ppl 1.18 cum. examples 32552, speed 8466.37 words/sec, time elapsed 236.84 sec\n",
      "epoch 65, iter 3030, avg. loss 3.45, avg. ppl 1.19 cum. examples 32872, speed 8477.16 words/sec, time elapsed 237.60 sec\n",
      "epoch 65, iter 3040, avg. loss 3.69, avg. ppl 1.19 cum. examples 33192, speed 8988.99 words/sec, time elapsed 238.37 sec\n",
      "epoch 65, iter 3050, avg. loss 3.30, avg. ppl 1.18 cum. examples 33512, speed 8096.00 words/sec, time elapsed 239.14 sec\n",
      "epoch 66, iter 3060, avg. loss 3.22, avg. ppl 1.17 cum. examples 33828, speed 8290.60 words/sec, time elapsed 239.92 sec\n",
      "epoch 66, iter 3070, avg. loss 2.93, avg. ppl 1.16 cum. examples 34148, speed 8680.43 words/sec, time elapsed 240.65 sec\n",
      "epoch 66, iter 3080, avg. loss 3.30, avg. ppl 1.17 cum. examples 34468, speed 8484.84 words/sec, time elapsed 241.43 sec\n",
      "epoch 66, iter 3090, avg. loss 3.42, avg. ppl 1.18 cum. examples 34788, speed 8953.21 words/sec, time elapsed 242.17 sec\n",
      "epoch 66, iter 3100, avg. loss 3.48, avg. ppl 1.19 cum. examples 35108, speed 8113.19 words/sec, time elapsed 242.96 sec\n",
      "epoch 67, iter 3110, avg. loss 3.08, avg. ppl 1.16 cum. examples 35424, speed 8429.66 words/sec, time elapsed 243.72 sec\n",
      "epoch 67, iter 3120, avg. loss 3.12, avg. ppl 1.16 cum. examples 35744, speed 8767.06 words/sec, time elapsed 244.49 sec\n",
      "epoch 67, iter 3130, avg. loss 3.16, avg. ppl 1.17 cum. examples 36064, speed 8214.72 words/sec, time elapsed 245.26 sec\n",
      "epoch 67, iter 3140, avg. loss 3.24, avg. ppl 1.17 cum. examples 36384, speed 8675.00 words/sec, time elapsed 246.03 sec\n",
      "epoch 68, iter 3150, avg. loss 2.89, avg. ppl 1.16 cum. examples 36700, speed 7692.40 words/sec, time elapsed 246.84 sec\n",
      "epoch 68, iter 3160, avg. loss 2.79, avg. ppl 1.15 cum. examples 37020, speed 8146.16 words/sec, time elapsed 247.63 sec\n",
      "epoch 68, iter 3170, avg. loss 3.13, avg. ppl 1.16 cum. examples 37340, speed 8970.55 words/sec, time elapsed 248.37 sec\n",
      "epoch 68, iter 3180, avg. loss 3.09, avg. ppl 1.16 cum. examples 37660, speed 8671.21 words/sec, time elapsed 249.12 sec\n",
      "epoch 68, iter 3190, avg. loss 3.25, avg. ppl 1.17 cum. examples 37980, speed 8752.23 words/sec, time elapsed 249.87 sec\n",
      "epoch 69, iter 3200, avg. loss 3.18, avg. ppl 1.16 cum. examples 38296, speed 8958.42 words/sec, time elapsed 250.62 sec\n",
      "epoch 69, iter 3210, avg. loss 2.73, avg. ppl 1.14 cum. examples 38616, speed 8218.95 words/sec, time elapsed 251.40 sec\n",
      "epoch 69, iter 3220, avg. loss 3.08, avg. ppl 1.16 cum. examples 38936, speed 8484.44 words/sec, time elapsed 252.20 sec\n",
      "epoch 69, iter 3230, avg. loss 2.80, avg. ppl 1.16 cum. examples 39256, speed 8223.13 words/sec, time elapsed 252.95 sec\n",
      "epoch 69, iter 3240, avg. loss 3.04, avg. ppl 1.16 cum. examples 39576, speed 8495.09 words/sec, time elapsed 253.72 sec\n",
      "epoch 70, iter 3250, avg. loss 2.79, avg. ppl 1.15 cum. examples 39892, speed 7100.76 words/sec, time elapsed 254.64 sec\n",
      "epoch 70, iter 3260, avg. loss 2.86, avg. ppl 1.15 cum. examples 40212, speed 8348.29 words/sec, time elapsed 255.40 sec\n",
      "epoch 70, iter 3270, avg. loss 2.94, avg. ppl 1.15 cum. examples 40532, speed 8847.35 words/sec, time elapsed 256.15 sec\n",
      "epoch 70, iter 3280, avg. loss 2.80, avg. ppl 1.15 cum. examples 40852, speed 8389.25 words/sec, time elapsed 256.93 sec\n",
      "epoch 70, iter 3290, avg. loss 2.97, avg. ppl 1.16 cum. examples 41168, speed 8396.51 words/sec, time elapsed 257.69 sec\n",
      "epoch 71, iter 3300, avg. loss 2.80, avg. ppl 1.14 cum. examples 41488, speed 8553.09 words/sec, time elapsed 258.47 sec\n",
      "epoch 71, iter 3310, avg. loss 2.73, avg. ppl 1.15 cum. examples 41808, speed 8559.75 words/sec, time elapsed 259.21 sec\n",
      "epoch 71, iter 3320, avg. loss 2.83, avg. ppl 1.15 cum. examples 42128, speed 8548.74 words/sec, time elapsed 259.98 sec\n",
      "epoch 71, iter 3330, avg. loss 2.92, avg. ppl 1.15 cum. examples 42448, speed 8609.23 words/sec, time elapsed 260.74 sec\n",
      "epoch 72, iter 3340, avg. loss 2.54, avg. ppl 1.13 cum. examples 42764, speed 8622.87 words/sec, time elapsed 261.48 sec\n",
      "epoch 72, iter 3350, avg. loss 2.61, avg. ppl 1.14 cum. examples 43084, speed 8205.28 words/sec, time elapsed 262.28 sec\n",
      "epoch 72, iter 3360, avg. loss 2.55, avg. ppl 1.14 cum. examples 43404, speed 8530.03 words/sec, time elapsed 263.02 sec\n",
      "epoch 72, iter 3370, avg. loss 2.98, avg. ppl 1.16 cum. examples 43724, speed 7911.60 words/sec, time elapsed 263.85 sec\n",
      "epoch 72, iter 3380, avg. loss 2.73, avg. ppl 1.14 cum. examples 44044, speed 8460.30 words/sec, time elapsed 264.62 sec\n",
      "epoch 73, iter 3390, avg. loss 2.96, avg. ppl 1.15 cum. examples 44360, speed 8642.84 words/sec, time elapsed 265.39 sec\n",
      "epoch 73, iter 3400, avg. loss 2.27, avg. ppl 1.12 cum. examples 44680, speed 8368.64 words/sec, time elapsed 266.15 sec\n",
      "epoch 73, iter 3410, avg. loss 2.49, avg. ppl 1.13 cum. examples 45000, speed 8435.41 words/sec, time elapsed 266.92 sec\n",
      "epoch 73, iter 3420, avg. loss 2.83, avg. ppl 1.14 cum. examples 45320, speed 8594.53 words/sec, time elapsed 267.70 sec\n",
      "epoch 73, iter 3430, avg. loss 2.59, avg. ppl 1.14 cum. examples 45640, speed 8613.81 words/sec, time elapsed 268.46 sec\n",
      "epoch 74, iter 3440, avg. loss 2.30, avg. ppl 1.12 cum. examples 45956, speed 8626.37 words/sec, time elapsed 269.19 sec\n",
      "epoch 74, iter 3450, avg. loss 2.35, avg. ppl 1.12 cum. examples 46276, speed 8160.65 words/sec, time elapsed 269.99 sec\n",
      "epoch 74, iter 3460, avg. loss 2.72, avg. ppl 1.14 cum. examples 46596, speed 7979.13 words/sec, time elapsed 270.80 sec\n",
      "epoch 74, iter 3470, avg. loss 2.46, avg. ppl 1.13 cum. examples 46916, speed 8415.67 words/sec, time elapsed 271.58 sec\n",
      "epoch 75, iter 3480, avg. loss 2.55, avg. ppl 1.13 cum. examples 47232, speed 8208.24 words/sec, time elapsed 272.37 sec\n",
      "epoch 75, iter 3490, avg. loss 2.36, avg. ppl 1.12 cum. examples 47552, speed 8114.43 words/sec, time elapsed 273.17 sec\n",
      "epoch 75, iter 3500, avg. loss 2.16, avg. ppl 1.12 cum. examples 47872, speed 7812.55 words/sec, time elapsed 273.97 sec\n",
      "epoch 75, iter 3510, avg. loss 2.65, avg. ppl 1.13 cum. examples 48192, speed 7918.68 words/sec, time elapsed 274.83 sec\n",
      "epoch 75, iter 3520, avg. loss 2.49, avg. ppl 1.13 cum. examples 48512, speed 8408.60 words/sec, time elapsed 275.61 sec\n",
      "epoch 76, iter 3530, avg. loss 2.39, avg. ppl 1.12 cum. examples 48828, speed 8650.78 words/sec, time elapsed 276.36 sec\n",
      "epoch 76, iter 3540, avg. loss 2.08, avg. ppl 1.11 cum. examples 49148, speed 8369.57 words/sec, time elapsed 277.11 sec\n",
      "epoch 76, iter 3550, avg. loss 2.44, avg. ppl 1.13 cum. examples 49468, speed 7708.86 words/sec, time elapsed 277.94 sec\n",
      "epoch 76, iter 3560, avg. loss 2.31, avg. ppl 1.12 cum. examples 49788, speed 8223.13 words/sec, time elapsed 278.73 sec\n",
      "epoch 76, iter 3570, avg. loss 2.95, avg. ppl 1.14 cum. examples 50108, speed 7270.94 words/sec, time elapsed 279.70 sec\n",
      "epoch 77, iter 3580, avg. loss 2.33, avg. ppl 1.12 cum. examples 50424, speed 8319.18 words/sec, time elapsed 280.48 sec\n",
      "epoch 77, iter 3590, avg. loss 2.11, avg. ppl 1.11 cum. examples 50744, speed 7725.17 words/sec, time elapsed 281.31 sec\n",
      "epoch 77, iter 3600, avg. loss 2.41, avg. ppl 1.12 cum. examples 51064, speed 7911.94 words/sec, time elapsed 282.15 sec\n",
      "epoch 77, iter 3610, avg. loss 2.22, avg. ppl 1.12 cum. examples 51384, speed 7790.66 words/sec, time elapsed 282.98 sec\n",
      "epoch 78, iter 3620, avg. loss 2.44, avg. ppl 1.13 cum. examples 51700, speed 8246.40 words/sec, time elapsed 283.76 sec\n",
      "epoch 78, iter 3630, avg. loss 1.93, avg. ppl 1.10 cum. examples 52020, speed 8344.47 words/sec, time elapsed 284.51 sec\n",
      "epoch 78, iter 3640, avg. loss 2.16, avg. ppl 1.11 cum. examples 52340, speed 8789.47 words/sec, time elapsed 285.25 sec\n",
      "epoch 78, iter 3650, avg. loss 1.99, avg. ppl 1.11 cum. examples 52660, speed 8484.94 words/sec, time elapsed 285.98 sec\n",
      "epoch 78, iter 3660, avg. loss 2.41, avg. ppl 1.12 cum. examples 52980, speed 8842.75 words/sec, time elapsed 286.77 sec\n",
      "epoch 79, iter 3670, avg. loss 2.30, avg. ppl 1.12 cum. examples 53296, speed 8375.06 words/sec, time elapsed 287.55 sec\n",
      "epoch 79, iter 3680, avg. loss 2.01, avg. ppl 1.11 cum. examples 53616, speed 8632.21 words/sec, time elapsed 288.30 sec\n",
      "epoch 79, iter 3690, avg. loss 2.11, avg. ppl 1.11 cum. examples 53936, speed 8428.38 words/sec, time elapsed 289.08 sec\n",
      "epoch 79, iter 3700, avg. loss 2.13, avg. ppl 1.11 cum. examples 54256, speed 8863.05 words/sec, time elapsed 289.82 sec\n",
      "epoch 79, iter 3710, avg. loss 2.20, avg. ppl 1.11 cum. examples 54576, speed 8776.72 words/sec, time elapsed 290.56 sec\n",
      "epoch 80, iter 3720, avg. loss 2.24, avg. ppl 1.11 cum. examples 54892, speed 9427.30 words/sec, time elapsed 291.28 sec\n",
      "epoch 80, iter 3730, avg. loss 2.09, avg. ppl 1.11 cum. examples 55212, speed 8784.91 words/sec, time elapsed 292.02 sec\n",
      "epoch 80, iter 3740, avg. loss 1.88, avg. ppl 1.10 cum. examples 55532, speed 8908.41 words/sec, time elapsed 292.73 sec\n",
      "epoch 80, iter 3750, avg. loss 2.06, avg. ppl 1.10 cum. examples 55852, speed 8650.82 words/sec, time elapsed 293.50 sec\n",
      "epoch 80, iter 3760, avg. loss 2.08, avg. ppl 1.11 cum. examples 56168, speed 8273.65 words/sec, time elapsed 294.25 sec\n",
      "epoch 81, iter 3770, avg. loss 2.07, avg. ppl 1.10 cum. examples 56488, speed 8994.43 words/sec, time elapsed 295.01 sec\n",
      "epoch 81, iter 3780, avg. loss 1.92, avg. ppl 1.10 cum. examples 56808, speed 8232.42 words/sec, time elapsed 295.78 sec\n",
      "epoch 81, iter 3790, avg. loss 2.00, avg. ppl 1.11 cum. examples 57128, speed 7906.69 words/sec, time elapsed 296.58 sec\n",
      "epoch 81, iter 3800, avg. loss 2.18, avg. ppl 1.11 cum. examples 57448, speed 8357.94 words/sec, time elapsed 297.38 sec\n",
      "epoch 82, iter 3810, avg. loss 1.98, avg. ppl 1.11 cum. examples 57764, speed 8115.96 words/sec, time elapsed 298.15 sec\n",
      "epoch 82, iter 3820, avg. loss 1.93, avg. ppl 1.10 cum. examples 58084, speed 8503.98 words/sec, time elapsed 298.93 sec\n",
      "epoch 82, iter 3830, avg. loss 2.07, avg. ppl 1.11 cum. examples 58404, speed 8771.33 words/sec, time elapsed 299.67 sec\n",
      "epoch 82, iter 3840, avg. loss 2.09, avg. ppl 1.10 cum. examples 58724, speed 8856.01 words/sec, time elapsed 300.44 sec\n",
      "epoch 82, iter 3850, avg. loss 1.98, avg. ppl 1.10 cum. examples 59044, speed 8564.43 words/sec, time elapsed 301.19 sec\n",
      "epoch 83, iter 3860, avg. loss 1.89, avg. ppl 1.10 cum. examples 59360, speed 8704.50 words/sec, time elapsed 301.93 sec\n",
      "epoch 83, iter 3870, avg. loss 1.94, avg. ppl 1.10 cum. examples 59680, speed 8429.49 words/sec, time elapsed 302.70 sec\n",
      "epoch 83, iter 3880, avg. loss 1.62, avg. ppl 1.09 cum. examples 60000, speed 8348.65 words/sec, time elapsed 303.44 sec\n",
      "epoch 83, iter 3890, avg. loss 2.01, avg. ppl 1.10 cum. examples 60320, speed 8657.45 words/sec, time elapsed 304.20 sec\n",
      "epoch 83, iter 3900, avg. loss 2.20, avg. ppl 1.11 cum. examples 60640, speed 9296.35 words/sec, time elapsed 304.94 sec\n",
      "epoch 84, iter 3910, avg. loss 1.79, avg. ppl 1.09 cum. examples 60956, speed 8697.21 words/sec, time elapsed 305.68 sec\n",
      "epoch 84, iter 3920, avg. loss 1.71, avg. ppl 1.09 cum. examples 61276, speed 8566.40 words/sec, time elapsed 306.41 sec\n",
      "epoch 84, iter 3930, avg. loss 1.89, avg. ppl 1.10 cum. examples 61596, speed 8463.27 words/sec, time elapsed 307.16 sec\n",
      "epoch 84, iter 3940, avg. loss 1.93, avg. ppl 1.10 cum. examples 61916, speed 8982.18 words/sec, time elapsed 307.91 sec\n",
      "epoch 85, iter 3950, avg. loss 1.95, avg. ppl 1.10 cum. examples 62232, speed 8156.80 words/sec, time elapsed 308.70 sec\n",
      "epoch 85, iter 3960, avg. loss 1.72, avg. ppl 1.09 cum. examples 62552, speed 8683.64 words/sec, time elapsed 309.46 sec\n",
      "epoch 85, iter 3970, avg. loss 1.78, avg. ppl 1.09 cum. examples 62872, speed 8313.16 words/sec, time elapsed 310.22 sec\n",
      "epoch 85, iter 3980, avg. loss 1.91, avg. ppl 1.10 cum. examples 63192, speed 8982.02 words/sec, time elapsed 310.95 sec\n",
      "epoch 85, iter 3990, avg. loss 1.95, avg. ppl 1.10 cum. examples 63512, speed 8702.27 words/sec, time elapsed 311.72 sec\n",
      "epoch 86, iter 4000, avg. loss 1.78, avg. ppl 1.09 cum. examples 63828, speed 8624.81 words/sec, time elapsed 312.47 sec\n",
      "epoch 86, iter 4000, cum. loss 4.30, cum. ppl 1.23 cum. examples 63828\n",
      "begin validation ...\n",
      "validation: iter 4000, dev. ppl 112976.938571\n",
      "hit patience 1\n",
      "epoch 86, iter 4010, avg. loss 1.78, avg. ppl 1.10 cum. examples 320, speed 5575.27 words/sec, time elapsed 313.59 sec\n",
      "epoch 86, iter 4020, avg. loss 1.89, avg. ppl 1.10 cum. examples 640, speed 8162.69 words/sec, time elapsed 314.40 sec\n",
      "epoch 86, iter 4030, avg. loss 1.89, avg. ppl 1.10 cum. examples 960, speed 8453.79 words/sec, time elapsed 315.16 sec\n",
      "epoch 86, iter 4040, avg. loss 1.82, avg. ppl 1.09 cum. examples 1280, speed 9118.35 words/sec, time elapsed 315.90 sec\n",
      "epoch 87, iter 4050, avg. loss 1.85, avg. ppl 1.09 cum. examples 1596, speed 8810.30 words/sec, time elapsed 316.66 sec\n",
      "epoch 87, iter 4060, avg. loss 1.58, avg. ppl 1.08 cum. examples 1916, speed 8767.16 words/sec, time elapsed 317.38 sec\n",
      "epoch 87, iter 4070, avg. loss 1.81, avg. ppl 1.09 cum. examples 2236, speed 8977.87 words/sec, time elapsed 318.14 sec\n",
      "epoch 87, iter 4080, avg. loss 1.72, avg. ppl 1.09 cum. examples 2556, speed 8445.53 words/sec, time elapsed 318.91 sec\n",
      "epoch 88, iter 4090, avg. loss 1.59, avg. ppl 1.08 cum. examples 2872, speed 8149.14 words/sec, time elapsed 319.68 sec\n",
      "epoch 88, iter 4100, avg. loss 1.53, avg. ppl 1.08 cum. examples 3192, speed 8198.02 words/sec, time elapsed 320.43 sec\n",
      "epoch 88, iter 4110, avg. loss 1.86, avg. ppl 1.09 cum. examples 3512, speed 8844.73 words/sec, time elapsed 321.17 sec\n",
      "epoch 88, iter 4120, avg. loss 1.70, avg. ppl 1.09 cum. examples 3832, speed 8408.41 words/sec, time elapsed 321.96 sec\n",
      "epoch 88, iter 4130, avg. loss 1.72, avg. ppl 1.09 cum. examples 4152, speed 8885.26 words/sec, time elapsed 322.69 sec\n",
      "epoch 89, iter 4140, avg. loss 1.82, avg. ppl 1.09 cum. examples 4468, speed 8972.22 words/sec, time elapsed 323.44 sec\n",
      "epoch 89, iter 4150, avg. loss 1.54, avg. ppl 1.08 cum. examples 4788, speed 8393.35 words/sec, time elapsed 324.21 sec\n",
      "epoch 89, iter 4160, avg. loss 1.56, avg. ppl 1.08 cum. examples 5108, speed 9001.52 words/sec, time elapsed 324.91 sec\n",
      "epoch 89, iter 4170, avg. loss 1.88, avg. ppl 1.09 cum. examples 5428, speed 8986.60 words/sec, time elapsed 325.67 sec\n",
      "epoch 89, iter 4180, avg. loss 1.62, avg. ppl 1.08 cum. examples 5748, speed 8645.39 words/sec, time elapsed 326.43 sec\n",
      "epoch 90, iter 4190, avg. loss 1.67, avg. ppl 1.09 cum. examples 6064, speed 8554.53 words/sec, time elapsed 327.18 sec\n",
      "epoch 90, iter 4200, avg. loss 1.51, avg. ppl 1.08 cum. examples 6384, speed 8352.65 words/sec, time elapsed 327.95 sec\n",
      "epoch 90, iter 4210, avg. loss 1.57, avg. ppl 1.08 cum. examples 6704, speed 8368.27 words/sec, time elapsed 328.70 sec\n",
      "epoch 90, iter 4220, avg. loss 1.80, avg. ppl 1.09 cum. examples 7024, speed 8554.72 words/sec, time elapsed 329.51 sec\n",
      "epoch 90, iter 4230, avg. loss 1.63, avg. ppl 1.08 cum. examples 7340, speed 8156.59 words/sec, time elapsed 330.29 sec\n",
      "epoch 91, iter 4240, avg. loss 1.55, avg. ppl 1.08 cum. examples 7660, speed 8572.34 words/sec, time elapsed 331.04 sec\n",
      "epoch 91, iter 4250, avg. loss 1.78, avg. ppl 1.09 cum. examples 7980, speed 8410.53 words/sec, time elapsed 331.84 sec\n",
      "epoch 91, iter 4260, avg. loss 1.58, avg. ppl 1.08 cum. examples 8300, speed 8549.58 words/sec, time elapsed 332.59 sec\n",
      "epoch 91, iter 4270, avg. loss 1.63, avg. ppl 1.08 cum. examples 8620, speed 8835.12 words/sec, time elapsed 333.32 sec\n",
      "epoch 92, iter 4280, avg. loss 1.59, avg. ppl 1.08 cum. examples 8936, speed 8762.73 words/sec, time elapsed 334.08 sec\n",
      "epoch 92, iter 4290, avg. loss 1.54, avg. ppl 1.08 cum. examples 9256, speed 8506.06 words/sec, time elapsed 334.85 sec\n",
      "epoch 92, iter 4300, avg. loss 1.51, avg. ppl 1.08 cum. examples 9576, speed 8088.56 words/sec, time elapsed 335.64 sec\n",
      "epoch 92, iter 4310, avg. loss 1.67, avg. ppl 1.09 cum. examples 9896, speed 8832.13 words/sec, time elapsed 336.36 sec\n",
      "epoch 92, iter 4320, avg. loss 1.61, avg. ppl 1.09 cum. examples 10216, speed 8370.12 words/sec, time elapsed 337.11 sec\n",
      "epoch 93, iter 4330, avg. loss 1.65, avg. ppl 1.08 cum. examples 10532, speed 8565.25 words/sec, time elapsed 337.86 sec\n",
      "epoch 93, iter 4340, avg. loss 1.53, avg. ppl 1.08 cum. examples 10852, speed 8841.36 words/sec, time elapsed 338.62 sec\n",
      "epoch 93, iter 4350, avg. loss 1.58, avg. ppl 1.08 cum. examples 11172, speed 8749.38 words/sec, time elapsed 339.37 sec\n",
      "epoch 93, iter 4360, avg. loss 1.58, avg. ppl 1.08 cum. examples 11492, speed 8569.39 words/sec, time elapsed 340.14 sec\n",
      "epoch 93, iter 4370, avg. loss 1.60, avg. ppl 1.08 cum. examples 11812, speed 8792.99 words/sec, time elapsed 340.88 sec\n",
      "epoch 94, iter 4380, avg. loss 1.67, avg. ppl 1.08 cum. examples 12128, speed 8356.45 words/sec, time elapsed 341.69 sec\n",
      "epoch 94, iter 4390, avg. loss 1.47, avg. ppl 1.07 cum. examples 12448, speed 9013.37 words/sec, time elapsed 342.45 sec\n",
      "epoch 94, iter 4400, avg. loss 1.55, avg. ppl 1.08 cum. examples 12768, speed 8474.70 words/sec, time elapsed 343.20 sec\n",
      "epoch 94, iter 4410, avg. loss 1.44, avg. ppl 1.08 cum. examples 13088, speed 7907.26 words/sec, time elapsed 344.01 sec\n",
      "epoch 95, iter 4420, avg. loss 1.52, avg. ppl 1.08 cum. examples 13404, speed 8220.03 words/sec, time elapsed 344.77 sec\n",
      "epoch 95, iter 4430, avg. loss 1.51, avg. ppl 1.07 cum. examples 13724, speed 8494.54 words/sec, time elapsed 345.57 sec\n",
      "epoch 95, iter 4440, avg. loss 1.39, avg. ppl 1.07 cum. examples 14044, speed 7676.37 words/sec, time elapsed 346.42 sec\n",
      "epoch 95, iter 4450, avg. loss 1.47, avg. ppl 1.08 cum. examples 14364, speed 8795.22 words/sec, time elapsed 347.14 sec\n",
      "epoch 95, iter 4460, avg. loss 1.45, avg. ppl 1.07 cum. examples 14684, speed 8199.02 words/sec, time elapsed 347.93 sec\n",
      "epoch 96, iter 4470, avg. loss 1.62, avg. ppl 1.08 cum. examples 15000, speed 8401.03 words/sec, time elapsed 348.69 sec\n",
      "epoch 96, iter 4480, avg. loss 1.31, avg. ppl 1.07 cum. examples 15320, speed 9080.42 words/sec, time elapsed 349.40 sec\n",
      "epoch 96, iter 4490, avg. loss 1.34, avg. ppl 1.07 cum. examples 15640, speed 8838.88 words/sec, time elapsed 350.13 sec\n",
      "epoch 96, iter 4500, avg. loss 1.56, avg. ppl 1.07 cum. examples 15960, speed 8941.79 words/sec, time elapsed 350.91 sec\n",
      "epoch 96, iter 4510, avg. loss 1.49, avg. ppl 1.08 cum. examples 16280, speed 8484.94 words/sec, time elapsed 351.64 sec\n",
      "epoch 97, iter 4520, avg. loss 1.25, avg. ppl 1.06 cum. examples 16596, speed 8766.58 words/sec, time elapsed 352.39 sec\n",
      "epoch 97, iter 4530, avg. loss 1.41, avg. ppl 1.07 cum. examples 16916, speed 8559.93 words/sec, time elapsed 353.15 sec\n",
      "epoch 97, iter 4540, avg. loss 1.40, avg. ppl 1.07 cum. examples 17236, speed 8879.35 words/sec, time elapsed 353.88 sec\n",
      "epoch 97, iter 4550, avg. loss 1.63, avg. ppl 1.08 cum. examples 17556, speed 8537.38 words/sec, time elapsed 354.63 sec\n",
      "epoch 98, iter 4560, avg. loss 1.50, avg. ppl 1.07 cum. examples 17872, speed 8467.96 words/sec, time elapsed 355.41 sec\n",
      "epoch 98, iter 4570, avg. loss 1.17, avg. ppl 1.06 cum. examples 18192, speed 8537.00 words/sec, time elapsed 356.16 sec\n",
      "epoch 98, iter 4580, avg. loss 1.33, avg. ppl 1.07 cum. examples 18512, speed 8791.51 words/sec, time elapsed 356.88 sec\n",
      "epoch 98, iter 4590, avg. loss 1.40, avg. ppl 1.07 cum. examples 18832, speed 7898.65 words/sec, time elapsed 357.72 sec\n",
      "epoch 98, iter 4600, avg. loss 1.45, avg. ppl 1.07 cum. examples 19152, speed 8191.47 words/sec, time elapsed 358.53 sec\n",
      "epoch 99, iter 4610, avg. loss 1.33, avg. ppl 1.07 cum. examples 19468, speed 8559.34 words/sec, time elapsed 359.27 sec\n",
      "epoch 99, iter 4620, avg. loss 1.29, avg. ppl 1.06 cum. examples 19788, speed 7787.78 words/sec, time elapsed 360.13 sec\n",
      "epoch 99, iter 4630, avg. loss 1.47, avg. ppl 1.07 cum. examples 20108, speed 8623.72 words/sec, time elapsed 360.90 sec\n",
      "epoch 99, iter 4640, avg. loss 1.43, avg. ppl 1.08 cum. examples 20428, speed 8905.99 words/sec, time elapsed 361.61 sec\n",
      "epoch 99, iter 4650, avg. loss 1.30, avg. ppl 1.07 cum. examples 20748, speed 8694.18 words/sec, time elapsed 362.36 sec\n",
      "epoch 100, iter 4660, avg. loss 1.27, avg. ppl 1.07 cum. examples 21064, speed 7951.36 words/sec, time elapsed 363.16 sec\n",
      "epoch 100, iter 4670, avg. loss 1.37, avg. ppl 1.07 cum. examples 21384, speed 8719.55 words/sec, time elapsed 363.93 sec\n",
      "epoch 100, iter 4680, avg. loss 1.29, avg. ppl 1.07 cum. examples 21704, speed 8322.31 words/sec, time elapsed 364.70 sec\n",
      "epoch 100, iter 4690, avg. loss 1.20, avg. ppl 1.06 cum. examples 22024, speed 8102.18 words/sec, time elapsed 365.52 sec\n",
      "epoch 100, iter 4700, avg. loss 1.26, avg. ppl 1.06 cum. examples 22340, speed 8023.86 words/sec, time elapsed 366.32 sec\n",
      "epoch 101, iter 4710, avg. loss 1.17, avg. ppl 1.06 cum. examples 22660, speed 8130.53 words/sec, time elapsed 367.08 sec\n",
      "epoch 101, iter 4720, avg. loss 1.28, avg. ppl 1.07 cum. examples 22980, speed 7846.96 words/sec, time elapsed 367.90 sec\n",
      "epoch 101, iter 4730, avg. loss 1.33, avg. ppl 1.06 cum. examples 23300, speed 8395.40 words/sec, time elapsed 368.71 sec\n",
      "epoch 101, iter 4740, avg. loss 1.15, avg. ppl 1.06 cum. examples 23620, speed 8836.89 words/sec, time elapsed 369.46 sec\n",
      "epoch 102, iter 4750, avg. loss 1.16, avg. ppl 1.06 cum. examples 23936, speed 8460.69 words/sec, time elapsed 370.21 sec\n",
      "epoch 102, iter 4760, avg. loss 1.09, avg. ppl 1.06 cum. examples 24256, speed 7888.54 words/sec, time elapsed 371.02 sec\n",
      "epoch 102, iter 4770, avg. loss 1.25, avg. ppl 1.06 cum. examples 24576, speed 8681.46 words/sec, time elapsed 371.78 sec\n",
      "epoch 102, iter 4780, avg. loss 1.30, avg. ppl 1.07 cum. examples 24896, speed 8797.51 words/sec, time elapsed 372.52 sec\n",
      "epoch 102, iter 4790, avg. loss 1.45, avg. ppl 1.07 cum. examples 25216, speed 8532.52 words/sec, time elapsed 373.30 sec\n",
      "epoch 103, iter 4800, avg. loss 1.31, avg. ppl 1.06 cum. examples 25532, speed 8626.32 words/sec, time elapsed 374.09 sec\n",
      "epoch 103, iter 4810, avg. loss 1.16, avg. ppl 1.06 cum. examples 25852, speed 8492.99 words/sec, time elapsed 374.85 sec\n",
      "epoch 103, iter 4820, avg. loss 1.17, avg. ppl 1.06 cum. examples 26172, speed 8488.60 words/sec, time elapsed 375.62 sec\n",
      "epoch 103, iter 4830, avg. loss 1.10, avg. ppl 1.06 cum. examples 26492, speed 8046.31 words/sec, time elapsed 376.39 sec\n",
      "epoch 103, iter 4840, avg. loss 1.37, avg. ppl 1.07 cum. examples 26812, speed 8637.12 words/sec, time elapsed 377.16 sec\n",
      "epoch 104, iter 4850, avg. loss 1.12, avg. ppl 1.06 cum. examples 27128, speed 8756.84 words/sec, time elapsed 377.90 sec\n",
      "epoch 104, iter 4860, avg. loss 1.25, avg. ppl 1.06 cum. examples 27448, speed 8896.83 words/sec, time elapsed 378.63 sec\n",
      "epoch 104, iter 4870, avg. loss 1.22, avg. ppl 1.06 cum. examples 27768, speed 8349.88 words/sec, time elapsed 379.40 sec\n",
      "epoch 104, iter 4880, avg. loss 1.15, avg. ppl 1.06 cum. examples 28088, speed 7895.33 words/sec, time elapsed 380.24 sec\n",
      "epoch 105, iter 4890, avg. loss 1.34, avg. ppl 1.07 cum. examples 28404, speed 8103.50 words/sec, time elapsed 381.05 sec\n",
      "epoch 105, iter 4900, avg. loss 1.14, avg. ppl 1.06 cum. examples 28724, speed 7976.34 words/sec, time elapsed 381.85 sec\n",
      "epoch 105, iter 4910, avg. loss 1.24, avg. ppl 1.06 cum. examples 29044, speed 8244.26 words/sec, time elapsed 382.66 sec\n",
      "epoch 105, iter 4920, avg. loss 1.21, avg. ppl 1.06 cum. examples 29364, speed 8197.59 words/sec, time elapsed 383.44 sec\n",
      "epoch 105, iter 4930, avg. loss 1.22, avg. ppl 1.06 cum. examples 29684, speed 8980.86 words/sec, time elapsed 384.16 sec\n",
      "epoch 106, iter 4940, avg. loss 1.10, avg. ppl 1.06 cum. examples 30000, speed 8574.57 words/sec, time elapsed 384.90 sec\n",
      "epoch 106, iter 4950, avg. loss 1.29, avg. ppl 1.06 cum. examples 30320, speed 8568.57 words/sec, time elapsed 385.67 sec\n",
      "epoch 106, iter 4960, avg. loss 1.18, avg. ppl 1.06 cum. examples 30640, speed 8326.26 words/sec, time elapsed 386.45 sec\n",
      "epoch 106, iter 4970, avg. loss 1.27, avg. ppl 1.06 cum. examples 30960, speed 8816.08 words/sec, time elapsed 387.21 sec\n",
      "epoch 106, iter 4980, avg. loss 1.23, avg. ppl 1.06 cum. examples 31280, speed 8474.22 words/sec, time elapsed 387.96 sec\n",
      "epoch 107, iter 4990, avg. loss 1.12, avg. ppl 1.05 cum. examples 31596, speed 8804.72 words/sec, time elapsed 388.73 sec\n",
      "epoch 107, iter 5000, avg. loss 1.07, avg. ppl 1.05 cum. examples 31916, speed 8394.64 words/sec, time elapsed 389.51 sec\n",
      "epoch 107, iter 5010, avg. loss 1.09, avg. ppl 1.06 cum. examples 32236, speed 8716.31 words/sec, time elapsed 390.26 sec\n",
      "epoch 107, iter 5020, avg. loss 1.18, avg. ppl 1.06 cum. examples 32556, speed 8307.21 words/sec, time elapsed 391.04 sec\n",
      "epoch 108, iter 5030, avg. loss 1.09, avg. ppl 1.06 cum. examples 32872, speed 8127.81 words/sec, time elapsed 391.81 sec\n",
      "epoch 108, iter 5040, avg. loss 0.98, avg. ppl 1.05 cum. examples 33192, speed 7970.66 words/sec, time elapsed 392.60 sec\n",
      "epoch 108, iter 5050, avg. loss 1.01, avg. ppl 1.05 cum. examples 33512, speed 8568.59 words/sec, time elapsed 393.34 sec\n",
      "epoch 108, iter 5060, avg. loss 1.26, avg. ppl 1.06 cum. examples 33832, speed 8597.91 words/sec, time elapsed 394.12 sec\n",
      "epoch 108, iter 5070, avg. loss 1.20, avg. ppl 1.06 cum. examples 34152, speed 8672.35 words/sec, time elapsed 394.87 sec\n",
      "epoch 109, iter 5080, avg. loss 1.28, avg. ppl 1.06 cum. examples 34468, speed 8806.60 words/sec, time elapsed 395.62 sec\n",
      "epoch 109, iter 5090, avg. loss 1.09, avg. ppl 1.05 cum. examples 34788, speed 9112.79 words/sec, time elapsed 396.36 sec\n",
      "epoch 109, iter 5100, avg. loss 1.09, avg. ppl 1.05 cum. examples 35108, speed 8476.99 words/sec, time elapsed 397.13 sec\n",
      "epoch 109, iter 5110, avg. loss 0.95, avg. ppl 1.05 cum. examples 35428, speed 8523.98 words/sec, time elapsed 397.85 sec\n",
      "epoch 109, iter 5120, avg. loss 1.20, avg. ppl 1.06 cum. examples 35748, speed 8736.96 words/sec, time elapsed 398.63 sec\n",
      "epoch 110, iter 5130, avg. loss 1.03, avg. ppl 1.05 cum. examples 36064, speed 8411.10 words/sec, time elapsed 399.38 sec\n",
      "epoch 110, iter 5140, avg. loss 1.10, avg. ppl 1.05 cum. examples 36384, speed 8431.56 words/sec, time elapsed 400.15 sec\n",
      "epoch 110, iter 5150, avg. loss 1.07, avg. ppl 1.05 cum. examples 36704, speed 8287.26 words/sec, time elapsed 400.95 sec\n",
      "epoch 110, iter 5160, avg. loss 1.13, avg. ppl 1.06 cum. examples 37024, speed 8127.35 words/sec, time elapsed 401.73 sec\n",
      "epoch 110, iter 5170, avg. loss 1.24, avg. ppl 1.06 cum. examples 37340, speed 8529.67 words/sec, time elapsed 402.51 sec\n",
      "epoch 111, iter 5180, avg. loss 1.14, avg. ppl 1.05 cum. examples 37660, speed 8967.14 words/sec, time elapsed 403.28 sec\n",
      "epoch 111, iter 5190, avg. loss 0.97, avg. ppl 1.05 cum. examples 37980, speed 8614.67 words/sec, time elapsed 404.04 sec\n",
      "epoch 111, iter 5200, avg. loss 1.03, avg. ppl 1.06 cum. examples 38300, speed 8312.21 words/sec, time elapsed 404.77 sec\n",
      "epoch 111, iter 5210, avg. loss 1.15, avg. ppl 1.06 cum. examples 38620, speed 8873.96 words/sec, time elapsed 405.50 sec\n",
      "epoch 112, iter 5220, avg. loss 1.12, avg. ppl 1.06 cum. examples 38936, speed 8237.99 words/sec, time elapsed 406.29 sec\n",
      "epoch 112, iter 5230, avg. loss 1.05, avg. ppl 1.05 cum. examples 39256, speed 8301.50 words/sec, time elapsed 407.09 sec\n",
      "epoch 112, iter 5240, avg. loss 1.16, avg. ppl 1.06 cum. examples 39576, speed 8522.27 words/sec, time elapsed 407.87 sec\n",
      "epoch 112, iter 5250, avg. loss 1.12, avg. ppl 1.06 cum. examples 39896, speed 8067.28 words/sec, time elapsed 408.67 sec\n",
      "epoch 112, iter 5260, avg. loss 1.10, avg. ppl 1.06 cum. examples 40216, speed 7924.73 words/sec, time elapsed 409.48 sec\n",
      "epoch 113, iter 5270, avg. loss 1.04, avg. ppl 1.05 cum. examples 40532, speed 8642.26 words/sec, time elapsed 410.23 sec\n",
      "epoch 113, iter 5280, avg. loss 1.10, avg. ppl 1.05 cum. examples 40852, speed 8539.16 words/sec, time elapsed 411.00 sec\n",
      "epoch 113, iter 5290, avg. loss 1.06, avg. ppl 1.05 cum. examples 41172, speed 8269.65 words/sec, time elapsed 411.79 sec\n",
      "epoch 113, iter 5300, avg. loss 1.06, avg. ppl 1.05 cum. examples 41492, speed 8198.22 words/sec, time elapsed 412.60 sec\n",
      "epoch 113, iter 5310, avg. loss 1.12, avg. ppl 1.06 cum. examples 41812, speed 8067.26 words/sec, time elapsed 413.39 sec\n",
      "epoch 114, iter 5320, avg. loss 0.98, avg. ppl 1.05 cum. examples 42128, speed 7937.85 words/sec, time elapsed 414.19 sec\n",
      "epoch 114, iter 5330, avg. loss 1.12, avg. ppl 1.06 cum. examples 42448, speed 8382.53 words/sec, time elapsed 414.95 sec\n",
      "epoch 114, iter 5340, avg. loss 0.96, avg. ppl 1.05 cum. examples 42768, speed 8762.32 words/sec, time elapsed 415.69 sec\n",
      "epoch 114, iter 5350, avg. loss 0.99, avg. ppl 1.05 cum. examples 43088, speed 8682.50 words/sec, time elapsed 416.46 sec\n",
      "epoch 115, iter 5360, avg. loss 1.14, avg. ppl 1.06 cum. examples 43404, speed 8877.24 words/sec, time elapsed 417.20 sec\n",
      "epoch 115, iter 5370, avg. loss 0.93, avg. ppl 1.05 cum. examples 43724, speed 8592.60 words/sec, time elapsed 417.91 sec\n",
      "epoch 115, iter 5380, avg. loss 0.88, avg. ppl 1.04 cum. examples 44044, speed 8963.48 words/sec, time elapsed 418.63 sec\n",
      "epoch 115, iter 5390, avg. loss 1.08, avg. ppl 1.05 cum. examples 44364, speed 9152.59 words/sec, time elapsed 419.36 sec\n",
      "epoch 115, iter 5400, avg. loss 1.15, avg. ppl 1.06 cum. examples 44684, speed 7828.56 words/sec, time elapsed 420.22 sec\n",
      "epoch 116, iter 5410, avg. loss 1.03, avg. ppl 1.05 cum. examples 45000, speed 7728.33 words/sec, time elapsed 421.09 sec\n",
      "epoch 116, iter 5420, avg. loss 1.00, avg. ppl 1.05 cum. examples 45320, speed 7640.12 words/sec, time elapsed 421.92 sec\n",
      "epoch 116, iter 5430, avg. loss 0.92, avg. ppl 1.05 cum. examples 45640, speed 7100.84 words/sec, time elapsed 422.83 sec\n",
      "epoch 116, iter 5440, avg. loss 0.89, avg. ppl 1.05 cum. examples 45960, speed 7403.13 words/sec, time elapsed 423.68 sec\n",
      "epoch 116, iter 5450, avg. loss 1.06, avg. ppl 1.05 cum. examples 46280, speed 7457.44 words/sec, time elapsed 424.60 sec\n",
      "epoch 117, iter 5460, avg. loss 1.00, avg. ppl 1.05 cum. examples 46596, speed 7670.30 words/sec, time elapsed 425.44 sec\n",
      "epoch 117, iter 5470, avg. loss 0.83, avg. ppl 1.04 cum. examples 46916, speed 7447.86 words/sec, time elapsed 426.31 sec\n",
      "epoch 117, iter 5480, avg. loss 0.99, avg. ppl 1.05 cum. examples 47236, speed 7326.16 words/sec, time elapsed 427.19 sec\n",
      "epoch 117, iter 5490, avg. loss 0.95, avg. ppl 1.05 cum. examples 47556, speed 7251.94 words/sec, time elapsed 428.08 sec\n",
      "epoch 118, iter 5500, avg. loss 1.13, avg. ppl 1.05 cum. examples 47872, speed 7397.63 words/sec, time elapsed 428.99 sec\n",
      "epoch 118, iter 5510, avg. loss 0.97, avg. ppl 1.05 cum. examples 48192, speed 7490.33 words/sec, time elapsed 429.83 sec\n",
      "epoch 118, iter 5520, avg. loss 1.01, avg. ppl 1.05 cum. examples 48512, speed 7862.81 words/sec, time elapsed 430.67 sec\n",
      "epoch 118, iter 5530, avg. loss 0.91, avg. ppl 1.05 cum. examples 48832, speed 8505.83 words/sec, time elapsed 431.44 sec\n",
      "epoch 118, iter 5540, avg. loss 0.92, avg. ppl 1.05 cum. examples 49152, speed 8561.68 words/sec, time elapsed 432.21 sec\n",
      "epoch 119, iter 5550, avg. loss 0.85, avg. ppl 1.04 cum. examples 49468, speed 8734.84 words/sec, time elapsed 432.92 sec\n",
      "epoch 119, iter 5560, avg. loss 0.90, avg. ppl 1.05 cum. examples 49788, speed 8080.19 words/sec, time elapsed 433.70 sec\n",
      "epoch 119, iter 5570, avg. loss 1.03, avg. ppl 1.05 cum. examples 50108, speed 8947.71 words/sec, time elapsed 434.47 sec\n",
      "epoch 119, iter 5580, avg. loss 0.89, avg. ppl 1.05 cum. examples 50428, speed 8722.68 words/sec, time elapsed 435.19 sec\n",
      "epoch 119, iter 5590, avg. loss 0.98, avg. ppl 1.05 cum. examples 50748, speed 8977.50 words/sec, time elapsed 435.94 sec\n",
      "epoch 120, iter 5600, avg. loss 0.99, avg. ppl 1.05 cum. examples 51064, speed 8086.94 words/sec, time elapsed 436.75 sec\n",
      "epoch 120, iter 5610, avg. loss 0.99, avg. ppl 1.05 cum. examples 51384, speed 7758.84 words/sec, time elapsed 437.60 sec\n",
      "epoch 120, iter 5620, avg. loss 0.84, avg. ppl 1.04 cum. examples 51704, speed 7839.30 words/sec, time elapsed 438.43 sec\n",
      "epoch 120, iter 5630, avg. loss 0.96, avg. ppl 1.05 cum. examples 52024, speed 7864.78 words/sec, time elapsed 439.24 sec\n",
      "epoch 120, iter 5640, avg. loss 0.92, avg. ppl 1.05 cum. examples 52340, speed 8408.23 words/sec, time elapsed 440.02 sec\n",
      "epoch 121, iter 5650, avg. loss 0.88, avg. ppl 1.04 cum. examples 52660, speed 8644.31 words/sec, time elapsed 440.79 sec\n",
      "epoch 121, iter 5660, avg. loss 1.02, avg. ppl 1.05 cum. examples 52980, speed 8778.66 words/sec, time elapsed 441.55 sec\n",
      "epoch 121, iter 5670, avg. loss 0.92, avg. ppl 1.05 cum. examples 53300, speed 8188.39 words/sec, time elapsed 442.33 sec\n",
      "epoch 121, iter 5680, avg. loss 0.94, avg. ppl 1.05 cum. examples 53620, speed 9007.47 words/sec, time elapsed 443.07 sec\n",
      "epoch 122, iter 5690, avg. loss 0.90, avg. ppl 1.05 cum. examples 53936, speed 8638.31 words/sec, time elapsed 443.80 sec\n",
      "epoch 122, iter 5700, avg. loss 0.90, avg. ppl 1.05 cum. examples 54256, speed 8351.29 words/sec, time elapsed 444.54 sec\n",
      "epoch 122, iter 5710, avg. loss 0.99, avg. ppl 1.05 cum. examples 54576, speed 8560.34 words/sec, time elapsed 445.29 sec\n",
      "epoch 122, iter 5720, avg. loss 0.87, avg. ppl 1.05 cum. examples 54896, speed 8836.46 words/sec, time elapsed 446.00 sec\n",
      "epoch 122, iter 5730, avg. loss 1.08, avg. ppl 1.05 cum. examples 55216, speed 9232.73 words/sec, time elapsed 446.77 sec\n",
      "epoch 123, iter 5740, avg. loss 0.87, avg. ppl 1.04 cum. examples 55532, speed 8160.74 words/sec, time elapsed 447.54 sec\n",
      "epoch 123, iter 5750, avg. loss 0.84, avg. ppl 1.04 cum. examples 55852, speed 8246.69 words/sec, time elapsed 448.30 sec\n",
      "epoch 123, iter 5760, avg. loss 1.02, avg. ppl 1.05 cum. examples 56172, speed 8784.98 words/sec, time elapsed 449.06 sec\n",
      "epoch 123, iter 5770, avg. loss 0.89, avg. ppl 1.04 cum. examples 56492, speed 8696.93 words/sec, time elapsed 449.83 sec\n",
      "epoch 123, iter 5780, avg. loss 0.89, avg. ppl 1.04 cum. examples 56812, speed 8626.88 words/sec, time elapsed 450.60 sec\n",
      "epoch 124, iter 5790, avg. loss 0.81, avg. ppl 1.04 cum. examples 57128, speed 8624.79 words/sec, time elapsed 451.32 sec\n",
      "epoch 124, iter 5800, avg. loss 0.91, avg. ppl 1.04 cum. examples 57448, speed 8540.01 words/sec, time elapsed 452.12 sec\n",
      "epoch 124, iter 5810, avg. loss 0.92, avg. ppl 1.04 cum. examples 57768, speed 8139.97 words/sec, time elapsed 452.97 sec\n",
      "epoch 124, iter 5820, avg. loss 0.87, avg. ppl 1.05 cum. examples 58088, speed 8392.23 words/sec, time elapsed 453.71 sec\n",
      "epoch 125, iter 5830, avg. loss 0.88, avg. ppl 1.05 cum. examples 58404, speed 8462.06 words/sec, time elapsed 454.43 sec\n",
      "epoch 125, iter 5840, avg. loss 0.87, avg. ppl 1.04 cum. examples 58724, speed 9247.40 words/sec, time elapsed 455.13 sec\n",
      "epoch 125, iter 5850, avg. loss 0.89, avg. ppl 1.05 cum. examples 59044, speed 8625.04 words/sec, time elapsed 455.87 sec\n",
      "epoch 125, iter 5860, avg. loss 0.93, avg. ppl 1.04 cum. examples 59364, speed 8822.33 words/sec, time elapsed 456.64 sec\n",
      "epoch 125, iter 5870, avg. loss 0.91, avg. ppl 1.04 cum. examples 59684, speed 9218.66 words/sec, time elapsed 457.37 sec\n",
      "epoch 126, iter 5880, avg. loss 0.72, avg. ppl 1.04 cum. examples 60000, speed 7928.21 words/sec, time elapsed 458.15 sec\n",
      "epoch 126, iter 5890, avg. loss 0.81, avg. ppl 1.04 cum. examples 60320, speed 8324.15 words/sec, time elapsed 458.96 sec\n",
      "epoch 126, iter 5900, avg. loss 0.88, avg. ppl 1.04 cum. examples 60640, speed 8004.67 words/sec, time elapsed 459.78 sec\n",
      "epoch 126, iter 5910, avg. loss 0.80, avg. ppl 1.04 cum. examples 60960, speed 8314.94 words/sec, time elapsed 460.51 sec\n",
      "epoch 126, iter 5920, avg. loss 0.91, avg. ppl 1.04 cum. examples 61280, speed 8599.00 words/sec, time elapsed 461.30 sec\n",
      "epoch 127, iter 5930, avg. loss 0.79, avg. ppl 1.04 cum. examples 61596, speed 8249.70 words/sec, time elapsed 462.05 sec\n",
      "epoch 127, iter 5940, avg. loss 0.89, avg. ppl 1.04 cum. examples 61916, speed 8215.45 words/sec, time elapsed 462.85 sec\n",
      "epoch 127, iter 5950, avg. loss 0.88, avg. ppl 1.04 cum. examples 62236, speed 8392.52 words/sec, time elapsed 463.66 sec\n",
      "epoch 127, iter 5960, avg. loss 0.88, avg. ppl 1.04 cum. examples 62556, speed 8992.84 words/sec, time elapsed 464.38 sec\n",
      "epoch 128, iter 5970, avg. loss 0.85, avg. ppl 1.04 cum. examples 62872, speed 8214.01 words/sec, time elapsed 465.17 sec\n",
      "epoch 128, iter 5980, avg. loss 0.88, avg. ppl 1.04 cum. examples 63192, speed 8596.86 words/sec, time elapsed 465.92 sec\n",
      "epoch 128, iter 5990, avg. loss 0.80, avg. ppl 1.04 cum. examples 63512, speed 8866.30 words/sec, time elapsed 466.65 sec\n",
      "epoch 128, iter 6000, avg. loss 0.90, avg. ppl 1.04 cum. examples 63832, speed 8808.88 words/sec, time elapsed 467.42 sec\n",
      "epoch 128, iter 6000, cum. loss 1.22, cum. ppl 1.06 cum. examples 63832\n",
      "begin validation ...\n",
      "validation: iter 6000, dev. ppl 511593.877356\n",
      "hit patience 2\n",
      "epoch 128, iter 6010, avg. loss 0.85, avg. ppl 1.04 cum. examples 320, speed 5786.90 words/sec, time elapsed 468.55 sec\n",
      "epoch 129, iter 6020, avg. loss 0.70, avg. ppl 1.04 cum. examples 636, speed 7494.90 words/sec, time elapsed 469.37 sec\n",
      "epoch 129, iter 6030, avg. loss 0.98, avg. ppl 1.05 cum. examples 956, speed 8345.84 words/sec, time elapsed 470.20 sec\n",
      "epoch 129, iter 6040, avg. loss 0.89, avg. ppl 1.04 cum. examples 1276, speed 8601.14 words/sec, time elapsed 470.98 sec\n",
      "epoch 129, iter 6050, avg. loss 0.77, avg. ppl 1.04 cum. examples 1596, speed 8574.79 words/sec, time elapsed 471.68 sec\n",
      "epoch 129, iter 6060, avg. loss 0.87, avg. ppl 1.04 cum. examples 1916, speed 9145.08 words/sec, time elapsed 472.40 sec\n",
      "epoch 130, iter 6070, avg. loss 0.80, avg. ppl 1.04 cum. examples 2232, speed 8812.38 words/sec, time elapsed 473.14 sec\n",
      "epoch 130, iter 6080, avg. loss 0.92, avg. ppl 1.04 cum. examples 2552, speed 8623.49 words/sec, time elapsed 473.92 sec\n",
      "epoch 130, iter 6090, avg. loss 0.85, avg. ppl 1.04 cum. examples 2872, speed 8571.60 words/sec, time elapsed 474.67 sec\n",
      "epoch 130, iter 6100, avg. loss 0.82, avg. ppl 1.04 cum. examples 3192, speed 8886.38 words/sec, time elapsed 475.39 sec\n",
      "epoch 130, iter 6110, avg. loss 0.93, avg. ppl 1.05 cum. examples 3508, speed 8824.83 words/sec, time elapsed 476.13 sec\n",
      "epoch 131, iter 6120, avg. loss 0.74, avg. ppl 1.04 cum. examples 3828, speed 8637.69 words/sec, time elapsed 476.88 sec\n",
      "epoch 131, iter 6130, avg. loss 0.89, avg. ppl 1.04 cum. examples 4148, speed 9278.14 words/sec, time elapsed 477.60 sec\n",
      "epoch 131, iter 6140, avg. loss 0.83, avg. ppl 1.04 cum. examples 4468, speed 8699.00 words/sec, time elapsed 478.36 sec\n",
      "epoch 131, iter 6150, avg. loss 0.84, avg. ppl 1.04 cum. examples 4788, speed 8198.90 words/sec, time elapsed 479.18 sec\n",
      "epoch 132, iter 6160, avg. loss 0.72, avg. ppl 1.04 cum. examples 5104, speed 7988.55 words/sec, time elapsed 479.93 sec\n",
      "epoch 132, iter 6170, avg. loss 0.68, avg. ppl 1.04 cum. examples 5424, speed 8751.94 words/sec, time elapsed 480.65 sec\n",
      "epoch 132, iter 6180, avg. loss 0.81, avg. ppl 1.04 cum. examples 5744, speed 8937.53 words/sec, time elapsed 481.38 sec\n",
      "epoch 132, iter 6190, avg. loss 0.77, avg. ppl 1.04 cum. examples 6064, speed 9182.07 words/sec, time elapsed 482.10 sec\n",
      "epoch 132, iter 6200, avg. loss 0.86, avg. ppl 1.04 cum. examples 6384, speed 9232.91 words/sec, time elapsed 482.84 sec\n",
      "epoch 133, iter 6210, avg. loss 0.75, avg. ppl 1.04 cum. examples 6700, speed 8306.45 words/sec, time elapsed 483.59 sec\n",
      "epoch 133, iter 6220, avg. loss 0.99, avg. ppl 1.05 cum. examples 7020, speed 8480.89 words/sec, time elapsed 484.35 sec\n",
      "epoch 133, iter 6230, avg. loss 0.76, avg. ppl 1.04 cum. examples 7340, speed 8757.50 words/sec, time elapsed 485.10 sec\n",
      "epoch 133, iter 6240, avg. loss 0.77, avg. ppl 1.04 cum. examples 7660, speed 8963.52 words/sec, time elapsed 485.83 sec\n",
      "epoch 133, iter 6250, avg. loss 0.88, avg. ppl 1.04 cum. examples 7980, speed 9093.69 words/sec, time elapsed 486.58 sec\n",
      "epoch 134, iter 6260, avg. loss 0.76, avg. ppl 1.04 cum. examples 8296, speed 8160.97 words/sec, time elapsed 487.35 sec\n",
      "epoch 134, iter 6270, avg. loss 0.78, avg. ppl 1.04 cum. examples 8616, speed 8885.93 words/sec, time elapsed 488.09 sec\n",
      "epoch 134, iter 6280, avg. loss 0.81, avg. ppl 1.04 cum. examples 8936, speed 8720.80 words/sec, time elapsed 488.85 sec\n",
      "epoch 134, iter 6290, avg. loss 0.76, avg. ppl 1.04 cum. examples 9256, speed 8492.87 words/sec, time elapsed 489.61 sec\n",
      "epoch 135, iter 6300, avg. loss 0.86, avg. ppl 1.04 cum. examples 9572, speed 8055.91 words/sec, time elapsed 490.42 sec\n",
      "epoch 135, iter 6310, avg. loss 0.91, avg. ppl 1.05 cum. examples 9892, speed 6946.37 words/sec, time elapsed 491.36 sec\n",
      "epoch 135, iter 6320, avg. loss 0.71, avg. ppl 1.03 cum. examples 10212, speed 8198.65 words/sec, time elapsed 492.17 sec\n",
      "epoch 135, iter 6330, avg. loss 0.77, avg. ppl 1.04 cum. examples 10532, speed 8201.14 words/sec, time elapsed 492.98 sec\n",
      "epoch 135, iter 6340, avg. loss 0.76, avg. ppl 1.04 cum. examples 10852, speed 8779.84 words/sec, time elapsed 493.71 sec\n",
      "epoch 136, iter 6350, avg. loss 0.67, avg. ppl 1.03 cum. examples 11168, speed 8726.16 words/sec, time elapsed 494.43 sec\n",
      "epoch 136, iter 6360, avg. loss 0.74, avg. ppl 1.04 cum. examples 11488, speed 8646.07 words/sec, time elapsed 495.19 sec\n",
      "epoch 136, iter 6370, avg. loss 0.77, avg. ppl 1.04 cum. examples 11808, speed 8863.78 words/sec, time elapsed 495.95 sec\n",
      "epoch 136, iter 6380, avg. loss 0.73, avg. ppl 1.04 cum. examples 12128, speed 7975.10 words/sec, time elapsed 496.73 sec\n",
      "epoch 136, iter 6390, avg. loss 0.81, avg. ppl 1.04 cum. examples 12448, speed 8476.27 words/sec, time elapsed 497.50 sec\n",
      "epoch 137, iter 6400, avg. loss 0.66, avg. ppl 1.03 cum. examples 12764, speed 8560.45 words/sec, time elapsed 498.21 sec\n",
      "epoch 137, iter 6410, avg. loss 0.83, avg. ppl 1.04 cum. examples 13084, speed 8531.22 words/sec, time elapsed 499.01 sec\n",
      "epoch 137, iter 6420, avg. loss 0.89, avg. ppl 1.04 cum. examples 13404, speed 8933.78 words/sec, time elapsed 499.78 sec\n",
      "epoch 137, iter 6430, avg. loss 0.65, avg. ppl 1.03 cum. examples 13724, speed 8122.57 words/sec, time elapsed 500.55 sec\n",
      "epoch 138, iter 6440, avg. loss 0.71, avg. ppl 1.04 cum. examples 14040, speed 8227.83 words/sec, time elapsed 501.31 sec\n",
      "epoch 138, iter 6450, avg. loss 0.69, avg. ppl 1.04 cum. examples 14360, speed 8080.73 words/sec, time elapsed 502.10 sec\n",
      "epoch 138, iter 6460, avg. loss 0.74, avg. ppl 1.04 cum. examples 14680, speed 8276.56 words/sec, time elapsed 502.90 sec\n",
      "epoch 138, iter 6470, avg. loss 0.85, avg. ppl 1.04 cum. examples 15000, speed 8079.47 words/sec, time elapsed 503.73 sec\n",
      "epoch 138, iter 6480, avg. loss 0.85, avg. ppl 1.04 cum. examples 15320, speed 8055.70 words/sec, time elapsed 504.55 sec\n",
      "epoch 139, iter 6490, avg. loss 0.78, avg. ppl 1.04 cum. examples 15636, speed 7641.60 words/sec, time elapsed 505.39 sec\n",
      "epoch 139, iter 6500, avg. loss 0.79, avg. ppl 1.04 cum. examples 15956, speed 8307.87 words/sec, time elapsed 506.20 sec\n",
      "epoch 139, iter 6510, avg. loss 0.70, avg. ppl 1.04 cum. examples 16276, speed 8212.48 words/sec, time elapsed 506.97 sec\n",
      "epoch 139, iter 6520, avg. loss 0.65, avg. ppl 1.03 cum. examples 16596, speed 8447.83 words/sec, time elapsed 507.72 sec\n",
      "epoch 139, iter 6530, avg. loss 0.72, avg. ppl 1.04 cum. examples 16916, speed 8602.11 words/sec, time elapsed 508.50 sec\n",
      "epoch 140, iter 6540, avg. loss 0.76, avg. ppl 1.04 cum. examples 17232, speed 8011.36 words/sec, time elapsed 509.34 sec\n",
      "epoch 140, iter 6550, avg. loss 0.75, avg. ppl 1.04 cum. examples 17552, speed 8717.05 words/sec, time elapsed 510.09 sec\n",
      "epoch 140, iter 6560, avg. loss 0.70, avg. ppl 1.04 cum. examples 17872, speed 8280.65 words/sec, time elapsed 510.87 sec\n",
      "epoch 140, iter 6570, avg. loss 0.64, avg. ppl 1.03 cum. examples 18192, speed 8254.63 words/sec, time elapsed 511.61 sec\n",
      "epoch 140, iter 6580, avg. loss 0.71, avg. ppl 1.03 cum. examples 18508, speed 8653.05 words/sec, time elapsed 512.37 sec\n",
      "epoch 141, iter 6590, avg. loss 0.79, avg. ppl 1.04 cum. examples 18828, speed 8492.18 words/sec, time elapsed 513.18 sec\n",
      "epoch 141, iter 6600, avg. loss 0.68, avg. ppl 1.03 cum. examples 19148, speed 8442.92 words/sec, time elapsed 513.96 sec\n",
      "epoch 141, iter 6610, avg. loss 0.69, avg. ppl 1.04 cum. examples 19468, speed 8581.39 words/sec, time elapsed 514.71 sec\n",
      "epoch 141, iter 6620, avg. loss 0.69, avg. ppl 1.04 cum. examples 19788, speed 8521.19 words/sec, time elapsed 515.43 sec\n",
      "epoch 142, iter 6630, avg. loss 0.66, avg. ppl 1.03 cum. examples 20104, speed 8630.54 words/sec, time elapsed 516.16 sec\n",
      "epoch 142, iter 6640, avg. loss 0.80, avg. ppl 1.04 cum. examples 20424, speed 8815.12 words/sec, time elapsed 516.93 sec\n",
      "epoch 142, iter 6650, avg. loss 0.75, avg. ppl 1.04 cum. examples 20744, speed 8288.87 words/sec, time elapsed 517.72 sec\n",
      "epoch 142, iter 6660, avg. loss 0.77, avg. ppl 1.04 cum. examples 21064, speed 8245.60 words/sec, time elapsed 518.52 sec\n",
      "epoch 142, iter 6670, avg. loss 0.73, avg. ppl 1.04 cum. examples 21384, speed 8680.47 words/sec, time elapsed 519.27 sec\n",
      "epoch 143, iter 6680, avg. loss 0.65, avg. ppl 1.03 cum. examples 21700, speed 8114.31 words/sec, time elapsed 520.06 sec\n",
      "epoch 143, iter 6690, avg. loss 0.68, avg. ppl 1.03 cum. examples 22020, speed 8444.12 words/sec, time elapsed 520.83 sec\n",
      "epoch 143, iter 6700, avg. loss 0.70, avg. ppl 1.04 cum. examples 22340, speed 8799.91 words/sec, time elapsed 521.56 sec\n",
      "epoch 143, iter 6710, avg. loss 0.58, avg. ppl 1.03 cum. examples 22660, speed 8473.63 words/sec, time elapsed 522.35 sec\n",
      "epoch 143, iter 6720, avg. loss 0.66, avg. ppl 1.03 cum. examples 22980, speed 8024.79 words/sec, time elapsed 523.15 sec\n",
      "epoch 144, iter 6730, avg. loss 0.62, avg. ppl 1.03 cum. examples 23296, speed 7873.87 words/sec, time elapsed 523.96 sec\n",
      "epoch 144, iter 6740, avg. loss 0.67, avg. ppl 1.03 cum. examples 23616, speed 8572.46 words/sec, time elapsed 524.71 sec\n",
      "epoch 144, iter 6750, avg. loss 0.73, avg. ppl 1.04 cum. examples 23936, speed 8467.24 words/sec, time elapsed 525.46 sec\n",
      "epoch 144, iter 6760, avg. loss 0.81, avg. ppl 1.04 cum. examples 24256, speed 8532.65 words/sec, time elapsed 526.24 sec\n",
      "epoch 145, iter 6770, avg. loss 0.71, avg. ppl 1.03 cum. examples 24572, speed 8541.93 words/sec, time elapsed 527.01 sec\n",
      "epoch 145, iter 6780, avg. loss 0.67, avg. ppl 1.03 cum. examples 24892, speed 8631.80 words/sec, time elapsed 527.77 sec\n",
      "epoch 145, iter 6790, avg. loss 0.72, avg. ppl 1.04 cum. examples 25212, speed 8205.16 words/sec, time elapsed 528.58 sec\n",
      "epoch 145, iter 6800, avg. loss 0.73, avg. ppl 1.04 cum. examples 25532, speed 8305.90 words/sec, time elapsed 529.37 sec\n",
      "epoch 145, iter 6810, avg. loss 0.66, avg. ppl 1.03 cum. examples 25852, speed 7724.38 words/sec, time elapsed 530.17 sec\n",
      "epoch 146, iter 6820, avg. loss 0.81, avg. ppl 1.04 cum. examples 26168, speed 8557.82 words/sec, time elapsed 530.96 sec\n",
      "epoch 146, iter 6830, avg. loss 0.57, avg. ppl 1.03 cum. examples 26488, speed 8443.25 words/sec, time elapsed 531.72 sec\n",
      "epoch 146, iter 6840, avg. loss 0.64, avg. ppl 1.03 cum. examples 26808, speed 8141.57 words/sec, time elapsed 532.55 sec\n",
      "epoch 146, iter 6850, avg. loss 0.86, avg. ppl 1.04 cum. examples 27128, speed 7646.21 words/sec, time elapsed 533.44 sec\n",
      "epoch 146, iter 6860, avg. loss 0.61, avg. ppl 1.03 cum. examples 27448, speed 8046.94 words/sec, time elapsed 534.20 sec\n",
      "epoch 147, iter 6870, avg. loss 0.67, avg. ppl 1.03 cum. examples 27764, speed 8473.32 words/sec, time elapsed 534.95 sec\n",
      "epoch 147, iter 6880, avg. loss 0.64, avg. ppl 1.03 cum. examples 28084, speed 9712.25 words/sec, time elapsed 535.61 sec\n",
      "epoch 147, iter 6890, avg. loss 0.85, avg. ppl 1.04 cum. examples 28404, speed 8429.37 words/sec, time elapsed 536.41 sec\n",
      "epoch 147, iter 6900, avg. loss 0.69, avg. ppl 1.03 cum. examples 28724, speed 8659.82 words/sec, time elapsed 537.17 sec\n",
      "epoch 148, iter 6910, avg. loss 0.77, avg. ppl 1.04 cum. examples 29040, speed 8695.18 words/sec, time elapsed 537.90 sec\n",
      "epoch 148, iter 6920, avg. loss 0.82, avg. ppl 1.04 cum. examples 29360, speed 8753.77 words/sec, time elapsed 538.70 sec\n",
      "epoch 148, iter 6930, avg. loss 0.69, avg. ppl 1.03 cum. examples 29680, speed 7993.60 words/sec, time elapsed 539.51 sec\n",
      "epoch 148, iter 6940, avg. loss 0.64, avg. ppl 1.03 cum. examples 30000, speed 8453.88 words/sec, time elapsed 540.26 sec\n",
      "epoch 148, iter 6950, avg. loss 0.70, avg. ppl 1.03 cum. examples 30320, speed 8687.78 words/sec, time elapsed 541.02 sec\n",
      "epoch 149, iter 6960, avg. loss 0.62, avg. ppl 1.03 cum. examples 30636, speed 8506.95 words/sec, time elapsed 541.73 sec\n",
      "epoch 149, iter 6970, avg. loss 0.58, avg. ppl 1.03 cum. examples 30956, speed 8502.92 words/sec, time elapsed 542.48 sec\n",
      "epoch 149, iter 6980, avg. loss 0.71, avg. ppl 1.04 cum. examples 31276, speed 8250.21 words/sec, time elapsed 543.27 sec\n",
      "epoch 149, iter 6990, avg. loss 0.71, avg. ppl 1.03 cum. examples 31596, speed 8015.85 words/sec, time elapsed 544.11 sec\n",
      "epoch 149, iter 7000, avg. loss 0.74, avg. ppl 1.04 cum. examples 31916, speed 8824.92 words/sec, time elapsed 544.84 sec\n",
      "epoch 150, iter 7010, avg. loss 0.72, avg. ppl 1.03 cum. examples 32232, speed 8117.92 words/sec, time elapsed 545.68 sec\n",
      "epoch 150, iter 7020, avg. loss 0.74, avg. ppl 1.04 cum. examples 32552, speed 8358.51 words/sec, time elapsed 546.46 sec\n",
      "epoch 150, iter 7030, avg. loss 0.61, avg. ppl 1.03 cum. examples 32872, speed 9175.29 words/sec, time elapsed 547.21 sec\n",
      "epoch 150, iter 7040, avg. loss 0.59, avg. ppl 1.03 cum. examples 33192, speed 8299.59 words/sec, time elapsed 547.95 sec\n",
      "epoch 150, iter 7050, avg. loss 0.64, avg. ppl 1.03 cum. examples 33508, speed 8431.63 words/sec, time elapsed 548.68 sec\n",
      "epoch 151, iter 7060, avg. loss 0.61, avg. ppl 1.03 cum. examples 33828, speed 8378.06 words/sec, time elapsed 549.46 sec\n",
      "epoch 151, iter 7070, avg. loss 0.70, avg. ppl 1.03 cum. examples 34148, speed 8390.26 words/sec, time elapsed 550.24 sec\n",
      "epoch 151, iter 7080, avg. loss 0.61, avg. ppl 1.03 cum. examples 34468, speed 8732.69 words/sec, time elapsed 550.99 sec\n",
      "epoch 151, iter 7090, avg. loss 0.70, avg. ppl 1.03 cum. examples 34788, speed 8786.39 words/sec, time elapsed 551.73 sec\n",
      "epoch 152, iter 7100, avg. loss 0.69, avg. ppl 1.03 cum. examples 35104, speed 8827.41 words/sec, time elapsed 552.48 sec\n",
      "epoch 152, iter 7110, avg. loss 0.63, avg. ppl 1.03 cum. examples 35424, speed 7915.59 words/sec, time elapsed 553.29 sec\n",
      "epoch 152, iter 7120, avg. loss 0.60, avg. ppl 1.03 cum. examples 35744, speed 8714.94 words/sec, time elapsed 554.05 sec\n",
      "epoch 152, iter 7130, avg. loss 0.74, avg. ppl 1.04 cum. examples 36064, speed 8394.22 words/sec, time elapsed 554.85 sec\n",
      "epoch 152, iter 7140, avg. loss 0.62, avg. ppl 1.03 cum. examples 36384, speed 7988.64 words/sec, time elapsed 555.63 sec\n",
      "epoch 153, iter 7150, avg. loss 0.59, avg. ppl 1.03 cum. examples 36700, speed 8308.68 words/sec, time elapsed 556.39 sec\n",
      "epoch 153, iter 7160, avg. loss 0.71, avg. ppl 1.04 cum. examples 37020, speed 8003.55 words/sec, time elapsed 557.17 sec\n",
      "epoch 153, iter 7170, avg. loss 0.69, avg. ppl 1.03 cum. examples 37340, speed 8300.38 words/sec, time elapsed 558.00 sec\n",
      "epoch 153, iter 7180, avg. loss 0.71, avg. ppl 1.03 cum. examples 37660, speed 7544.63 words/sec, time elapsed 558.88 sec\n",
      "epoch 153, iter 7190, avg. loss 0.64, avg. ppl 1.03 cum. examples 37980, speed 7810.19 words/sec, time elapsed 559.69 sec\n",
      "epoch 154, iter 7200, avg. loss 0.64, avg. ppl 1.03 cum. examples 38296, speed 7030.96 words/sec, time elapsed 560.58 sec\n",
      "epoch 154, iter 7210, avg. loss 0.65, avg. ppl 1.03 cum. examples 38616, speed 8372.59 words/sec, time elapsed 561.36 sec\n",
      "epoch 154, iter 7220, avg. loss 0.77, avg. ppl 1.04 cum. examples 38936, speed 8953.54 words/sec, time elapsed 562.07 sec\n",
      "epoch 154, iter 7230, avg. loss 0.71, avg. ppl 1.03 cum. examples 39256, speed 8439.06 words/sec, time elapsed 562.87 sec\n",
      "epoch 155, iter 7240, avg. loss 0.70, avg. ppl 1.03 cum. examples 39572, speed 8808.48 words/sec, time elapsed 563.63 sec\n",
      "epoch 155, iter 7250, avg. loss 0.58, avg. ppl 1.03 cum. examples 39892, speed 8078.93 words/sec, time elapsed 564.43 sec\n",
      "epoch 155, iter 7260, avg. loss 0.67, avg. ppl 1.03 cum. examples 40212, speed 8753.70 words/sec, time elapsed 565.20 sec\n",
      "epoch 155, iter 7270, avg. loss 0.70, avg. ppl 1.03 cum. examples 40532, speed 8448.07 words/sec, time elapsed 565.97 sec\n",
      "epoch 155, iter 7280, avg. loss 0.68, avg. ppl 1.03 cum. examples 40852, speed 8379.00 words/sec, time elapsed 566.74 sec\n",
      "epoch 156, iter 7290, avg. loss 0.61, avg. ppl 1.03 cum. examples 41168, speed 8488.58 words/sec, time elapsed 567.47 sec\n",
      "epoch 156, iter 7300, avg. loss 0.62, avg. ppl 1.03 cum. examples 41488, speed 8505.34 words/sec, time elapsed 568.25 sec\n",
      "epoch 156, iter 7310, avg. loss 0.69, avg. ppl 1.03 cum. examples 41808, speed 7790.24 words/sec, time elapsed 569.08 sec\n",
      "epoch 156, iter 7320, avg. loss 0.71, avg. ppl 1.04 cum. examples 42128, speed 7901.26 words/sec, time elapsed 569.91 sec\n",
      "epoch 156, iter 7330, avg. loss 0.61, avg. ppl 1.03 cum. examples 42448, speed 8171.30 words/sec, time elapsed 570.71 sec\n",
      "epoch 157, iter 7340, avg. loss 0.66, avg. ppl 1.03 cum. examples 42764, speed 7697.70 words/sec, time elapsed 571.55 sec\n",
      "epoch 157, iter 7350, avg. loss 0.56, avg. ppl 1.03 cum. examples 43084, speed 7288.56 words/sec, time elapsed 572.42 sec\n",
      "epoch 157, iter 7360, avg. loss 0.68, avg. ppl 1.03 cum. examples 43404, speed 8756.34 words/sec, time elapsed 573.22 sec\n",
      "epoch 157, iter 7370, avg. loss 0.63, avg. ppl 1.03 cum. examples 43724, speed 8381.47 words/sec, time elapsed 573.98 sec\n",
      "epoch 158, iter 7380, avg. loss 0.68, avg. ppl 1.03 cum. examples 44040, speed 8453.25 words/sec, time elapsed 574.74 sec\n",
      "epoch 158, iter 7390, avg. loss 0.52, avg. ppl 1.03 cum. examples 44360, speed 8927.53 words/sec, time elapsed 575.47 sec\n",
      "epoch 158, iter 7400, avg. loss 0.62, avg. ppl 1.03 cum. examples 44680, speed 7914.23 words/sec, time elapsed 576.25 sec\n",
      "epoch 158, iter 7410, avg. loss 0.55, avg. ppl 1.03 cum. examples 45000, speed 8367.13 words/sec, time elapsed 577.03 sec\n",
      "epoch 158, iter 7420, avg. loss 0.64, avg. ppl 1.03 cum. examples 45320, speed 9066.49 words/sec, time elapsed 577.79 sec\n",
      "epoch 159, iter 7430, avg. loss 0.68, avg. ppl 1.03 cum. examples 45636, speed 8217.76 words/sec, time elapsed 578.57 sec\n",
      "epoch 159, iter 7440, avg. loss 0.54, avg. ppl 1.03 cum. examples 45956, speed 8412.05 words/sec, time elapsed 579.33 sec\n",
      "epoch 159, iter 7450, avg. loss 0.61, avg. ppl 1.03 cum. examples 46276, speed 8556.74 words/sec, time elapsed 580.09 sec\n",
      "epoch 159, iter 7460, avg. loss 0.60, avg. ppl 1.03 cum. examples 46596, speed 8904.35 words/sec, time elapsed 580.85 sec\n",
      "epoch 159, iter 7470, avg. loss 0.68, avg. ppl 1.03 cum. examples 46916, speed 8883.54 words/sec, time elapsed 581.58 sec\n",
      "epoch 160, iter 7480, avg. loss 0.49, avg. ppl 1.03 cum. examples 47232, speed 7615.67 words/sec, time elapsed 582.38 sec\n",
      "epoch 160, iter 7490, avg. loss 0.56, avg. ppl 1.03 cum. examples 47552, speed 7976.65 words/sec, time elapsed 583.17 sec\n",
      "epoch 160, iter 7500, avg. loss 0.71, avg. ppl 1.03 cum. examples 47872, speed 8432.92 words/sec, time elapsed 583.99 sec\n",
      "epoch 160, iter 7510, avg. loss 0.70, avg. ppl 1.03 cum. examples 48192, speed 8487.12 words/sec, time elapsed 584.78 sec\n",
      "epoch 160, iter 7520, avg. loss 0.62, avg. ppl 1.03 cum. examples 48508, speed 9057.28 words/sec, time elapsed 585.49 sec\n",
      "epoch 161, iter 7530, avg. loss 0.57, avg. ppl 1.03 cum. examples 48828, speed 8323.01 words/sec, time elapsed 586.28 sec\n",
      "epoch 161, iter 7540, avg. loss 0.63, avg. ppl 1.03 cum. examples 49148, speed 7756.61 words/sec, time elapsed 587.14 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-85655c8ddd7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-85655c8ddd7f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_mode)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-94ad97656db4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mexample_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_sents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-02250bb747ff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0menc_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0menc_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_sent_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mcombined_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_init_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_vocab_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-02250bb747ff>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, enc_hiddens, enc_masks, dec_init_state, target_padded)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mY_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# (b, e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mYbar_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (b, h + e)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mdec_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYbar_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hiddens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hiddens_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0moutput_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mo_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-02250bb747ff>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m###     Tensor Squeeze:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mdec_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYbar_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_state\u001b[0m    \u001b[0;31m# (b, h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0me_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hiddens_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# (b, src_len, h) @ (b, h, 1) -> (b, src_len, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m         )\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(train_mode=True):\n",
    "    \"\"\" Main func.\n",
    "    \"\"\"\n",
    "    # args = docopt(__doc__)\n",
    "\n",
    "    # Check pytorch version\n",
    "    assert(torch.__version__ == \"1.1.0\"), \"Please update your installation of PyTorch. You have {} and you should have version 1.0.0\".format(torch.__version__)\n",
    "\n",
    "    # seed the random number generators\n",
    "    print(args)\n",
    "    seed = int(args['--seed'])\n",
    "    torch.manual_seed(seed)\n",
    "    if args['--cuda']:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed * 13 // 7)\n",
    "\n",
    "    if train_mode is True:\n",
    "        train(args)\n",
    "    else:\n",
    "        decode(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.5.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
